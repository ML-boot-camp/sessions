{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>ML course made with \u2665 by data scientists for beginners.</p> <p>It's a bootcamp, to be done in a few sessions of a few days of training.</p> <p>Feel free to explore the github repository or start browsing the courses materials on the left \ud83d\udc48.</p>"},{"location":"01-Introduction/","title":"01 Introduction","text":"<ul> <li>Data is eating the world</li> <li>What is AI</li> <li>State of the art in AI</li> <li>Bottlenecks for AI</li> </ul>"},{"location":"01-Introduction/1-data-is-eating-the-world/","title":"Data is eating the world","text":""},{"location":"01-Introduction/1-data-is-eating-the-world/#envisioning-the-future","title":"Envisioning the future","text":"<p>To understand the future, look at weak signals with exponential growths.</p> <p>What are those weak signals ?</p> <p> Source: @vb_jens</p>"},{"location":"01-Introduction/1-data-is-eating-the-world/#cheap-computing","title":"Cheap computing","text":"<p>Computing power per $ = x2 every 18 months</p> <p> Source: Moore's Law over 120 Years (Ray Kurzweil)</p>"},{"location":"01-Introduction/1-data-is-eating-the-world/#ubiquitous-computing","title":"Ubiquitous computing","text":"<p>Transistors per human = x10 every 5 years</p> <p> Source: Darrin Qualman</p>"},{"location":"01-Introduction/1-data-is-eating-the-world/#growing-data","title":"Growing data","text":"<p>Data stored = x5 every 4 years</p> <p>400 exaBytes = 40 GB/human</p> <p>40 000 exaBytes = 4 TB/human</p> <p> Source: The Digital Universe in 2020 (IDC)</p>"},{"location":"01-Introduction/1-data-is-eating-the-world/#slow-growth-of-it-jobs","title":"Slow growth of IT jobs","text":"<p>Slow growth of number of technologists in the population.</p> <p>Non-exponential !</p> <p> Source: Employed ICT specialists (Eurostat by DB nomics)</p>"},{"location":"01-Introduction/1-data-is-eating-the-world/#big-data-is-eating-the-world","title":"Big data is eating the world","text":"<p>IT &amp; Engineering departments are forced to get into cloud, big data and machine learning.</p> <p> Source: The Business Imperatives - EMC</p>"},{"location":"01-Introduction/1-data-is-eating-the-world/#information-brings-value-by-improving-decisions","title":"Information brings value by improving decisions","text":"<p>A weather forecast is not valuated the same by a citizen and by a wine producer.</p> <p>The assessment of oil reserves is not valuated the same by a citizen and by a trader on the commodity market.</p> <p>AI creates information which is exploited through decisions.</p> <p>You can't leverage AI without understanding decision making !</p> <p>\ud83d\udcca Information + \u270b Decision = \ud83d\udcb0 Value</p>"},{"location":"01-Introduction/2-what-is-AI/","title":"What is AI","text":""},{"location":"01-Introduction/2-what-is-AI/#the-3-eras-of-ai","title":"The 3 eras of AI","text":"<p>AI has embraced very different technologies during its history:</p> <ol> <li>classical code     if \u2192 then \u2192 else</li> <li>expert system     using human-made rules</li> <li>statistical algorithms     machine learning the rules</li> </ol> <p> Source: State-of-the-Art Mobile Intelligence (research paper)</p>"},{"location":"01-Introduction/2-what-is-AI/#ai-is-machine-learning","title":"AI is machine learning","text":"<p>Knowledge is extracted from data. Machine learning is a combination of:</p> <ul> <li>statistical algorithms</li> <li>systematic experimental process</li> </ul> <p>Basically, it's experimentation with algorithms.</p> <p> Source: xkcd</p>"},{"location":"01-Introduction/2-what-is-AI/#principles-of-machine-learning","title":"Principles of machine learning","text":"<p>The model computes predictions. E.g:</p> <ul> <li>linear regression</li> <li>decision trees</li> <li> <p>SVM The optimizer tunes the model to reduce the prediction error. E.g:</p> </li> <li> <p>gradient descent</p> </li> <li>genetic algorithms</li> </ul> <p> Source: From Linear Regression to Deep Learning in 5 Minutes</p>"},{"location":"01-Introduction/2-what-is-AI/#differentiable-programming","title":"Differentiable programming","text":"<p>Machine learning by gradient descent is an optimisation of a differentiable function:</p> <ul> <li>A differentiable function allows to compute the error gradient</li> <li>Each iteration, the gradient shows how to modify the model parameters to reduce the error.</li> </ul> <p> Source: Linear Regression by using Gradient Descent Algorithm: Your first step towards Machine Learning (medium)</p> Note <p>If the function we want to optimize is not differentiable, we use other optimization algorithms (such as random search, genetic algorithms, bayesian optimization, ...), it is then called black box optimization.</p>"},{"location":"01-Introduction/2-what-is-AI/#demo-interactive-linear-regression","title":"Demo: Interactive linear regression","text":"<p>\ud83d\udc49 Interactive linear regression \u2013 GeoGebra</p> <p> Source: Interactive linear regression \u2013 GeoGebra </p>"},{"location":"01-Introduction/2-what-is-AI/#the-deep-learning-revolution","title":"The deep learning revolution","text":"<p>Simple models (neurons) are combined together to create a complex model.</p> <p> Source: News Feature: What are the limits of deep learning? (PNAAS)</p>"},{"location":"01-Introduction/2-what-is-AI/#why-deep-learning-is-big-deal","title":"Why deep learning is big deal","text":"<p>\ud83d\udc4d Less preprocessing &amp; feature engineering</p> <p>\ud83d\udc4e Needs much more data \ud83d\udcbe and computing \ud83e\udd75</p> <p> Source: Blue Hexagon</p>"},{"location":"01-Introduction/2-what-is-AI/#demo-image-classification","title":"Demo: Image classification","text":"<p>Teachable machine - image model</p> <p> Source: Teachable machine - image model</p>"},{"location":"01-Introduction/2-what-is-AI/#but-deep-learning-is-superficial","title":"But deep learning is superficial","text":"<p>Deep learning is cool, but you can't deliver without mastering:</p> <ul> <li>data collection</li> <li>data storage</li> <li>data cleaning &amp; preparation</li> <li>feature engineering</li> <li>simple ML algorithms</li> </ul> <p>Deep learning is less than 5% of data projects in industry.</p> <p> Source: The AI Hierarchy of Needs</p>"},{"location":"01-Introduction/2-what-is-AI/#data-is-the-enabler","title":"Data is the enabler","text":"<p>AI breakthroughs happen thanks to:</p> <ul> <li>Old algorithms</li> <li>New datasets</li> </ul> <p>Data is the true enabler of AI research breakthroughs.</p> <p> Source: Datasets Over Algorithms (kdnuggets)</p>"},{"location":"01-Introduction/2-what-is-AI/#careers-in-big-data-ai","title":"Careers in big data &amp; AI","text":"<p> Source: </p> <p> Source: </p> <p> Source: </p> <p> Source: </p> <p>Forget the ambiguous job names, focus on skills. What are the skills mentioned in a job description ?</p>"},{"location":"01-Introduction/2-what-is-AI/#ai-applications-in-civil-aviation","title":"AI Applications in civil aviation","text":"<p>Air traffic conflict detection &amp; resolution</p> <p>  Air traffic conflict detection &amp; resolution </p> <p>Aircraft taxi routing</p> <p>  Aircraft taxi routing </p> <p>ATM workload forecast &amp; ATM sector management</p> <p>  ATM workload forecast &amp; ATM sector management </p> <p>Air traffic planification</p> <p>  Air traffic planification </p> Links to ENAC/OPTIM team research works <ul> <li>ENAC/OPTIM \u2013 Conference papers</li> <li>ENAC/OPTIM \u2013 Publications</li> </ul>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/","title":"State of the art in AI","text":"<p>Taken from this excellent course: Deep Learning Lectures</p>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/#speech-to-text","title":"Speech to text","text":"<p>Waveform \u2192 Text</p> <p> Waveform \u2192 Text</p>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/#computer-vision","title":"Computer vision","text":"<p>Image \u2192 class</p> <p>Image \u2192 (class, bounding box)</p> <p>Image \u2192 (class, shape)</p> <p>  Image \u2192 class Image \u2192 (class, bounding box) Image \u2192 (class, shape) </p> <p>Image \u2192 (class, shape)</p> <p>Image \u2192 facial landmarks</p> <p>  Image \u2192 (class, shape) Image \u2192 facial landmarks </p>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/#natural-language-processing-nlp","title":"Natural language processing (NLP)","text":"<p>Text \u2192 text (different language)</p> <p>Text \u2192 syntax tree</p> <p>  Text \u2192 text (different language) Text \u2192 syntax tree </p> <p>Text \u2192 text (probable short answer)</p> <p>Text \u2192 text (query)</p> <p>  Text \u2192 text (probable short answer) Text \u2192 text (query) </p>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/#computer-vision-nlp","title":"Computer vision &amp; NLP","text":"<p>Image &amp; text (question) \u2192 text (answer)</p> <p>Image \u2192 text (description)</p> <p>  Image &amp; text (question) \u2192 text (answer) Image \u2192 text (description) </p>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/#image-translation","title":"Image translation","text":"<p>Image \u2192 image (with artifacts)</p> <p>Image \u2192 image styled as the other</p> <p>Image \u2192 Image (higher resolution)</p> <p>  Image \u2192 image (with artifacts) Image \u2192 image styled as the other Image \u2192 Image (higher resolution) </p>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/#audio-generation","title":"Audio generation","text":"<p>Waveform \u2192 Waveform (continued)</p> <p>  Waveform \u2192 Waveform (continued) </p> <p>Guess which one is generated ?</p> <p>  Your browser does not support the audio element. </p> <p>  Your browser does not support the audio element. </p>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/#image-generation","title":"Image generation","text":"<p>Vector (random) \u2192 image</p> <p>  Vector (random) \u2192 image </p> <p>Text \u2192 image</p> <p>  Text \u2192 image </p>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/#science-genomics-biology-chemistry-physics","title":"Science - Genomics, biology, chemistry, physics","text":"<p>DNA sequence \u2192 drug</p> <p>  DNA sequence \u2192 drug </p> <p>Protein sequence \u2192 folding shape (protein properties)</p> <p>  Protein sequence \u2192 folding shape (protein properties) </p> <p>Chemical structure \u2192 properties</p> <p>  Chemical structure \u2192 properties </p> <p>Incompressible Euler equations (Navier-stokes for fluids) \u2192 guess of the solution</p> <p>100x speedup in solving time</p> <p>  Incompressible Euler equations (Navier-stokes for fluids) \u2192 guess of the solution </p>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/#gaming","title":"Gaming","text":"<p>Image sequence \u2192 next action</p> <p>  Image sequence \u2192 next action </p>"},{"location":"01-Introduction/3-state-of-the-art-in-AI/#more-models-and-tasks","title":"More models and tasks","text":"<p>Papers with Code - Browse the State-of-the-Art in Machine Learning</p> <p>  Image sequence \u2192 next action </p>"},{"location":"01-Introduction/4-bottlenecks-for-AI/","title":"Bottlenecks for AI","text":""},{"location":"01-Introduction/4-bottlenecks-for-AI/#ai-applications-automation-everywhere","title":"AI applications: automation everywhere","text":"<p>All activities are susceptible to be automated, if there is enough data to train models.</p> <p>The automation equation: \ud83e\uddbe Automation probability = \ud83c\udfbc Coherence * \ud83d\udcca Data</p> <ul> <li>Coherence: similarity between cases, all configurations are known. We can improve coherence by controlling the environment (ex: battery hens, factory, autonomous driving on private property, prefab in construction)</li> <li>Data: the less coherent the environment, the more data we need to describe it (e.g.: driving in Florida on a private property, driving in US, driving in Toulouse).</li> </ul> Source <p>WORKING PAPER The Future of Employment &amp; Q.Chenevier's own opinions</p>"},{"location":"01-Introduction/4-bottlenecks-for-AI/#perception-manipulation","title":"Perception &amp; manipulation","text":"<p>2 unsolved challenges:</p> <ul> <li>Perception in unstructured environments: houses, fields</li> <li>New or irregular objects handling, with a soft grasp and through learning of mistakes</li> </ul> <p>The human hand is a wonderful tool, full of sensors:</p> <ul> <li>Position</li> <li>Toughness / Roughness</li> <li>Heat</li> <li>Humidity</li> </ul> <p>  Difficult manipulation environment </p> <p>  Difficult perception environment </p> <p> Manipulation learning experiment (from 19:48 to 21:11) </p>"},{"location":"01-Introduction/4-bottlenecks-for-AI/#social-intelligence","title":"Social intelligence","text":"<p>Social intelligence tasks require several hard-to-automate skills:</p> <ul> <li>Recognize emotions</li> <li>Being empathetic with our interlocutor (\"compute its state of mind\")</li> <li>Exploit this context and the past interactions</li> </ul> <p>\"Recognizing emotions is a challenge, but the ability to answer in a smart way to this information is even harder.\"</p> <p>Nevertheless, Woebot Health: Relational Agent for Mental Health, a CBT (Cognitive Behavioral Therapies) chatbot, demonstrates shows that many CBT therapists' sessions could be automated.</p> <p>It doesn't replace the therapist, but can reduce significantly the needed amount of sessions to do a therapy.</p> <p>  Woebot health </p>"},{"location":"01-Introduction/4-bottlenecks-for-AI/#creative-intelligence","title":"Creative intelligence","text":"<p>Creative intelligence is about creating new ideas which have a creative value. </p> <p>Generating novelty is easy, the main challenge is to know how to describe our creative values to encode them in a software.</p> <p>  AARON by Harold Cohen </p> <p> EMI by David Cope </p> <p> AIVA </p>"},{"location":"01-Introduction/4-bottlenecks-for-AI/#demo-image-generation-from-text","title":"Demo: Image generation from text","text":"<ol> <li>Generate your image with stable diffusion</li> <li>Inspire from prompt examples in diffusionDB to create your own prompt</li> <li>Repeat step 1 &amp; 2 until you have a nice result</li> </ol> <p> stable diffusion on huggingface </p>"},{"location":"01-Introduction/4-bottlenecks-for-AI/#image-generation-from-text-prompt-explanation","title":"Image generation from text prompt: explanation","text":"<p>CLIP: image \u2192 text</p> <p>VQGAN: vector \u2192 image</p> <p>VQGAN+CLIP: vector \u2192 image \u2192 text</p> <p>The input (vector) is optimized so that the output (text) matches the text prompt </p> <p> </p>"},{"location":"02-Data-engineering/","title":"02 Data engineering","text":"<ul> <li>Basic data engineering</li> <li>Data format</li> <li>\ud83d\udc0d Practice</li> </ul>"},{"location":"02-Data-engineering/1-basic-data-engineering/","title":"Basic data engineering","text":"<p>Join, group by &amp; aggregate</p>"},{"location":"02-Data-engineering/1-basic-data-engineering/#join","title":"Join","text":""},{"location":"02-Data-engineering/1-basic-data-engineering/#join-type","title":"Join type","text":"<p>A join is a database operation that combines rows from two tables based on a related column between them, allowing you to query data from both tables as if they were a single entity. Different types of joins determine which rows are included in the result, providing flexibility in how the tables are merged.</p> <p></p> Join types Join type Description Includes all data from May loose data from Inner  Combines matching rows from both tables None Both  Left  Includes all rows from the left table Left  Right  Right  Includes all rows from the right table Right  Left  Outer  Combines all rows from both tables, matching where possible Both  None <p>Join types are related to the concepts of inclusion and completeness:</p> <ul> <li>Use outer join, if you don't want to loose data, but you may have incomplete data.</li> <li>Use inner join, if you don't want to have incomplete data, but you may loose data.</li> </ul> <p>Note</p> <p>When exploring data using pandas, always start your joins (using the <code>merge</code> method) by:</p> <ul> <li>doing an outer join (<code>how=\"outer\"</code>)</li> <li>checking the result of the join in the <code>_merge</code> column (e.g: using the <code>value_counts</code> method)</li> </ul> <p>E.g:</p> Python<pre><code>result = (\n    df1\n    .merge(df2, on=\"key_column\", how=\"outer\")\n    ._merge\n    .value_counts()\n)\n</code></pre> <p>This shows how many rows are coming from each dataframe (<code>both</code>, <code>left_only</code> or <code>right_only</code>).</p>"},{"location":"02-Data-engineering/1-basic-data-engineering/#matching-type","title":"Matching type","text":"<p>A matching type refers to the relationship between key columns in the tables being joined, such as one-to-one, one-to-many, or many-to-many. It defines how many records in one table correspond to records in another, guiding the structure of the join and affecting the final result.</p> <p></p> Matching types Matching Type Description Example One-to-One (1:1) Each row in the first table corresponds to one row in the second table. Employee to Social Security Number One-to-Many (1:m) Each row in the first table corresponds to multiple rows in the second table. Customer to Orders (One customer, many orders) Many-to-Many (m:m) Multiple rows in the first table correspond to multiple rows in the second table. Products to Categories (Many products, many categories) <p>Matching types are related to the concept of cardinality.</p> <p>In a one-to-one matching:</p> <ul> <li>The cardinality of the inner join is the cardinality of the intersection \\( |   L \\cap R | \\).</li> <li>The cardinality of the outer join is the cardinality of the union \\( | L \\cup   R | \\).</li> </ul> <p>Whereas one-to-many, many-to-one or many-to-many matchings do duplicate rows in the join result and don't preserve cardinality.</p> <p>Note</p> <p>When exploring data using pandas, always use the <code>validate</code> argument specify the matching type of your joins to avoid unpleasant surprises. Like:</p> <ul> <li><code>validate=\"1:1\"</code></li> <li><code>validate=\"1:m\"</code></li> <li><code>validate=\"m:1\"</code></li> <li><code>validate=\"m:m\"</code> (No effect for this one. Can you guess why?)</li> </ul> <p>E.g:</p> Python<pre><code>df1.merge(df2, on=\"key_column\", how=\"inner\", validate=\"1:1\")\n</code></pre>"},{"location":"02-Data-engineering/1-basic-data-engineering/#key-type","title":"Key type","text":"<p>Key columns have 2 types:</p> <ul> <li>Key columns that have unique values are called primary keys. In other words, a   primary key uniquely identifies each row within a table.</li> <li>Key columns in a table that are referring to a primary key in another table are called   foreign keys. In other words, a foreign key links the data from one table to   another (but its values are not expected to be unique !).</li> </ul> <p>A join between 2 tables using:</p> <ul> <li>2 primary keys is a one-to-one matching</li> <li>1 primary key and 1 foreign key is a one-to-many matching</li> <li>2 foreign keys is a many-to-many matching</li> </ul>"},{"location":"02-Data-engineering/1-basic-data-engineering/#examples","title":"Examples","text":"<p>Let's use 2 example tables:</p> <p>Customers table</p> customer_id name 1 Alice 2 Bob 3 Carol <p>Orders table</p> order_id customer_id total_price 101 1 100 102 2 200 103 4 150 <p>Inner Join on <code>customer_id</code>: combines rows that are matching customer IDs in both data frames.</p> customer_id name order_id total_price 1 Alice 101 100 2 Bob 102 200 <p>Left Join on <code>customer_id</code>: same tables as Inner Join, but includes all customer rows.</p> customer_id name order_id total_price 1 Alice 101 100 2 Bob 102 200 3 Carol NaN NaN <p>Right Join on <code>customer_id</code>: includes all order rows and matched customer rows.</p> customer_id name order_id total_price 1 Alice 101 100 2 Bob 102 200 NaN NaN 103 150 <p>Outer Join on <code>customer_id</code>: includes all rows from both data frames.</p> customer_id name order_id total_price 1 Alice 101 100 2 Bob 102 200 3 Carol NaN NaN NaN NaN 103 150"},{"location":"02-Data-engineering/1-basic-data-engineering/#groupby-and-aggregation","title":"Groupby and aggregation","text":"<p>Group by divides rows into groups based on column values and aggregation operations (like sum, average, or maximum) condense these groups into summary data. This allows for targeted analysis of specific segments of your data.</p>"},{"location":"02-Data-engineering/1-basic-data-engineering/#aggregation-types","title":"Aggregation types","text":"<p>Here are the most common SQL aggregation functions:</p> SQL Function Description <code>SUM</code> Adds up the values in a numeric column <code>AVG</code> Calculates the average of a numeric column <code>COUNT</code> Counts the number of rows in a column or table <code>MAX</code> Returns the maximum value in a column <code>MIN</code> Returns the minimum value in a column <code>MEDIAN</code> Finds the middle value of a numeric column <code>MODE</code> Finds the most frequently occurring value in a column <code>STDDEV</code> Calculates the standard deviation of a numeric column <code>VARIANCE</code> Calculates the variance of a numeric column <code>FIRST</code> Returns the first value in a column (order may vary by DBMS) <code>LAST</code> Returns the last value in a column (order may vary by DBMS) <p>These functions provide various ways to summarize and analyze data. Different databases might offer additional aggregation functions tailored to specific needs or data types.</p> <p>Note</p> <p>The behavior of \"FIRST\" and \"LAST\" may depend on the database management system (DBMS) and how the data is ordered in the query. In some systems, you may need to use specific ordering clauses to get the desired behavior.</p>"},{"location":"02-Data-engineering/1-basic-data-engineering/#example","title":"Example","text":"<p>Customer &amp; orders table (join between customers and orders):</p> CustomerID Name OrderID Amount 1 Alice 1 100 1 Alice 3 200 2 Bob 2 50 3 Charlie 4 150 <p>In SQL, if we want to find the total amount spent by each customer, we'll use a <code>GROUP BY</code> operation on the <code>CustomerID</code> and <code>Name</code> columns, and aggregate the <code>Amount</code> column using the <code>SUM</code> operation:</p> SQL<pre><code>SELECT CustomerID, Name, SUM(Amount) as TotalAmount\nFROM CustomerOrders\nGROUP BY CustomerID, Name;\n</code></pre> <p>Which would give:</p> CustomerID Name TotalAmount 1 Alice 300 2 Bob 50 3 Charlie 150 <p>The <code>GROUP BY</code> operation groups the rows by <code>CustomerID</code> and <code>Name</code>, and the <code>SUM</code> function calculates the total amount for each group, giving us a summarized view of the spending by each customer.</p> <p>Note</p> <p>The python/pandas equivalent of this SQL statement would be:</p> Python<pre><code>result = (\n    customer_orders\n    .groupby(['CustomerID', 'Name'])\n    .agg(TotalAmount=('Amount', 'sum'))\n    .reset_index()\n)\n</code></pre>"},{"location":"02-Data-engineering/2-data-format/","title":"Data format","text":"<p>You should be able to reshape your data according to your need. Should you use wide format or long format or none of them ?</p> <p>Let's find out.</p>"},{"location":"02-Data-engineering/2-data-format/#formats","title":"Formats","text":"<p>We describe below 4 types of format for data: raw, long, compact, wide.</p> <p>Note</p> <p>The term Compact has been defined only in the context of this course. Also the Long, Wide and Raw terms have no standardized definition in the industry. Don't use those terms as if it's something widely known in the data engineering community.</p> <p></p> Wide &amp; long format"},{"location":"02-Data-engineering/2-data-format/#raw","title":"Raw","text":"<p>In its raw form, data is usually made of payloads containing keys and values (i.e. dictionaries).</p> <p>Example when receiving payload of meteorological data with temperature and humidity (integrated sensor):</p> JSON<pre><code>[\n{\n\"Date\": \"2023-01-01\",\n\"City\": \"CityA\",\n\"Temperature\": 20,\n\"Humidity\": \"30%\"\n},\n{\n\"Date\": \"2023-01-01\",\n\"City\": \"CityB\",\n\"Temperature\": 15,\n\"Humidity\": \"25%\"\n},\n{\n\"Date\": \"2023-01-01\",\n\"City\": \"CityC\",\n\"Temperature\": 18,\n\"Humidity\": \"28%\"\n},\n{\n\"Date\": \"2023-01-02\",\n\"City\": \"CityA\",\n\"Temperature\": 21,\n\"Humidity\": \"31%\"\n},\n{\n\"Date\": \"2023-01-02\",\n\"City\": \"CityB\",\n\"Temperature\": 16,\n\"Humidity\": \"26%\"\n},\n{\n\"Date\": \"2023-01-02\",\n\"City\": \"CityC\",\n\"Temperature\": 19,\n\"Humidity\": \"29%\"\n}\n]\n</code></pre> <p>Or alternatively when receiving payloads of meteorological data with either temperature or humidity (different sensors):</p> JSON<pre><code>[\n{ \"Date\": \"2023-01-01\", \"City\": \"CityA\", \"Temperature\": 20 },\n{ \"Date\": \"2023-01-01\", \"City\": \"CityA\", \"Humidity\": \"30%\" },\n{ \"Date\": \"2023-01-01\", \"City\": \"CityB\", \"Temperature\": 15 },\n{ \"Date\": \"2023-01-01\", \"City\": \"CityB\", \"Humidity\": \"25%\" },\n{ \"Date\": \"2023-01-01\", \"City\": \"CityC\", \"Temperature\": 18 },\n{ \"Date\": \"2023-01-01\", \"City\": \"CityC\", \"Humidity\": \"28%\" },\n{ \"Date\": \"2023-01-02\", \"City\": \"CityA\", \"Temperature\": 21 },\n{ \"Date\": \"2023-01-02\", \"City\": \"CityA\", \"Humidity\": \"31%\" },\n{ \"Date\": \"2023-01-02\", \"City\": \"CityB\", \"Temperature\": 16 },\n{ \"Date\": \"2023-01-02\", \"City\": \"CityB\", \"Humidity\": \"26%\" },\n{ \"Date\": \"2023-01-02\", \"City\": \"CityC\", \"Temperature\": 19 },\n{ \"Date\": \"2023-01-02\", \"City\": \"CityC\", \"Humidity\": \"29%\" }\n]\n</code></pre> <p>Each record is dictionary made of variables which can be:</p> <ul> <li>categories: used to select and filter data or link to other table. Usually a   string or an integer.</li> <li>values: an actual measurement of a sensor. Usually a float.</li> </ul>"},{"location":"02-Data-engineering/2-data-format/#long","title":"Long","text":"<p>The easiest way to store the data in the long format using only 3 columns:</p> <ul> <li>an ID of the measurement</li> <li>the variable name</li> <li>the variable value</li> </ul> <p>The long format would look like this:</p> ID Variable Value 1 Date 2023-01-01 1 City CityA 1 Temperature 20 1 Humidity 30% 2 Date 2023-01-01 2 City CityB 2 Temperature 15 2 Humidity 25% 3 Date 2023-01-01 3 City CityC 3 Temperature 18 3 Humidity 28% 4 Date 2023-01-02 4 City CityA 4 Temperature 21 4 Humidity 31% 5 Date 2023-01-02 5 City CityB 5 Temperature 16 5 Humidity 26% <p>In this representation, the integer ID serves to group the related values for each observation, while the variable column specifies the type of information, and the value column contains the actual data.</p> <p>This format is best suited for storage:</p> <ul> <li>it is extremely flexible, as we can add more variables for each measurement without   changing the table schema</li> <li>it has a better storage efficiency when working with sparse data.</li> </ul> <p>But it can become more complex to work with, particularly for data analysis tasks.</p>"},{"location":"02-Data-engineering/2-data-format/#compact","title":"Compact","text":"<p>Usually the best format for your data is neither fully wide nor fully long. The purpose of those \"extreme\" format examples is to show that data can (and should) be reshaped depending on the use case.</p> <p>The best practice for data engineering is to format your data to have 1 column per variable. This implies:</p> <ul> <li>not wide format: don't use variables in column names</li> <li>not long format: don't mix different variables in the same column</li> </ul> <p>The compact format applied to our meteorological data example looks like this:</p> Date City Temperature Humidity 2023-01-01 CityA 20 30% 2023-01-01 CityB 15 25% 2023-01-01 CityC 18 28% 2023-01-02 CityA 21 31% 2023-01-02 CityB 16 26% 2023-01-02 CityC 19 29% <p>It naturally aligns with the structure of the data: each row represents a single observation (in a city on a particular date), and each variable (temperature and humidity) has its column.</p> <p>The compact format is best suited for data engineering:</p> <ul> <li>to filter and select the data for various criteria</li> <li>to transform variables (scaling, outliers management)</li> <li>to compute new variables (e.g: wet bulb temperature from temperature &amp; humidity)</li> </ul> <p>It's not very readable but is very convenient to work with.</p>"},{"location":"02-Data-engineering/2-data-format/#wide","title":"Wide","text":"<p>The wide format uses the categories to create the columns, it displays only values for a given index (on the left).</p> <p>Here is how the meteorological would look in the wide format:</p> Date Temperature_CityA Humidity_CityA Temperature_CityB Humidity_CityB Temperature_CityC Humidity_CityC 2023-01-01 20 30% 15 25% 18 28% 2023-01-02 21 31% 16 26% 19 29% <p>This is not very readable and usually the only reasonable use of wide format is to display data for a particular use-case:</p> <ul> <li>select a single category for the index (e.g: year)</li> <li>select a single category for the columns (e.g: city)</li> <li>select a single value to fill the cells (e.g: temperature)</li> </ul> <p>So, if we'd like to compare temperatures in various cities:</p> Date Temperature_CityA Temperature_CityB Temperature_CityC 2023-01-01 20 15 18 2023-01-02 21 16 19 <p>The wide format is best suited for visualization.</p>"},{"location":"02-Data-engineering/2-data-format/#summary","title":"Summary","text":"Format Description Best Suited For Example Characteristics Raw Dictionaries containing keys and values (e.g., JSON). Initial data storage Separate dictionaries for each record Long 3 columns: ID, variable, value. Storage (sparse data &amp; flexible schema) Each variable is a separate row Compact 1 column per variable. Data engineering Each row represents a single observation Wide Categories define columns to spread a variable. Visualization Column names include categories, like <code>Temperature_CityA</code>"},{"location":"02-Data-engineering/2-data-format/#reshaping-data-format","title":"Reshaping data format","text":"<p>A good data scientist/analyst/engineer should be able to easily reshape and reformat data from one format to another, depending on the need.</p> <p>Let's see how to do this in python.</p> <p>Note</p> <p>The transformation are described below as Long -&gt; Wide (or Wide -&gt; Long), but depending on the number variables transformed from rows to columns (or from columns to rows), this could be seen as a (not full) transformation like Long -&gt; Compact and Compact -&gt; Wide (or Wide -&gt; Compact and Compact -&gt; Long).</p> <p>Note</p> <p>This section is a subset of the excellent article \"Reshaping and pivoting tables\" from the pandas documentation. It is highly recommended to check it out.</p>"},{"location":"02-Data-engineering/2-data-format/#long-wide","title":"Long -&gt; Wide","text":"<p>This operation aims at transferring some data from columns to rows. It takes:</p> <ul> <li>1+ variable(s) to define an index (i.e. the rows)</li> <li>1+ variable(s) to create columns</li> <li>1 variable to fill the created cells.</li> </ul> <p>To do this, there are 2 functions available in pandas: <code>pivot</code> and <code>pivot_table</code></p> <p></p> <p>In <code>pivot</code>, the variables used to define the index and the columns should define a unique value per cell of the dataframe. In other words, those index &amp; columns variables should be a primary key of the input (long) dataframe</p> <p>Whereas <code>pivot_table</code> is able to handle multiple values per cell and to aggregate them, using the argument <code>aggfunc</code> (for aggregation function such as: sum, mean, min, max, count, and so on). You can even use external aggregation functions:</p> <ul> <li>such as the   numpy statistics</li> <li>or the   scipy statistics</li> <li>or even your own code.</li> </ul> <p>In fact, this aggregation capability is its main purpose: <code>pivot_table</code> is a convenient alternative to the use of the <code>groupby</code> &amp; <code>agg</code> methods.</p>"},{"location":"02-Data-engineering/2-data-format/#wide-long","title":"Wide -&gt; Long","text":"<p>This operation aims at transferring some data from rows to columns. It takes 1+ variable(s) to be kept as index and uses the remaining variables to be put in 2 columns (usually named <code>variable</code> and <code>value</code>).</p> <p>To do this, there are 2 functions available in pandas: <code>melt</code> and <code>wide_to_long</code></p> <p></p> <p>The <code>melt</code> function basically lets you specify the column to be kept as index.</p> <p>Whereas the <code>wide_to_long</code> function, even if it uses <code>melt</code> under the hood, is helpful if you need to extract categorical values from the column names using a regex pattern, rather than using the full column name.</p>"},{"location":"02-Data-engineering/2-data-format/#summary_1","title":"Summary","text":"Transformation Direction Functions Long -&gt; Wide Columns to Rows <code>pivot</code>: Requires unique value per cell.<code>pivot_table</code>: Aggregates multiple values per cell. Wide -&gt; Long Rows to Columns <code>melt</code>: Only specifies index column.<code>wide_to_long</code>: Extracts categorical values using regex."},{"location":"02-Data-engineering/3-practice/","title":"\ud83d\udc0d Practice","text":"<p>Links to tutorial:</p> <ul> <li>Students version</li> <li>Full version</li> </ul>"},{"location":"03-Big-data/","title":"03 Big data","text":"<ul> <li>Context</li> <li>Data processing history</li> <li>Storing data at scale</li> <li>Transforming data at scale</li> <li>Databases types (SQL / NoSQL)</li> <li>Case study: digital transformation</li> </ul>"},{"location":"03-Big-data/1-context/","title":"Context","text":""},{"location":"03-Big-data/1-context/#digitalization","title":"Digitalization","text":""},{"location":"03-Big-data/1-context/#networking","title":"Networking","text":"<p>Cloud computing  Broadband  Mobile internet</p>"},{"location":"03-Big-data/1-context/#digital-data-customer-access","title":"Digital data &amp; Customer access","text":"<p>Internet of Things (IoT) and wearables Big data analytics technologies  Social networks</p>"},{"location":"03-Big-data/1-context/#automation","title":"Automation","text":"<p>Robotics (drones, etc.) Additive layer manufacturing Sensor technologies</p> <p></p>"},{"location":"03-Big-data/1-context/#size-of-data-sphere-grows","title":"Size of data sphere grows","text":"<ul> <li>Upward trend in data being collected and owned across the board (connected products, wireless broadband technologies, etc.)</li> <li>Data growth and ownership is unequally distributed (e.g. consumer internet vs non-digital native companies)</li> </ul>"},{"location":"03-Big-data/1-context/#one-definition-of-big-data","title":"One definition of big data","text":"<p>Big data is high-Volume, high-Velocity and/or high-Variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision making, and process automation.</p> <p>Gartner</p>"},{"location":"03-Big-data/1-context/#what-is-a-big-data-problem","title":"What is a (big) data problem?","text":"<ul> <li>Better marketing and segmentation</li> <li>Early detection of quality defects in industry (automotive, aerospace, etc.)</li> <li>Anti money laundering activity in financial services</li> <li>Supply chain and logistics disruption management</li> <li>Measuring CO2 footprint with data </li> <li>COVID-19 response (medical supply chain, hospitals capacity management)</li> <li>Law enforcement, counter terrorism, counter narcotics, etc.</li> </ul>"},{"location":"03-Big-data/2-data-processing-history/","title":"Data processing history","text":""},{"location":"03-Big-data/2-data-processing-history/#mid-1990s","title":"Mid 1990s","text":"Data infrastructure in the mid 1990s <ul> <li>Web application writes to an Relational Database Management System (very good at transactional processing)</li> <li>Example of transactional processing: </li> <li>conceptually think of an ATM (Auto Teller Machine) where you are mostly concerned about integrity. In that case the transaction is a single event where something get subtracted at one place and added to another place</li> <li>Very good to power application on the web but less good for powering analytics at scale (with multiple RDBMSs and web applications)</li> </ul>"},{"location":"03-Big-data/2-data-processing-history/#early-2000s","title":"Early 2000s","text":"Data infrastructure in the early 2000s <ul> <li>ETL = Extract, Transform and Load. It extracts data from RDBMS or transactional system, transform it through more or less complex business logics (aggregations, filters, joins, etc.) and loads it into a data warehouse or BI (business intelligence) tool. </li> <li>Example: think of a user facing application relying on a transactional system for managing trips reservations. If you want to see historical trends of bookings, prices at different point in time, you need to collect snapshots of the data everyday that were present, you can use an ETL + BI processing. </li> <li>Data stored in the form of cubes of data to make processing faster but scaling issues at some point in time. Difficulties to handle high loads </li> </ul> Schema of a data cube"},{"location":"03-Big-data/2-data-processing-history/#late-2000s","title":"Late 2000s","text":"Data infrastructure in the late 2000s <ul> <li>Data warehouses systems like Vertica or Teradata where build to write SQL like queries to data</li> <li>Not real strong visualization tools with these data warehouse systems </li> <li>Tableau and others came in to provide a visualization layer on top of data warehousing technologies</li> </ul>"},{"location":"03-Big-data/2-data-processing-history/#and-then","title":"And then...","text":"The growing trend of big data (source: Google trends) <ul> <li>Explosion of volumes of data in general</li> <li>Explosion of unstructured data (images, video, audio, etc.) </li> <li>Limits of existing stack required new technologies including (not limited to)</li> <li>NoSQL storage (better suited for large amount of data)</li> <li>Distributed storage (HDFS)</li> <li>Parallel computation frameworks (Hadoop / Spark)</li> </ul>"},{"location":"03-Big-data/2-data-processing-history/#mid-2010s","title":"Mid 2010s","text":"Data infrastructure in the mid 2010s <ul> <li>Data warehouses had difficulty to scale with very high loads. </li> <li>Google realized early the limits of vertical scalability. HDFS came in with possibility to scale horizontally with the possibility to distribute load across multiple machine.</li> <li>HDFS could be interrogated using Hive/Presto types of framework </li> <li>Hadoop and Spark used for computation transformation at scale</li> </ul>"},{"location":"03-Big-data/2-data-processing-history/#now-2020s","title":"Now (2020s)","text":"Data infrastructure in the 2020s <ul> <li>Massive amount of unstructured and sensor data</li> <li>General complexification of the landscape </li> <li>Notebooks became very popular and data scientist came into the picture. </li> <li>Notebook is probably the most popular interface data scientists use to train ML models </li> <li>Multiple roles in the organization to make it work</li> </ul>"},{"location":"03-Big-data/2-data-processing-history/#what-the-world-looks-like","title":"What the world looks like","text":"Big data landscape in 2020 <p>Note: never use this landscape as it was a peer-reviewed source of truth.</p>"},{"location":"03-Big-data/2-data-processing-history/#generic-data-stack","title":"Generic data stack","text":"Generic data stack <ul> <li>You will find very complex descriptions of data stacks but most data ecosystems are boiling down to a flavor of that architecture</li> <li>Large commercial offerings can cover and traverse the layers on that slide (e.g. AWS, Microsoft, Google Cloud)</li> <li>The (Analytical) data store block can be pretty thick and involve different techno/applications (from resource management, data storage, orchestration, batch and real time ingestion to some analytical capabilities)</li> <li>In this course we will make a focus on data storage (HDFS) and transformation engine (with Spark)</li> </ul>"},{"location":"03-Big-data/3-storing-data-at-scale/","title":"Storing data at scale","text":"<p>An example with HDFS</p>"},{"location":"03-Big-data/3-storing-data-at-scale/#its-not-all-about-software","title":"It's not all about software","text":"A data center: the physical instanciation of the cloud How the internet travels across oceans."},{"location":"03-Big-data/3-storing-data-at-scale/#vertical-horizontal-scaling","title":"Vertical &amp; horizontal scaling","text":"<ul> <li>Limits to vertical scaling (Yahoo and Google experienced that early): You can\u2019t just make a server bigger and bigger without reaching limits</li> <li>Horizontal scaling enables to circumvent these limitations by distributing storage of data and compute (transformation of data) across multiple machines</li> <li>Horizontal scaling requires specific technologies to manage a number of challenges</li> <li>Fault tolerance (probability of losing a node is higher)</li> <li> <p>Splitting computation tasks across multiple machines</p> </li> <li> <p>But horizontal scaling is no panacea: it brings additional complexity and overhead, and should be seen only as a last resort. There are other options to consider first.</p> </li> </ul>"},{"location":"03-Big-data/3-storing-data-at-scale/#hadoop-hdfs","title":"Hadoop &amp; HDFS","text":"<ul> <li>Hadoop is the name of an entire ecosystem dedicated to the management of big data</li> <li>HDFS means Hadoop Distributed File System and is a technology to store data at scale</li> <li>HDFS is a file management system, not a database </li> </ul>"},{"location":"03-Big-data/3-storing-data-at-scale/#hadoop-distributed-file-system-hdfs-a-solution-for-distributing-storage","title":"Hadoop Distributed File System (HDFS): a solution for distributing storage","text":"<ul> <li>Reasons for storing across multiple files</li> <li>If data is big (billions of rows), data may not fit in a single file / hard drive disk, we can distribute the way data is stored</li> <li>Offer parallelism when we want to compute things (eg. Filtering billions of rows as part of a single processing)</li> <li>Data storage is in plain text file and is distributed across multiple files and machines </li> <li>Files can be in different formats such as txt but other formats are allowing for more compression (parquet)</li> </ul> HDFS architecture. <ul> <li>If we want to store more data, we can simply add more machines to the cluster that we have instead of buying a larger hard drive </li> <li> <p>In modern implementation service of HDFS (i.e. AWS S3), the complexity of adding more resources is abstracted away from the users. We just ask AWS for more storage space</p> </li> <li> <p>HDFS is made up of nodes which can be physical machines or virtual machines</p> </li> <li>Name node has full visibility on data distribution, handle fault tolerance, data distribution and replication across data nodes, etc.</li> <li>This is a typical <sub>master/slave</sub> controller/worker type of framework </li> <li>Data blocks are going to be distributed across nodes in a way that is fault tolerant </li> </ul>"},{"location":"03-Big-data/4-transforming-data-at-scale/","title":"Transforming data at scale","text":"<p>Example with Spark</p>"},{"location":"03-Big-data/4-transforming-data-at-scale/#the-challenge-of-distributed-computation","title":"The challenge of distributed computation","text":"<ul> <li>Distributed computation allows for increased computation speed</li> <li>Several machine work in parallel on a given job and data is spread across nodes/machines (horizontal scaling)</li> <li>When data gets bigger, you don\u2019t necessarily need more memory / CPU on each node but you can spin up more nodes</li> <li>This comes at a cost: there is a close relationship between the logic executed and the physical distribution </li> </ul>"},{"location":"03-Big-data/4-transforming-data-at-scale/#what-is-spark","title":"What is Spark?","text":"<ul> <li>Spark is a big data analytics engine siting above a distributed data store where data can be loaded in memory, distributed and analyzed in parallel across the nodes of the cluster</li> <li>Spark sits on top of:</li> <li>A distributed file system (usually HDFS or other commercial implementation of it such as AWS S3) </li> <li>A resource manager that is going to manage cluster resources (YARN, Apache Mesos, etc.). Resource manager allocation is usually dynamic and cluster scaling is static (in a cluster, the RM solves the problem of: optimally distribute 256v Cores and 256Gbs of RAMs to 3 jobs)</li> <li>[Not covered but nice to know] Increasingly clusters are managed through containerization with Kubernetes; Kubernetes can spin up container to execute spark jobs. This provides increased ability to isolate jobs (increased security), resources and enable auto scaling (in the cloud). This makes it possible to make resource allocation static but cluster scaling dynamic.</li> </ul>"},{"location":"03-Big-data/4-transforming-data-at-scale/#spark-infrastructure","title":"Spark infrastructure","text":"Spark infrastructure"},{"location":"03-Big-data/4-transforming-data-at-scale/#interacting-with-data-through-spark","title":"Interacting with data through Spark","text":"Spark APIs <ul> <li>Spark works conceptually similarly to MapReduce (which is part of the Hadoop ecosystem) but it runs in memory which makes it 100x faster. It progressively replace MapReduce. </li> <li>Apache Spark Core APIs are: R, SQL, Python, Scala and Java (PySpark will illustrate a few examples) </li> <li>Apache Spark provides a set of 3 abstractions to work with data, from the lowest to the higher level of abstraction:</li> <li>Resilient Distributed Dataset (Lowest level API, fundamental abstraction),</li> <li>Dataframe</li> <li>Datasets</li> </ul>"},{"location":"03-Big-data/4-transforming-data-at-scale/#few-things-about-resilient-distributed-dataset","title":"Few things about Resilient Distributed Dataset","text":"<ul> <li>Spark doesn\u2019t see the world in terms of files, it reads them and create RDD from it (=! from HDFS storage, RDDs don\u2019t work with the files stored in HDFS, it reads them and creates RDD from it)</li> <li>An RDD is a group of data elements called partitions that can be queued for parallel tasks.</li> <li>Spark cannot parallelize computation inside partitions --&gt; they are the atom of the compute job</li> <li>You can think of an RDD as a logical listing of data location on the cluster (i.e. which partition is where in memory)</li> <li>RDDs are resilient &amp; immutable: as you transform data and RDD and go from A to B, a \u201drecipe\u201d of the transformation is created so that if something goes wrong, you can recreate your RDD at any point in time</li> <li>Gives you flexibility on schema / typing but it requires better coding ability vs higher level APIs. It can cause some inefficiencies as Spark won\u2019t optimize things for you.</li> </ul>"},{"location":"03-Big-data/4-transforming-data-at-scale/#dataframes-and-datasets-apis","title":"Dataframes and Datasets APIs","text":"<ul> <li> <p>When coding in pySpark of Spark SQL, you generally don\u2019t directly handle RDD, you handle dataFrames or datasets, handling RDD is done under the hood by Spark for you.</p> </li> <li> <p>DataFrames are an abstraction of RDD, it only works with structured and semi-structured data. DataFrames are organized into named columns and are the conceptual equivalent of tables in a relational database. It allows Spark to manage schema.</p> </li> <li> <p>Spark provides a rich API to interact with DataFrames which enables Spark to perform extra optimization under the hood for you</p> </li> </ul>"},{"location":"03-Big-data/4-transforming-data-at-scale/#how-spark-parallelizes-computation","title":"How Spark parallelizes computation","text":"Spark job parallelism <ul> <li>Spark job is split into stages </li> <li>Each stages process a list of tasks which are performed by Executors in memory</li> <li>Tasks are mapping 1:1 with RDD partitions, this is what gets parallelized</li> </ul>"},{"location":"03-Big-data/4-transforming-data-at-scale/#example-of-processing","title":"Example of processing","text":"Username Model Make Price Nicolas Daveau Macbook Pro 15' 2018 Apple 2200$ Quentin Chenevier Surfacebook 2019 Microsoft 1700$ Laurent Lapasset Macbook Pro 13' 2019 Apple 1400$ ... ... ... ... Python<pre><code>import spark.sql.functions as F\n\ndf = df.groupBy(\"Make\").agg(F.sum(\"Price\")) \n</code></pre> Spark logical plan example Spark physical plan example"},{"location":"03-Big-data/4-transforming-data-at-scale/#types-of-operations-is-it-wide","title":"Types of operations: Is it wide ?","text":"<ul> <li>Filter</li> <li>Aggregate</li> <li>Windowing</li> <li>Cast type</li> <li>Join</li> <li>Explode</li> <li>Column concatenation</li> </ul>"},{"location":"03-Big-data/5-databases-types/","title":"Databases types (SQL / NoSQL)","text":""},{"location":"03-Big-data/5-databases-types/#different-types-of-databases-systems-sql-vs-nosql","title":"Different types of databases systems: SQL vs NoSQL","text":"SQL vs NoSQL databases <ul> <li>Relational databases are less suitable for horizontal scaling than NoSQL databases </li> <li>Joins become very inefficient when performed across nodes and difficulty to keep ACID (Atomic, Consistent, Isolated, Durable) guarantees while ensuring all replicas have fresh data</li> <li>READ can scale easily, EDIT is more challenging. Think of ensuring the integrity of a transaction (locking a row to ensure no concurrent access happens) if a data is being stored on multiple nodes/servers</li> <li>NoSQL are giving up on a number of requirements, are easier to scale horizontally (less immediate consistency, data freshness and accuracy in order to gain other benefits, like scale and resilience). NoSQL are designed to ensure high availability across cluster which means consistency/durability can be sacrificed to some degree</li> </ul>"},{"location":"03-Big-data/5-databases-types/#acid-base-properties","title":"ACID &amp; BASE properties","text":"ACID &amp; BASE properties <p>ACID:</p> <ul> <li>Atomicity : means that you guarantee that either all of the transaction succeeds or none of it does. </li> <li>Consistency : All clients see the same data at the same time . Data will be valid according to all defined rules, including any constraints, cascades, and triggers that have been applied on the database. </li> <li>Isolation No transaction will be affected by any other transaction. So a transaction cannot read data from any other transaction that has not yet completed.</li> <li>Durability means that, once a transaction is committed, it will remain in the system \u2013 even if there\u2019s a system crash immediately following the transaction. Any changes from the transaction must be stored permanently. </li> </ul> <p>BASE:</p> <ul> <li>Basically Available: Data are available as prescribed by the CAP theorem (Consistency, Availability, Partitioning Tolerance, chose 2 only). Query response can be inaccurate but system availability guaranteed</li> <li>Soft state: Data consistency is developer managed</li> <li>Eventually consistent: Data can be inconsistent at time t but converges towards consistency</li> </ul>"},{"location":"03-Big-data/5-databases-types/#sql-example-rdbms","title":"SQL: example RDBMS","text":"<p>Users table:</p> User_id User Material Id 1 Nicolas Daveau 1 2 Quentin Chenevier 2 3 Laurent Lapasset 3 <p>Laptop inventory table:</p> Material id Model Make 1 Macbook Pro 15' 2018 Apple 2 Surfacebook 2019 Microsoft 3 Macbook Pro 13' 2019 Apple <ul> <li>Data is organized in 2 dimensional tables interlinked by relations.</li> <li>Advantages: transparent data structure, Structured Query Language, major RDBMSs adhere to ACID </li> <li>Disadvantages: Scalability issues, challenge to handle large amount of data</li> <li>Applications: transactional system/workflows requiring high level of integrity / consistency</li> <li>Example of technos: AWS DynamoDB, Voldemort, Oracle NoSQL Database</li> </ul>"},{"location":"03-Big-data/5-databases-types/#cap-theorem","title":"CAP Theorem","text":"CAP Thorem <p>CAP theorem: \"You can only pick two\":</p> <ul> <li>CP Category: There is a risk of some data becoming unavailable.</li> <li>CA Category: Network problem might stop the system</li> <li>AP Category: Clients may read inconsistent data</li> </ul>"},{"location":"03-Big-data/5-databases-types/#nosql-example-key-value-database","title":"NoSQL: Example key-value database","text":"Key Value User_1 <code>{\"name\": \"Nicolas Daveau\", \"laptop\":\"Macbook Pro 15' 2018\", \"make\": \"Apple\"}</code> User_2 <code>{\"name\": \"Quentin Chenevier\", \"laptop\":\"Surfacebook 2019\", \"make\": \"Microsoft\"}</code> User_3 <code>{\"name\": \"Laurent Lapasset\", \"laptop\":\"Macbook Pro 13' 2019\", \"make\" : \"Apple\"}</code> <ul> <li>Data is stored in key/value pairs </li> <li>Advantages: Schema-less way and is very efficient for read and write operations. Efficient in dealing with high load, values can be stored in the form of a JSON blob, a string, etc.</li> <li>Disadvantages: Not optimized for lookups (need to scan whole collection) and a parser is needed to store multiple values </li> <li>Applications: storing users preference for a website, storing a shopping cart, real time adds recommendations</li> <li>Example of technos: AWS DynamoDB, Voldemort, Oracle NoSQL Database</li> </ul>"},{"location":"03-Big-data/5-databases-types/#nosql-example-column-oriented-database","title":"NoSQL: Example Column Oriented Database","text":"ID Name 01 Nicolas Daveau 02 Quentin Chenevier 03 Laurent Lapasset ID Model 01 Macbook Pro 15' 2018 02 Surfacebook 2019 03 Macbook Pro 13' 2019 ID Make 01 Apple 02 Microsoft 03 Apple <ul> <li>Every column is treated separately. Values of single column databases are stored contiguously.</li> <li>Advantages: Very efficient for reading and aggregating data (Sum, Mean, etc.) because no need to read all columns. </li> <li>Disadvantages: Less efficient for writing tuples \ud83e\udc6a requires multiple accesses. </li> <li>Applications: column-based NoSQL databases are widely used to manage data warehouses, business intelligence, CRM, etc.</li> <li>Example of technos: HBase, Cassandra, Google big tables</li> </ul>"},{"location":"03-Big-data/5-databases-types/#nosql-example-document-oriented-database","title":"NoSQL: Example Document Oriented Database","text":"Document 1 <code>{\"name\": \"Nicolas Daveau\", \"laptop\": \"Macbook Pro 15\u2019 2018\", \"make\": \"Apple\"}</code> Document 2 <code>{\"name\": \"Quentin Chenevier\", \"laptop\": \"Surfacebook 2019\", \"make\": \"Microsoft\"}</code> Document 3 <code>{\"name\": \"Laurent Lapasset\", \"laptop\": \"Macbook Pro 13\u2019 2019\", \"make\" : \"Apple\"}</code> <ul> <li>Data is retrieved as key value pair but the value part is stored as a documents. Documents are usually stored in a semi-structured data formal like JSON or XML.</li> <li>Advantages: schema less and flexible, ease to integrate additional information with low maintenance burden.</li> <li>Disadvantages: managing relationships between document is cumbersome, aggregations can also be more complex to handle</li> <li>Applications: Yelp / Tripadvisor type of use cases, book database, etc.</li> <li>Example of technos: MongoDB, Marklogic, Amazon DocumentDB</li> </ul>"},{"location":"03-Big-data/5-databases-types/#example-graph-oriented-database","title":"Example Graph Oriented Database","text":"Graph database example <ul> <li>A graph database stores entities as nodes and the relationships are represented through edges of the graph. Every node has a unique identifier.</li> <li>Advantages: Graph databases are by nature multi-relational and loosely connected. Traversing relationships is fast as it is captured into the data storage. Well suited for powering analytics (similarity analytics, etc.)</li> <li>Disadvantages: Difficult to scale on large amount of data (difficult to scale horizontally), queries can be very expensive, no uniform query language.</li> <li>Applications: Social network analytics, logistics, semantics search, intelligence, fraud detection.</li> <li>Example of technos: Neo4J, Infinite Graph, OrientDB</li> </ul>"},{"location":"03-Big-data/6-case-study-digital-transformation/","title":"Case study: digital transformation","text":""},{"location":"03-Big-data/6-case-study-digital-transformation/#case-study-airline","title":"Case Study - Airline","text":"<p>Need:</p> <ul> <li>Pricing and network strategy</li> <li>Improved marketing efforts</li> <li>Personalized offering</li> <li>Improved maintenance</li> <li>Improved flight operations</li> <li>Improved safety</li> </ul> <p>Solution:</p> <ul> <li>What are the sources of data ? </li> <li>What are the use cases ?</li> <li>What technologies do you need?</li> </ul>"},{"location":"03-Big-data/6-case-study-digital-transformation/#case-study-car-manufacturer","title":"Case Study \u2013 Car Manufacturer","text":"<p>Need:</p> <ul> <li>Improved marketing efforts</li> <li>Personalized offering</li> <li>Improved quality &amp; root cause analysis</li> <li>Improved engineering and design</li> <li>Manufacturing optimization</li> </ul> <p>Solution:</p> <ul> <li>What are the sources of data ? </li> <li>What are the use cases ?</li> <li>What technologies do you need?</li> </ul>"},{"location":"04-Data-visualization/","title":"04 Data visualization","text":"<ul> <li>Fundamentals</li> <li>Plots</li> <li>Exploratory data analysis</li> <li>\ud83d\udc0d Practice</li> </ul>"},{"location":"04-Data-visualization/1-fundamentals/","title":"Fundamentals","text":"<p>Creating powerful data visualitzaion might seem to be a difficult art, learnt through years of practice. But still there is method which can help to do basic.</p> <p>This course has 2 objectives:</p> <ul> <li>being able to \"explain\" the content of a dataset to a functional expert - usually the \"customer\" of the data scientist. This is the data story telling.</li> <li>being able to explore a dataset looking for powerful predicting variables to solve a prediction problem. This is the exploratory data analysis.</li> </ul> <p>To do so, this course provides basic knowledge about which kind of plot is suitable for each data type. </p>"},{"location":"04-Data-visualization/1-fundamentals/#levels-of-measurement-and-data-types","title":"Levels of measurement and data types","text":"<p>To classify data, the most used typology is Stevens' typology (1946):</p> Measurement Property Math. operations Advanced Operations Data Type Key idea Nominal Classification =, \u2260  (equality) Grouping String \"I can make groups\" Ordinal Comparison &gt;, &lt;  (inequality) Ranking Integer \"I can order stuff\" Interval Difference +, \u2212  (addition) Deviation from a standard Float \"I can measure differences\" Ratio Magnitude x, /  (multiplication) Ratio between values Float \"There is an absolute zero\" <p>Measurements types do not contain the same level of information. There is a - kind of - inclusion relationship between them:</p> <p>Nominal(group) &lt; Ordinal(order) &lt; Interval(relative) &lt; Ratio(absolute)</p> <p>E.g: You can convert an ordinal measurement to a nominal, but the opposite is often impossible without making an hypothesis.</p> <p>To be noted that, even if this typology is widely used because it's simple, it is heavily criticized. Stevens was a psychologist and his typology is an oversimplification of the diversity of measurement types we can find in nature. More exhaustive - and complex - typologies have been proposed, see the note below.</p> More information about measurement types <p>Scribbr - Levels of measurement</p> <p>Wikipedia - Level of measurement</p>"},{"location":"04-Data-visualization/1-fundamentals/#exercises","title":"Exercises","text":"<p>It's time to consolidate your understanding of the measurement types concept with the exercises below.</p> <p>Tip: Put your mouse over the question mark to display the answer: \u2753</p> <p>Exercise 1: For each measurement type, find if it's a quantitative or qualitative measurement.</p> Measurement Qualitative or Quantitative Nominal \u2753 Ordinal \u2753 Interval \u2753 Ratio \u2753 <p>Exercise 2: Find what is the measurement level for each data example</p> Data Example Measurement name Julie, Quentin, Hakim, Marta \u2753 country France, USA, Marocco, Spain \u2753 age 27, 35, 12, 3 \u2753 marathon ranking 13th, 1st, 2nd, 2037th \u2753 date 2020-01-01, 2022-03-27, 1977-12-04 \u2753 time duration 27s, 34m 12s, 1h 07m 01s \u2753 temperature \u00b0C 41 \u00b0C, 12 \u00b0C, -21 \u00b0C \u2753 temperature K 273.15 K, 2500 K, 0.2 K, 500 nK \u2753 app rating \u2b50, \u2b50\u2b50\u2b50\u2b50\u2b50 \u2753 USA school rating F, A+, B+, C- \u2753 french school rating 0, 5, 20, 12 \u2753 S&amp;P countries credit rating CCC, A+, BBB-, AAA \u2753 salary 25k\u20ac, 50k\u20ac, 150k\u20ac, 400k\u20ac, 8M\u20ac \u2753 money transfer +40 \u20ac, -127 \u20ac, +150k\u20ac, -20000\u20ac \u2753 images \u2753"},{"location":"04-Data-visualization/1-fundamentals/#visual-variables","title":"Visual variables","text":"<p>Jacques Bertin, a cartographer, introduced a basic set of visual variables (1967). A visual variable is \"the differences in graphical elements as perceived by the human eye\". In other words, they are visual properties which we can perceive.</p> <p>In other words, a visual variable is a way to display your (non-visual) data. Data visualization is the activity of encoding data on visual variables.</p> <p>The most used visual variables in cartography are position, shape, size, hue, value, texture and orientation (visually described by the figure below).</p> <p>  7 visual variables and their description (from Towards Visualization Recommendation ... [Kaur]) </p> <p>In practice, in data science, you'll use only the first 5 visual variables. There is also a - kind of - ordering relationship, in terms of expression power, between them:</p> <p>Position &gt; Size &gt; Shape &gt; Value &gt; Hue</p> <p>Position is the most easy to use visual property and can be used to display any kind of data. Color value and hue are only slightly useful in a graph.</p> More information on visual variables <p>Visualization of geographical data [Victor Olaya]</p> <p>Visual variables [Roth]</p>"},{"location":"04-Data-visualization/1-fundamentals/#visual-properties","title":"Visual properties","text":"<p>Visual variables can have four basic properties, which are related to the measurement types:</p> <ul> <li>Associative: Values can be grouped together. Suitable to represent nominal variables.</li> <li>Selective. Values from a group can be isolated from the other groups. Suitable to represent nominal variables.</li> <li>Ordered. Values show a linear order. Suitable to represent ordinal variables</li> <li>Quantitative. Values can be directly measured. Suitable to represent quantitative (interval &amp; ratio) variables.</li> </ul> <p>There is one big catch to representing quantitative variables though: you have to represent the zero value for ratio variables, because it has a meaning. This is the most common mistake in data visualization.</p> <p>Bertin has analyzed the properties of the visual variables:</p> <p>  Visual variables and their properties (from Visual variables [Axis maps]) </p> <p>Those properties emerge when our visual cortex in our brain does the visual information processing. It's a perceptual phenomenon and there is a subjective dimension here.</p> More information on visual properties <p>Visual variables [Axis maps]</p>"},{"location":"04-Data-visualization/1-fundamentals/#from-measurement-type-to-visual-variable","title":"From measurement type to visual variable","text":"<p>A professor recently proposed a refinement &amp; simplification of this analysis, doing a mapping from the visual variable to the variable type directly:</p> <p>  Visual variables and their syntactics (from Visual variables [Roth]) </p> <p>In the context of data science, we can even more simplify this analysis, since we rarely use texture or orientation:</p> Position Size Shape Value Hue Nominal \u2705 \u2754 \u2705 \u274c \u2705 Ordinal \u2705 \u2705 \u274c \u2705 \u2754 Interval \u2705 \u2705 \u274c \u2754 \u2754 Ratio \u2705 \u2705 \u274c \u2754 \u2754 <p> Legend: \u2705=Perfect for it, \u2754=Try it but don't expect much, \u274c=Don't even try <p>Measurement types &amp; visual variables,simplified in the context of data visualization for data science.(Maybe the one and only table you need to remember from this course) </p>"},{"location":"04-Data-visualization/1-fundamentals/#exercise","title":"Exercise","text":"<p>Exercise: Pick one bad visualization from tumblr.com/badvisualisations and explain what is wrong with the visualization.</p>"},{"location":"04-Data-visualization/2-plots/","title":"Plots","text":"<pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[1]: <p>What we learn from this plot is that there might be a quadratic relationship between $ X $ and $ Y $ like: $ Y = a X^{2} + b X + c $.</p> <p>Let's try to learn something by adding more information on this chart, mapping another data column on visual properties:</p> Visual variable Data Position (x) Horsepower (Ratio) Position (y) Miles per gallon (Ratio) Color hue Cylinders (Ordinal) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[2]: <p>We can improve this chart by using redundancy, i.e. several visual variables used to represent only a single column of data:</p> Visual variable Data Position (x) Horsepower (Ratio) Position (y) Miles per gallon (Ratio) Color hue Cylinders (Ordinal) Shape Cylinders (Ordinal) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[3]: <p>There is a slight issue of overplotting: the information is saturated due to elements being on top of each other. One solution is to use a bit of transparency.</p> Visual variable Data Position (x) Horsepower (Ratio) Position (y) Miles per gallon (Ratio) Color hue Cylinders (Ordinal) Shape Cylinders (Ordinal) Opacity 25% <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[4]: <p>We now see that most cars are in the long-distance driving region, with low consumption &amp; low power.</p> <p>Can we learn something by adding even more information ?</p> <p>Below we try to map another data column on a supplementary visual variable:</p> Visual variable Data Position (x) Horsepower (Ratio) Position (y) Miles per gallon (Ratio) Color hue Cylinders (Ordinal) Shape Cylinders (Ordinal) Opacity 25% Size Weights in lbs (Ratio) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[5]: <p>The plot is barely readable. \ud83d\ude15</p> <p>It is saturated due to overplotting, so we are not able to perceive the relationship between the new columns and the other. Also it's too complex: we are trying to show 4 data columns at once, it's often difficult to represent more than 3 information at once in an understandable way.</p> <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[6]: <p>But the result is often barely readable due to overplotting, even if we use transparency, due to points being drawn on a few lines (1 for each discrete value of the data).</p> <p>The strip plot is a simple chart composed of vertical lines instead of points to display information:</p> Visual variable Data Position (x) Horsepower (Ratio) Position (y) Cylinders (Ordinal) Opacity 25% <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[7]: <p>It's a bit more readable than the scatter plot and very compact, but It is rarely used. For teaching purposes, it allows to understand what how you can encode a mix of discrete and quantitative data on position as a visual parameter: the trick is to draw lines instead of points.</p> <p>Note that we can also use redundancy to improve the readability of the chart:</p> Visual variable Data Position (x) Horsepower (Ratio) Position (y) Cylinders (Ordinal) Color Cylinders (Ordinal) Opacity 25% <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[8]: Visual variable Data Position (x) Horsepower (Ratio) binned Size Count (aggregate for each bin) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[9]: <p>You can try also to encode a discrete variable on color as a secondary visual variable, which is called a stacked bar chart:</p> Visual variable Data Position (x) Horsepower binned (Ordinal) Size Count (for each bin) Color Cylinders (Ordinal) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[10]: <p>It gives an idea of the distribution of values in each class of the discrete data, but it's difficult to do a precise assessment: you can't compare the distributions in a stacked bar chart. The overall distribution is still understandable, though.</p> <p>To get a precise view of each distribution and to compare them, it's better to separate them using the position as a visual variable:</p> Visual variable Data Position (x) Horsepower binned (Ordinal) Size Count (for each bin) Color Cylinders (Ordinal) Position (y) - <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[11]: <p>But you lose the representation of the overall distribution.</p> Visual variable Data Position (x) Horsepower binned (Ordinal) Position (y) Miles per gallon binned (Ordinal) Size Count (Ratio) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[12]: <p>Which you can also improve using redundancy:</p> Visual variable Data Position (x) Horsepower binned (Ordinal) Position (y) Miles per gallon binned (Ordinal) Size Count (Ratio) Color Count (Ratio) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[13]: <p>When the count is encoded on the color of the points, it's a heatmap plot</p> Visual variable Data Position (x) Horsepower binned (Ordinal) Position (y) Miles per gallon binned (Ordinal) Color Count (Ratio) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[14]: <p>In certain cases, the scatter plot is not really relevant, as for the plot below, which shows the mean consumption for all car released a given year time, and breakdown by number of cylinders. Can you guess what's wrong with this chart?</p> Visual variable Data Position (x) Year (Ordinal) Position (y) Horsepower mean (Ratio) Color hue Cylinders (Ordinal) Shape Cylinders (Ordinal) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[15]: <p>This temporal relationship between data points can be represented using a line chart. A line chart uses small segments which have 2 additional visual variables - size &amp; slope - which are powerful to perceive a variation rate.</p> Visual variable Data Position (x) Year (Ordinal) Position (y) Horsepower mean (Ratio) Size (of segments) Horsepower mean variation (Interval) Slope (of segments) Color hue Cylinders (Ordinal) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[16]: <p>We can refine this chart by encoding the count of cars for each year and cylinder class, as we made a mean aggregate. This is a ratio data which can be shown using the size:</p> Visual variable Data Position (x) Year (Ordinal) Position (y) Horsepower mean (Ratio) Size (of segments) Horsepower mean variation (Interval) Slope (of segments) Color hue Cylinders (Ordinal) Size (of points) Count <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[17]: <p>This chart allows to tell the whole story of the car industry in the 70s and the 80s: at first, muscle cars were very popular due to their huge power and their 8 cylinders, but the oil shocks in the 70s led people to prefer more fuel-savvy models with 4 cylinders.</p> <p> \u279c </p> <p>At the end, the industry tried to design less powerful cars but with still with 8 cylinders, in an attempt to lure the consumers into thinking that they were still buying muscle cars (but more fuel-savvy).</p> Visual variable Data Position (x) Horsepower (Ratio) Position (y) Density (Ratio) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[18]: <p>Like with histograms, we can also encode an ordinal data on the color as a visual variable:</p> Visual variable Data Position (x) Horsepower (Ratio) Position (y) Density (Ratio) Color Cylinders (Ordinal) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[19]: <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/altair/utils/core.py:410: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n</pre> Out[20]:"},{"location":"04-Data-visualization/2-plots/#plots","title":"Plots\u00b6","text":""},{"location":"04-Data-visualization/2-plots/#point-based-plots","title":"Point based plots\u00b6","text":""},{"location":"04-Data-visualization/2-plots/#scatter","title":"Scatter\u00b6","text":"<p>Scatter plot is the most basic plot to encode any data using points and their properties. Location (X, Y) are 2 properties which are able to represent any measurement type.</p> <p>Below, two ratio variables are represented (notice how the scale shows the level zero) thanks to position:</p> Visual variable Data Position (x) Horsepower (Ratio) Position (y) Miles per gallon (Ratio)"},{"location":"04-Data-visualization/2-plots/#strip","title":"Strip\u00b6","text":"<p>If you'd like to visualize the relationship between a discrete variable (nominal or ordinal) and a quantitative variable (interval or ratio), you can try to use a scatter plot:</p> Visual variable Data Position (x) Horsepower (Ratio) Position (y) Cylinders (Ordinal) Opacity 25%"},{"location":"04-Data-visualization/2-plots/#rectangle-based-plots","title":"Rectangle based plots\u00b6","text":""},{"location":"04-Data-visualization/2-plots/#bar-histogram","title":"Bar &amp; histogram\u00b6","text":"<p>Bar plots use the bar size a visual variable to encode data. It is good to represent a quantity, so it is used by default in histograms:</p>"},{"location":"04-Data-visualization/2-plots/#2d-histogram","title":"2D-histogram\u00b6","text":"<p>Finally, if have too much overplotting with scatter plots, it's possible to make 2D-histograms. The data is binned across 2 quantitative dimensions, which are transformed intro ordinal data. The count aggregate can then be encoded into visual variables, such as the size or the color.</p> <p>When the count is encoded on the size of the points, it's a bubble plot:</p>"},{"location":"04-Data-visualization/2-plots/#line-based-plots","title":"Line based plots\u00b6","text":""},{"location":"04-Data-visualization/2-plots/#line","title":"Line\u00b6","text":""},{"location":"04-Data-visualization/2-plots/#area-distribution","title":"Area &amp; distribution\u00b6","text":"<p>Area charts are line charts, but filled with color. As bar charts, they are powerful to represent quantities.</p> <p>It can also replace an histogram made on a binned quantitative variable, by using kernel smoothing (which is a type of weighted moving average).</p>"},{"location":"04-Data-visualization/2-plots/#complex-plots","title":"Complex plots\u00b6","text":"<p>Many complex plots exists out there and we can't show everything. For this beginner course, the only one worth showing is the parallel coordinate plot, which allows to show all the dimensions of the data at the same time:</p>"},{"location":"04-Data-visualization/2-plots/#exercises","title":"Exercises\u00b6","text":"<p>Exercise 1: Pick one bad visualization from tumblr.com/badvisualisations and explain how you would do the visualization better.</p> <p>Exercise 2: Explain why the data visualization below made by Minart in 1869 is known as \"the best statistical graphic ever drawn\": </p>"},{"location":"04-Data-visualization/3-exploratory-data-analysis/","title":"Exploratory data analysis","text":"<p>This section is still a Work In Progress.</p>"},{"location":"04-Data-visualization/3-exploratory-data-analysis/#data-visualization-method","title":"Data visualization method","text":"<ol> <li>Analyze data type: analyze the \"measurement type\" of the data you want to plot</li> <li>Encode data on visuals: choose a set of visual variables adapted to each data, according to their measurement type</li> <li>Confront your subjectivity: make several versions of the plot and choose the best one by discussing it with other people.</li> </ol>"},{"location":"04-Data-visualization/3-exploratory-data-analysis/#golden-rules","title":"Golden rules","text":"<ul> <li>Avoid information overload: don't try to plot more that 3 informations on one plot.</li> <li>Use visual redundancy: use more than one visual variable per measurement.</li> <li>Never forget zero: always plot the zero if its meaningful</li> <li>Avoid overplotting: don't try to plot too much data points on the same plot, at the risk of saturating the visual space. If needed, reduce the number of data points (take a sample) or use transparency.</li> <li>Don't use pie charts: period.</li> </ul>"},{"location":"04-Data-visualization/3-exploratory-data-analysis/#descriptive-statistics","title":"Descriptive statistics","text":"Measurement Central stat. Variability stat Nominal Mode Unique count, Information entropy Ordinal Median Unique count, Interquartile range Interval Arithmetic mean Standard deviation Ratio Geometric mean Standard deviation, Coefficient of variation"},{"location":"04-Data-visualization/3-exploratory-data-analysis/#exploratory-data-analysis-in-short","title":"Exploratory data analysis in short","text":"<p>At each step, use plots &amp; descriptive statistics to:</p> <ol> <li>Describe each variable individually</li> <li>Describe the target</li> <li>Describe relationships between variables</li> <li>Describe variables' relationships with the target</li> </ol> <p>The goal is to give a subjective answer to those questions:</p> <ul> <li>Are the data clean ?</li> <li>What kind of information do we have in the data ?</li> <li>What are the variables which will help us the most to solve the prediction problem ?</li> <li>Is the prediction problem difficult ? Do we have the right data to do a prediction ? What kind of prediction performance can we expect ?</li> </ul> <p>A nice example can be found here.</p>"},{"location":"04-Data-visualization/4-practice/","title":"\ud83d\udc0d Practice","text":"<p>Links to tutorial:</p> <ul> <li>Students version</li> <li>Full version</li> </ul>"},{"location":"05-Regression/","title":"05 Regression","text":"<ul> <li>\ud83d\udc0d Practice</li> </ul>"},{"location":"05-Regression/1-practice/","title":"\ud83d\udc0d Practice","text":"<p>Links to tutorial:</p> <ul> <li>Students version</li> <li>Full version</li> </ul>"},{"location":"06-Classification/","title":"06 Classification","text":"<ul> <li>\ud83d\udc0d Practice</li> </ul>"},{"location":"06-Classification/1-practice/","title":"\ud83d\udc0d Practice","text":"<p>Links to tutorial:</p> <ul> <li>Students version</li> <li>Full version</li> </ul>"},{"location":"07-Models-panorama/","title":"07 Models panorama","text":"<ul> <li>Models properties</li> <li>Ensemble learning &amp; tree-based models</li> <li>Support vector machines</li> <li>Bayesian models</li> </ul>"},{"location":"07-Models-panorama/1-models-properties/","title":"Models properties","text":""},{"location":"07-Models-panorama/1-models-properties/#bias-variance","title":"Bias &amp; variance","text":"<p>Course:</p> <ul> <li>Principles and Methodology of Machine Learning - David Gianazza (slides 233-236)</li> </ul> <p>Some sources:</p> <ul> <li>Bias\u2013variance tradeoff - Wikipedia</li> <li>Bias vs Variance - Gunjan Agicha (Medium)</li> </ul> <p>High bias / low variance models generalize better on new tasks.</p> <p>Low bias / high variance models perform better on the task they have been trained on.</p>"},{"location":"07-Models-panorama/1-models-properties/#parametric-non-parametric","title":"Parametric &amp; non-parametric","text":"<ul> <li>What exactly is the difference between a parametric and non-parametric model? - stats.stackexchange</li> <li>Parametric and nonparametric ML algorithms - Machine learning mastery</li> </ul> <p>In very brief:</p> <ul> <li>Parametric models have fixed set of parameters. Their complexity is determined by the number of parameters. The parameters are fitted to the data. Popular parametric models:<ul> <li>Linear regression, Logistic regression</li> <li>Neural networks</li> </ul> </li> <li>Non-parametric models don't have a fixed parameters, but rather are built from the data. Their complexity depends on the data on which they are built on.<ul> <li>Tree-based models: CART, random forests, gradient boosting regressor or classifier</li> <li>K nearest neighbors</li> <li>Support Vector Machines</li> </ul> </li> </ul>"},{"location":"07-Models-panorama/1-models-properties/#computational-complexity","title":"Computational complexity","text":"<ul> <li>Computational Complexity of ML Models - Paritosh Kumar (Medium)</li> </ul>"},{"location":"07-Models-panorama/2-ensemble-learning-and-tree-based-models/","title":"Ensemble learning &amp; tree-based models","text":"<p>Sources:</p> <ul> <li>Classification And Regression Trees (CART) &amp; Random Forest - Richard Alligier, David Gianazza &amp; Pascal Lezaud</li> <li>Boosting - Richard Alligier</li> </ul> <p>Sources:</p> <ul> <li>Ensemble learning - Wikipedia</li> <li>Ensemble methods: bagging, boosting and stacking</li> </ul>"},{"location":"07-Models-panorama/2-ensemble-learning-and-tree-based-models/#decision-tree-cart","title":"Decision tree (CART)","text":"<p>CART stand for Classification And Regression Tree.</p> <ul> <li>How to visualize decision trees - Terence Parr and Prince Grover </li> <li>A visual introduction to machine learning (part 1)</li> </ul> <p>Decisions trees can overfit very quickly. </p>"},{"location":"07-Models-panorama/2-ensemble-learning-and-tree-based-models/#random-forest","title":"Random forest","text":"<p>See Ensemble methods: bagging, boosting and stacking</p>"},{"location":"07-Models-panorama/2-ensemble-learning-and-tree-based-models/#adaptative-boosting","title":"Adaptative boosting","text":"<p>See Ensemble methods: bagging, boosting and stacking</p>"},{"location":"07-Models-panorama/2-ensemble-learning-and-tree-based-models/#gradient-boosting","title":"Gradient boosting","text":"<p>See Ensemble methods: bagging, boosting and stacking</p>"},{"location":"07-Models-panorama/3-support-vector-machines/","title":"Support vector machines","text":"<p>Source:</p> <ul> <li>Support Vector Machines - Nicolas Couellan</li> <li>Support Vector Machines - scikit-learn</li> </ul> <p></p>"},{"location":"07-Models-panorama/4-bayesian-models/","title":"Bayesian models","text":""},{"location":"08-Model-selection/","title":"08 Model selection","text":"<ul> <li>Cross-validation</li> <li>Hyper-parameter tuning</li> <li>\ud83d\udc0d Practice</li> </ul>"},{"location":"08-Model-selection/1-cross-validation/","title":"Cross validation","text":"<p>Purpose: training 100 or 1000s of models and evaluating them all on the test set states the risk of \"overfitting the test set\". To avoid that, we use cross-validation to assess an approximation of test error, without fitting the test dataset.</p> <ul> <li>Principles and Methodology of Machine Learning - David Gianazza (slides 238-246)</li> <li>Model selection - scikit-learn</li> </ul>"},{"location":"08-Model-selection/1-cross-validation/#k-fold","title":"K-fold","text":"<p>..</p>"},{"location":"08-Model-selection/1-cross-validation/#leave-one-out","title":"Leave-one-out","text":"<p>..</p>"},{"location":"08-Model-selection/2-hyperparameter-tuning/","title":"Hyperparameter tuning","text":"<p>Grid search, random search, successive halving, ...</p> <p>For other algorithms, see:</p> <ul> <li>Hyper-parameter optimizers in scikit-learn</li> <li>Ray Tune API </li> </ul>"},{"location":"08-Model-selection/3-practice/","title":"\ud83d\udc0d Practice","text":"<p>Links to tutorial:</p> <ul> <li>Students version</li> <li>Full version</li> </ul>"},{"location":"09-Neural-networks/","title":"09 Neural networks","text":"<p>Course:</p> <ul> <li>Neural networks, from perceptrons to deep learning - David Gianazza (slides 33-75)</li> <li>Neural networks - Backpropagation &amp; chain rule - animated - Quentin Chenevier</li> <li>Interactive Gradient Descent Demo</li> <li>Batch, Mini-Batch and Stochastic Gradient Descent for Linear Regression</li> <li>Tensorflow playground demos - Quentin Chenevier</li> <li>Regularization techniques for training deep neural networks</li> </ul> <p>Other useful resources:</p> <ul> <li>The Matrix Calculus You Need For Deep Learning</li> <li>Why Momentum Really Works</li> <li>Deep Learning course: lecture slides and lab notebooks - Master Year 2 Data Science IP-Paris</li> </ul>"},{"location":"10-Tutorials/","title":"10 Tutorials","text":""},{"location":"10-Tutorials/1_data_engineering/","title":"\ud83d\udc0d Practice n\u00b01: data engineering","text":"In\u00a0[1]: Copied! <pre>!pip install pyarrow\n</pre> !pip install pyarrow <pre>Requirement already satisfied: pyarrow in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (13.0.0)\r\nRequirement already satisfied: numpy&gt;=1.16.6 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from pyarrow) (1.26.0)\r\n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\n\npd.set_option(\"display.max_columns\", 100)\n</pre> import pandas as pd  pd.set_option(\"display.max_columns\", 100) In\u00a0[3]: Copied! <pre>file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_clean.parquet\"\n</pre> file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_clean.parquet\" <p>Load the file <code>ratebeer_sample_clean.parquet</code> to extract a pandas DataFrame and assign it the variable <code>df_raw</code>. Hint:</p> <ul> <li><code>pd.read_parquet</code></li> </ul> In\u00a0[4]: Copied! <pre>df_raw = pd.read_parquet(file_url)\n</pre> df_raw = pd.read_parquet(file_url) In\u00a0[5]: Copied! <pre>df_raw.shape\n</pre> df_raw.shape Out[5]: <pre>(200000, 12)</pre> In\u00a0[6]: Copied! <pre>df_raw.isnull().sum()\n</pre> df_raw.isnull().sum() Out[6]: <pre>beer                 0\nbrewery              0\nalcohol              0\ntype                 0\nrating_appearance    0\nrating_aroma         0\nrating_palate        0\nrating_taste         0\nrating               0\ntimestamp            0\nuser                 0\ntext                 0\ndtype: int64</pre> In\u00a0[7]: Copied! <pre>df_raw.dtypes\n</pre> df_raw.dtypes Out[7]: <pre>beer                  object\nbrewery               object\nalcohol              float64\ntype                  object\nrating_appearance      int64\nrating_aroma           int64\nrating_palate          int64\nrating_taste           int64\nrating                 int64\ntimestamp             object\nuser                  object\ntext                  object\ndtype: object</pre> In\u00a0[8]: Copied! <pre>df_raw.info(memory_usage=\"deep\")  # LINE TO BE REMOVED FOR STUDENTS\n</pre> df_raw.info(memory_usage=\"deep\")  # LINE TO BE REMOVED FOR STUDENTS <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 200000 entries, 2747119 to 2130618\nData columns (total 12 columns):\n #   Column             Non-Null Count   Dtype  \n---  ------             --------------   -----  \n 0   beer               200000 non-null  object \n 1   brewery            200000 non-null  object \n 2   alcohol            200000 non-null  float64\n 3   type               200000 non-null  object \n 4   rating_appearance  200000 non-null  int64  \n 5   rating_aroma       200000 non-null  int64  \n 6   rating_palate      200000 non-null  int64  \n 7   rating_taste       200000 non-null  int64  \n 8   rating             200000 non-null  int64  \n 9   timestamp          200000 non-null  object \n 10  user               200000 non-null  object \n 11  text               200000 non-null  object \ndtypes: float64(1), int64(5), object(6)\nmemory usage: 146.1 MB\n</pre> In\u00a0[9]: Copied! <pre>df_raw.head(5)\n</pre> df_raw.head(5) Out[9]: beer brewery alcohol type rating_appearance rating_aroma rating_palate rating_taste rating timestamp user text 2747119 Breckenridge Oatmeal Stout 383 4.95 Stout 4 7 4 7 14 1217462400 blutt59 bottle, oat nose with black color, bitter choc... 680733 De Dolle Arabier 1163 8.00 Belgian Strong Ale 4 9 4 8 18 1291939200 AndreaDel Bottle. Orange color, big head. Aroma is hoppy... 2797108 Grafensteiner Pils 2545 4.70 Classic German Pilsener 3 7 4 5 9 1306108800 Koelschtrinker Die Frage ist, ob ich das selbe Bier wie die a... 824617 New Glarus Raspberry Tart 1248 4.00 Fruit Beer 4 8 5 8 16 1211760000 polomagnifico Bottle thanks to nflmvp, thanks Steve!  Aroma ... 2730732 Nils Oscar Hop Yard IPA 1086 7.30 India Pale Ale (IPA) 4 7 4 8 15 1275004800 dEnk Bottle, tfs Vaiz! Brownish amber, small head, ... In\u00a0[10]: Copied! <pre>df_raw.describe(include=\"all\").fillna(\"\")\n</pre> df_raw.describe(include=\"all\").fillna(\"\") Out[10]: beer brewery alcohol type rating_appearance rating_aroma rating_palate rating_taste rating timestamp user text count 200000 200000 200000.0 200000 200000.0 200000.0 200000.0 200000.0 200000.0 200000 200000 200000 unique 42125 5413 89 4105 11251 199261 top Guinness Draught 32 India Pale Ale (IPA) 1188604800 fonefan freq 252 3217 12698 392 1146 334 mean 6.276042 3.430205 6.3512 3.258225 6.44773 13.19231 std 2.781795 0.813961 1.643552 0.829915 1.623235 3.350988 min -1.0 1.0 1.0 1.0 1.0 1.0 25% 5.0 3.0 6.0 3.0 6.0 12.0 50% 5.8 3.0 7.0 3.0 7.0 14.0 75% 8.0 4.0 7.0 4.0 8.0 15.0 max 57.7 5.0 10.0 5.0 10.0 20.0 <p>Sometimes you only need the describing statistics for a single column. Count and display the distinct beer names.</p> <p>Hint:</p> <ul> <li><code>pd.Series.nunique</code></li> <li><code>pd.Series.unique</code></li> </ul> In\u00a0[11]: Copied! <pre>(df_raw.beer).nunique()  # LINE TO BE REMOVED FOR STUDENTS\n</pre> (df_raw.beer).nunique()  # LINE TO BE REMOVED FOR STUDENTS Out[11]: <pre>42125</pre> In\u00a0[12]: Copied! <pre>(df_raw.beer).unique()  # LINE TO BE REMOVED FOR STUDENTS\n</pre> (df_raw.beer).unique()  # LINE TO BE REMOVED FOR STUDENTS Out[12]: <pre>array(['Breckenridge Oatmeal Stout', 'De Dolle Arabier',\n       'Grafensteiner Pils', ..., 'Rockbottom Wheat Beer',\n       'Dow Bridge Adventus', 'Beach Chalet Third Eye Rye'], dtype=object)</pre> <p>Display the number of reviews per beer type.</p> <p>Hint:</p> <ul> <li><code>pd.Series.value_counts</code></li> </ul> In\u00a0[13]: Copied! <pre>(df_raw.type).value_counts()  # LINE TO BE REMOVED FOR STUDENTS\n</pre> (df_raw.type).value_counts()  # LINE TO BE REMOVED FOR STUDENTS Out[13]: <pre>type\nIndia Pale Ale (IPA)    12698\nPale Lager              10062\nBelgian Strong Ale       8486\nImperial Stout           8460\nImperial/Double IPA      7215\n                        ...  \nSak - Taru                  9\nSak - Genshu                6\nSak - Honjozo               5\nSak - Tokubetsu             5\nSak - Koshu                 3\nName: count, Length: 89, dtype: int64</pre> In\u00a0[14]: Copied! <pre>selected_columns = [\n    \"beer\",\n    \"alcohol\",\n    \"type\",\n    \"user\",\n    \"rating\",\n]\n\ndf_stout = (\n    (df_raw)\n    .loc[:, selected_columns]\n    .loc[lambda df: df.type.str.contains(\"Stout\")]  # LINE TO BE REMOVED FOR STUDENTS\n    .reset_index(drop=True)\n)\ndf_stout\n</pre> selected_columns = [     \"beer\",     \"alcohol\",     \"type\",     \"user\",     \"rating\", ]  df_stout = (     (df_raw)     .loc[:, selected_columns]     .loc[lambda df: df.type.str.contains(\"Stout\")]  # LINE TO BE REMOVED FOR STUDENTS     .reset_index(drop=True) ) df_stout Out[14]: beer alcohol type user rating 0 Breckenridge Oatmeal Stout 4.95 Stout blutt59 14 1 Founders Breakfast Stout 8.30 Imperial Stout beerandIT 20 2 Thirsty Dog Siberian Night Imperial Stout 9.70 Imperial Stout Lehighbri 14 3 St. Ambroise Oatmeal Stout 5.00 Stout SimonB 18 4 Ithaca Excelsior! Eleven 8.50 Stout vtafro 14 ... ... ... ... ... ... 18114 St Peters Cream Stout 6.50 Stout Quietman 14 18115 Weyerbacher Old Heathen 8.00 Imperial Stout cking 16 18116 Pisgah Valdez 6.80 Stout CaptainCougar 15 18117 De Dolle Extra Export Stout 9.00 Foreign Stout bierkoning 16 18118 Goose Island Honest Stout 5.70 Stout Mangino 14 <p>18119 rows \u00d7 5 columns</p> <p>Compute the number of occurences of each Stout beers.</p> In\u00a0[15]: Copied! <pre>df_stout.type.value_counts()\n</pre> df_stout.type.value_counts() Out[15]: <pre>type\nImperial Stout    8460\nStout             4539\nSweet Stout       2762\nDry Stout         1539\nForeign Stout      819\nName: count, dtype: int64</pre> In\u00a0[16]: Copied! <pre>df_beer_degree = (\n    (df_raw.beer)\n    .value_counts()  # LINE TO BE REMOVED FOR STUDENTS\n    .rename(\"beer_degree\")\n    .reset_index()\n)\ndf_beer_degree\n</pre> df_beer_degree = (     (df_raw.beer)     .value_counts()  # LINE TO BE REMOVED FOR STUDENTS     .rename(\"beer_degree\")     .reset_index() ) df_beer_degree Out[16]: beer beer_degree 0 Guinness Draught 252 1 Pabst Blue Ribbon 245 2 Dogfish Head 90 Minute Imperial IPA 232 3 Budweiser 217 4 Sierra Nevada Pale Ale (Bottle) 207 ... ... ... 42120 Meinel-Bru Zwickel 1 42121 3 Rivers Murphys Law 1 42122 Skagit River Golden Lager 1 42123 Allsvensk 1 42124 Brew Wharf &amp; The Kernel Collaboration 1 <p>42125 rows \u00d7 2 columns</p> <p>Check that this table will merge properly.</p> In\u00a0[17]: Copied! <pre>df_tmp = df_raw.merge(\n    df_beer_degree,\n    on=\"beer\",\n    how=\"outer\",\n    validate=\"m:1\",\n    indicator=True,\n)\ndf_tmp._merge.value_counts()\n</pre> df_tmp = df_raw.merge(     df_beer_degree,     on=\"beer\",     how=\"outer\",     validate=\"m:1\",     indicator=True, ) df_tmp._merge.value_counts() Out[17]: <pre>_merge\nboth          200000\nleft_only          0\nright_only         0\nName: count, dtype: int64</pre> In\u00a0[18]: Copied! <pre>df_brewery_degree = (\n    (df_raw.brewery)\n    .value_counts()  # LINE TO BE REMOVED FOR STUDENTS\n    .rename(\"brewery_degree\")\n    .reset_index()\n)\ndf_brewery_degree\n</pre> df_brewery_degree = (     (df_raw.brewery)     .value_counts()  # LINE TO BE REMOVED FOR STUDENTS     .rename(\"brewery_degree\")     .reset_index() ) df_brewery_degree Out[18]: brewery brewery_degree 0 32 3217 1 76 2648 2 198 2576 3 96 2401 4 84 2266 ... ... ... 5408 5407 1 5409 8930 1 5410 12924 1 5411 7842 1 5412 8639 1 <p>5413 rows \u00d7 2 columns</p> <p>Check that this table will merge properly.</p> In\u00a0[19]: Copied! <pre>df_tmp = df_raw.merge(\n    df_brewery_degree,\n    on=\"brewery\",\n    how=\"outer\",\n    validate=\"m:1\",\n    indicator=True,\n)\ndf_tmp._merge.value_counts()\n</pre> df_tmp = df_raw.merge(     df_brewery_degree,     on=\"brewery\",     how=\"outer\",     validate=\"m:1\",     indicator=True, ) df_tmp._merge.value_counts() Out[19]: <pre>_merge\nboth          200000\nleft_only          0\nright_only         0\nName: count, dtype: int64</pre> In\u00a0[20]: Copied! <pre>df_user_degree = (\n    (df_raw.user)\n    .value_counts()  # LINE TO BE REMOVED FOR STUDENTS\n    .rename(\"user_degree\")\n    .reset_index()\n)\ndf_user_degree\n</pre> df_user_degree = (     (df_raw.user)     .value_counts()  # LINE TO BE REMOVED FOR STUDENTS     .rename(\"user_degree\")     .reset_index() ) df_user_degree Out[20]: user user_degree 0 fonefan 1146 1 Ungstrup 1063 2 Papsoe 927 3 yespr 889 4 oh6gdx 701 ... ... ... 11246 BattleandBrew 1 11247 Borgen 1 11248 drainey99 1 11249 biermeister99 1 11250 rob51 1 <p>11251 rows \u00d7 2 columns</p> <p>Check that this table will merge properly.</p> In\u00a0[21]: Copied! <pre>df_tmp = df_raw.merge(\n    df_user_degree,\n    on=\"user\",\n    how=\"outer\",\n    validate=\"m:1\",\n    indicator=True,\n)\ndf_tmp._merge.value_counts()\n</pre> df_tmp = df_raw.merge(     df_user_degree,     on=\"user\",     how=\"outer\",     validate=\"m:1\",     indicator=True, ) df_tmp._merge.value_counts() Out[21]: <pre>_merge\nboth          200000\nleft_only          0\nright_only         0\nName: count, dtype: int64</pre> <p>We'll then merge the 3 dataframe at once.</p> In\u00a0[22]: Copied! <pre>text_length = df_raw.text.str.len()\ntext_length\n</pre> text_length = df_raw.text.str.len() text_length Out[22]: <pre>2747119     90\n680733     140\n2797108    324\n824617     371\n2730732    166\n          ... \n1563335    222\n2791415    157\n449553     198\n1666173    150\n2130618    243\nName: text, Length: 200000, dtype: int64</pre> In\u00a0[23]: Copied! <pre>date = (df_raw.timestamp).astype(int).apply(pd.Timestamp.fromtimestamp)\ndate\n</pre> date = (df_raw.timestamp).astype(int).apply(pd.Timestamp.fromtimestamp) date Out[23]: <pre>2747119   2008-07-31\n680733    2010-12-10\n2797108   2011-05-23\n824617    2008-05-26\n2730732   2010-05-28\n             ...    \n1563335   2009-09-05\n2791415   2009-05-23\n449553    2010-08-04\n1666173   2009-05-23\n2130618   2006-10-05\nName: timestamp, Length: 200000, dtype: datetime64[ns]</pre> In\u00a0[24]: Copied! <pre>is_good = (df_raw.rating &gt;= 16).astype(int)\nis_good\n</pre> is_good = (df_raw.rating &gt;= 16).astype(int) is_good Out[24]: <pre>2747119    0\n680733     1\n2797108    0\n824617     1\n2730732    0\n          ..\n1563335    0\n2791415    0\n449553     0\n1666173    0\n2130618    0\nName: rating, Length: 200000, dtype: int64</pre> <p>What are the values of this binary target ?</p> In\u00a0[25]: Copied! <pre>is_good.value_counts()\n</pre> is_good.value_counts() Out[25]: <pre>rating\n0    151269\n1     48731\nName: count, dtype: int64</pre> In\u00a0[26]: Copied! <pre>df_main = (\n    (df_raw)\n    .merge(\n        df_beer_degree,\n        on=\"beer\",  # LINE TO BE REMOVED FOR STUDENTS\n        how=\"inner\",\n        validate=\"m:1\",\n    )\n    .merge(\n        df_brewery_degree,\n        on=\"brewery\",  # LINE TO BE REMOVED FOR STUDENTS\n        how=\"inner\",\n        validate=\"m:1\",\n    )\n    .merge(\n        df_user_degree,\n        on=\"user\",  # LINE TO BE REMOVED FOR STUDENTS\n        how=\"inner\",\n        validate=\"m:1\",\n    )\n    .assign(text_length=lambda df: df.text.str.len())\n    .assign(\n        date=lambda df: (df.timestamp).astype(int).apply(pd.Timestamp.fromtimestamp)\n    )\n    .assign(is_good=lambda df: (df.rating &gt;= 16).astype(int))\n)\ndf_main\n</pre> df_main = (     (df_raw)     .merge(         df_beer_degree,         on=\"beer\",  # LINE TO BE REMOVED FOR STUDENTS         how=\"inner\",         validate=\"m:1\",     )     .merge(         df_brewery_degree,         on=\"brewery\",  # LINE TO BE REMOVED FOR STUDENTS         how=\"inner\",         validate=\"m:1\",     )     .merge(         df_user_degree,         on=\"user\",  # LINE TO BE REMOVED FOR STUDENTS         how=\"inner\",         validate=\"m:1\",     )     .assign(text_length=lambda df: df.text.str.len())     .assign(         date=lambda df: (df.timestamp).astype(int).apply(pd.Timestamp.fromtimestamp)     )     .assign(is_good=lambda df: (df.rating &gt;= 16).astype(int)) ) df_main Out[26]: beer brewery alcohol type rating_appearance rating_aroma rating_palate rating_taste rating timestamp user text beer_degree brewery_degree user_degree text_length date is_good 0 Breckenridge Oatmeal Stout 383 4.95 Stout 4 7 4 7 14 1217462400 blutt59 bottle, oat nose with black color, bitter choc... 51 413 300 90 2008-07-31 0 1 Breckenridge 471 Small Batch Imperial Porter 383 7.50 Imperial/Strong Porter 3 8 3 8 14 1312588800 blutt59 bottle, received in trade, dark brown with gar... 4 413 300 191 2011-08-06 0 2 Breckenridge Avalanche Amber 383 5.41 Amber Ale 3 5 3 5 10 1205020800 blutt59 12 oz. bottle, amber color with soapy head, sl... 43 413 300 109 2008-03-09 0 3 Breckenridge Lucky U IPA 383 6.20 India Pale Ale (IPA) 3 6 3 7 12 1255737600 blutt59 bottle, golden orange color with light tan foa... 20 413 300 151 2009-10-17 0 4 Fullers Vintage Ale 2009 55 8.50 English Strong Ale 3 7 3 8 14 1282003200 blutt59 bottle, thanks to SS, almond amber colored pou... 18 978 300 159 2010-08-17 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 199995 Die M Dunkel 6587 -1.00 Dunkel 4 5 3 3 8 1204070400 kuleko Bottle, supermarket. Very dark colour. Sweet c... 2 2 1 163 2008-02-27 0 199996 Montt Hafen Porter 8240 6.00 Porter 5 10 5 10 19 1175040000 Andressantis Sabrosa,con amargor justo y ligeramente dulce.... 1 1 1 97 2007-03-28 1 199997 Oxymore 8022 5.00 Saison 4 8 3 6 12 1181952000 DesmondLangham Very earthy nose.\\tClear and golden, head kept... 1 1 1 192 2007-06-16 0 199998 Stonehouse Station Bitter 8561 3.90 Bitter 3 5 4 6 13 1238716800 thedees Cask at the Three Pigeons Nescliffe. Maly swee... 1 2 1 88 2009-04-03 0 199999 Pilker Negra 6687 5.00 Stout 4 6 2 5 5 1142553600 apeters Roasty aroma.Black color.Dry and bitter flavor... 1 1 1 144 2006-03-17 0 <p>200000 rows \u00d7 18 columns</p> <p>Save the final result to a parquet file named <code>ratebeer_sample_enriched.parquet</code>.</p> <p>Hint:</p> <ul> <li><code>pd.DataFrame.to_parquet</code></li> </ul> In\u00a0[27]: Copied! <pre># Uncomment the line below to save the dataset to disk\ndf_main.to_parquet(\"ratebeer_sample_enriched.parquet\")\n</pre> # Uncomment the line below to save the dataset to disk df_main.to_parquet(\"ratebeer_sample_enriched.parquet\") <p>GOOD JOB \ud83d\udc4d</p> <p></p>"},{"location":"10-Tutorials/1_data_engineering/#practice-n1-data-engineering","title":"\ud83d\udc0d Practice n\u00b01: data engineering\u00b6","text":"<p>The objective of this session is to learn about the basics of data engineering. You will have to explore the Ratebeer dataset using sql and python.</p> <p>This dataset consists of beer reviews from ratebeer. The data span a period of more than 10 years, including all ~3 million reviews up to November 2011. Each review includes ratings in terms of five \"aspects\": appearance, aroma, palate, taste, and overall impression. Reviews include product and user information, followed by each of these five ratings, and a plaintext review. We also have reviews from beeradvocate.</p> <p>source ratebeer dataset description</p> <p>To avoid high compute time, we are going to work with a sample during the session. Also, the data is already cleaned.</p> <p>Here are the main steps of the notebook :</p> <ol> <li>Preparation</li> <li>Data engineering in sql with duckdb</li> <li>Data engineering in python with pandas</li> </ol> <p></p> <p>This is a data engineering tutorial in Python/pandas, it assumes you have already some some knowledge of data engineering in SQL.</p>"},{"location":"10-Tutorials/1_data_engineering/#preparation","title":"Preparation\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering/#install-import-modules","title":"Install &amp; import modules\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering/#preparation","title":"Preparation\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering/#get-some-doc","title":"Get some doc\u00b6","text":"<ul> <li>pandas doc: main page</li> <li>pandas doc: API reference</li> </ul>"},{"location":"10-Tutorials/1_data_engineering/#read-data","title":"Read data\u00b6","text":"<p>The data is in this git repository: ML-boot-camp/ratebeer.git.</p> <p>The data is located in the <code>ratebeer/data/</code> folder.</p>"},{"location":"10-Tutorials/1_data_engineering/#general-information","title":"General information\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering/#shape","title":"Shape\u00b6","text":"<p>Have a first overview of the dataframe size, i.e. number of rows &amp; columns.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.shape</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering/#overview","title":"Overview\u00b6","text":"<p>Get a few information about the content of the dataframe:</p> <ul> <li>number of null values per column</li> <li>data type of each column</li> <li>memory usage</li> </ul> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.isnull</code></li> <li><code>pd.DataFrame.sum</code></li> <li><code>pd.DataFrame.dtypes</code></li> <li><code>pd.DataFrame.info</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering/#sample","title":"Sample\u00b6","text":"<p>Show a sample of the data</p> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.head</code></li> <li><code>pd.DataFrame.sample</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering/#describing-statistics","title":"Describing statistics\u00b6","text":"<p>Compute statistics to understand the content of each column.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.describe</code></li> </ul> <p>Bonus: fill NaN values with an empty string <code>\"\"</code> for a better readability using:</p> <ul> <li><code>pd.DataFrame.fillna</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering/#select-data","title":"Select data\u00b6","text":"<p>Create the following dataframe :</p> <ul> <li>Keep only those columns:<ul> <li><code>beer</code></li> <li><code>alcohol</code></li> <li><code>type</code></li> <li><code>user</code></li> <li><code>rating</code></li> </ul> </li> <li>Keep only rows for which the <code>type</code> column contains the string <code>\"Stout\"</code></li> </ul> <p>Hint:</p> <ul> <li><code>pd.DataFrame.loc</code></li> <li><code>pd.Series.str.contains</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering/#feature-engineering","title":"Feature engineering\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering/#high-cardinality-variables","title":"High cardinality variables\u00b6","text":"<ul> <li><code>beer</code></li> <li><code>brewery</code></li> <li><code>user</code></li> </ul> <p>All those high cardinality variables can be thought as links of a network. Indeed, a review is an object comprising a beer, a brewery and a user and can be thought as a network link between them.</p> <p>In other words, the review table is the a table describing the links in a network with 3 types of nodes: users, beers and breweries.</p> <p>The first property to compute about each node is its \"degree\", which is its number of connections with other nodes. High degree means \"highly connected\".</p> <p>To compute the degree you'll need:</p> <ul> <li><code>pd.Series.value_counts</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering/#text-length","title":"Text length\u00b6","text":"<p>Compute the length of the texts in the dataset.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.Series.str.len</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering/#convert-timestamp","title":"Convert timestamp\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering/#binary-target","title":"Binary target\u00b6","text":"<p>The prediction problem is to predict <code>rating</code> based on other information. Since <code>rating</code> is a numeric variable, it is a regression problem. We'd like also to do a classification so we'll create a binary target based on the <code>rating</code>.</p> <p>The <code>\u00ecs_good</code> column is True if the rating is above or equal to 16 (expert judgement), False otherwise.</p> <p>Note: also convert the binary target to integer (O or 1) for better readability.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.Series.astype</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering/#combine-dataframes","title":"Combine dataframes\u00b6","text":"<p>Create a dataframe combining information from:</p> <ul> <li><code>df_raw</code>: the original dataset</li> <li><code>df_beer_degree</code>: merged on <code>beer</code> column</li> <li><code>df_brewery_degree</code>: merged on <code>brewery</code> column</li> <li><code>df_user_degree</code>: merged on <code>user</code> column</li> <li><code>text_length</code>: added as a new column</li> <li><code>date</code>: added as a new column</li> <li><code>is_good</code>: added as a new column</li> </ul> <p>Note: <code>merge</code> is the equivalent of <code>JOIN</code> in SQL, and it changes the order of the rows ! So to add our data columns properly in the dataset, we have 2 options:</p> <ul> <li>add the new columns using precomputed arrays, but before merging (not recommended): e.g: <code>df_raw.text.str.len()</code></li> <li>add the new columns using a function (recommended): e.g: <code>lambda df: df.text.str.len()</code></li> </ul> <p>Hint:</p> <ul> <li><code>pd.DataFrame.merge</code></li> <li><code>pd.DataFrame.assign</code></li> </ul> <p>Note: If some columns in both the left &amp; right dataframes have the same name, you'll obtain duplicated columns in the merge result. <code>pandas</code> adds the suffixes <code>_x</code> and <code>_y</code> to avoid creating duplicate columns. Use the <code>suffixes</code> argument to specify the suffixes to apply to duplicated columns. In this example, there is no common column name in both dataframes.</p> <p>We made lots of transformation to our datasets: we want to verify that all values in the \"primary keys\" columns are indeed unique. Use the <code>validate</code> argument to do so.</p>"},{"location":"10-Tutorials/1_data_engineering_students/","title":"\ud83d\udc0d Practice n\u00b01: data engineering (students version)","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install pyarrow\n</pre> !pip install pyarrow In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\npd.set_option(\"display.max_columns\", 100)\n</pre> import pandas as pd  pd.set_option(\"display.max_columns\", 100) In\u00a0[\u00a0]: Copied! <pre>file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_clean.parquet\"\n</pre> file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_clean.parquet\" <p>Load the file <code>ratebeer_sample_clean.parquet</code> to extract a pandas DataFrame and assign it the variable <code>df_raw</code>. Hint:</p> <ul> <li><code>pd.read_parquet</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre>df_raw = pd.read_parquet(file_url)\n</pre> df_raw = pd.read_parquet(file_url) In\u00a0[\u00a0]: Copied! <pre>df_raw.shape\n</pre> df_raw.shape In\u00a0[\u00a0]: Copied! <pre>df_raw.isnull().sum()\n</pre> df_raw.isnull().sum() In\u00a0[\u00a0]: Copied! <pre>df_raw.dtypes\n</pre> df_raw.dtypes In\u00a0[\u00a0]: Copied! <pre>*** FILL THE MISSING LINE ***\n</pre> *** FILL THE MISSING LINE *** In\u00a0[\u00a0]: Copied! <pre>df_raw.head(5)\n</pre> df_raw.head(5) In\u00a0[\u00a0]: Copied! <pre>df_raw.describe(include=\"all\").fillna(\"\")\n</pre> df_raw.describe(include=\"all\").fillna(\"\") <p>Sometimes you only need the describing statistics for a single column. Count and display the distinct beer names.</p> <p>Hint:</p> <ul> <li><code>pd.Series.nunique</code></li> <li><code>pd.Series.unique</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre>*** FILL THE MISSING LINE ***\n</pre> *** FILL THE MISSING LINE *** In\u00a0[\u00a0]: Copied! <pre>*** FILL THE MISSING LINE ***\n</pre> *** FILL THE MISSING LINE *** <p>Display the number of reviews per beer type.</p> <p>Hint:</p> <ul> <li><code>pd.Series.value_counts</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre>*** FILL THE MISSING LINE ***\n</pre> *** FILL THE MISSING LINE *** In\u00a0[\u00a0]: Copied! <pre>selected_columns = [\n    \"beer\",\n    \"alcohol\",\n    \"type\",\n    \"user\",\n    \"rating\",\n]\n\ndf_stout = (\n    (df_raw)\n    .loc[:, selected_columns]\n    *** FILL THE MISSING LINE ***\n    .reset_index(drop=True)\n)\ndf_stout\n</pre> selected_columns = [     \"beer\",     \"alcohol\",     \"type\",     \"user\",     \"rating\", ]  df_stout = (     (df_raw)     .loc[:, selected_columns]     *** FILL THE MISSING LINE ***     .reset_index(drop=True) ) df_stout <p>Compute the number of occurences of each Stout beers.</p> In\u00a0[\u00a0]: Copied! <pre>df_stout.type.value_counts()\n</pre> df_stout.type.value_counts() In\u00a0[\u00a0]: Copied! <pre>df_beer_degree = (\n    (df_raw.beer)\n    *** FILL THE MISSING LINE ***\n    .rename(\"beer_degree\")\n    .reset_index()\n)\ndf_beer_degree\n</pre> df_beer_degree = (     (df_raw.beer)     *** FILL THE MISSING LINE ***     .rename(\"beer_degree\")     .reset_index() ) df_beer_degree <p>Check that this table will merge properly.</p> In\u00a0[\u00a0]: Copied! <pre>df_tmp = df_raw.merge(\n    df_beer_degree,\n    on=\"beer\",\n    how=\"outer\",\n    validate=\"m:1\",\n    indicator=True,\n)\ndf_tmp._merge.value_counts()\n</pre> df_tmp = df_raw.merge(     df_beer_degree,     on=\"beer\",     how=\"outer\",     validate=\"m:1\",     indicator=True, ) df_tmp._merge.value_counts() In\u00a0[\u00a0]: Copied! <pre>df_brewery_degree = (\n    (df_raw.brewery)\n    *** FILL THE MISSING LINE ***\n    .rename(\"brewery_degree\")\n    .reset_index()\n)\ndf_brewery_degree\n</pre> df_brewery_degree = (     (df_raw.brewery)     *** FILL THE MISSING LINE ***     .rename(\"brewery_degree\")     .reset_index() ) df_brewery_degree <p>Check that this table will merge properly.</p> In\u00a0[\u00a0]: Copied! <pre>df_tmp = df_raw.merge(\n    df_brewery_degree,\n    on=\"brewery\",\n    how=\"outer\",\n    validate=\"m:1\",\n    indicator=True,\n)\ndf_tmp._merge.value_counts()\n</pre> df_tmp = df_raw.merge(     df_brewery_degree,     on=\"brewery\",     how=\"outer\",     validate=\"m:1\",     indicator=True, ) df_tmp._merge.value_counts() In\u00a0[\u00a0]: Copied! <pre>df_user_degree = (\n    (df_raw.user)\n    *** FILL THE MISSING LINE ***\n    .rename(\"user_degree\")\n    .reset_index()\n)\ndf_user_degree\n</pre> df_user_degree = (     (df_raw.user)     *** FILL THE MISSING LINE ***     .rename(\"user_degree\")     .reset_index() ) df_user_degree <p>Check that this table will merge properly.</p> In\u00a0[\u00a0]: Copied! <pre>df_tmp = df_raw.merge(\n    df_user_degree,\n    on=\"user\",\n    how=\"outer\",\n    validate=\"m:1\",\n    indicator=True,\n)\ndf_tmp._merge.value_counts()\n</pre> df_tmp = df_raw.merge(     df_user_degree,     on=\"user\",     how=\"outer\",     validate=\"m:1\",     indicator=True, ) df_tmp._merge.value_counts() <p>We'll then merge the 3 dataframe at once.</p> In\u00a0[\u00a0]: Copied! <pre>text_length = df_raw.text.str.len()\ntext_length\n</pre> text_length = df_raw.text.str.len() text_length In\u00a0[\u00a0]: Copied! <pre>date = (df_raw.timestamp).astype(int).apply(pd.Timestamp.fromtimestamp)\ndate\n</pre> date = (df_raw.timestamp).astype(int).apply(pd.Timestamp.fromtimestamp) date In\u00a0[\u00a0]: Copied! <pre>is_good = (df_raw.rating &gt;= 16).astype(int)\nis_good\n</pre> is_good = (df_raw.rating &gt;= 16).astype(int) is_good <p>What are the values of this binary target ?</p> In\u00a0[\u00a0]: Copied! <pre>is_good.value_counts()\n</pre> is_good.value_counts() In\u00a0[\u00a0]: Copied! <pre>df_main = (\n    (df_raw)\n    .merge(\n        df_beer_degree,\n        *** FILL THE MISSING LINE ***\n        how=\"inner\",\n        validate=\"m:1\",\n    )\n    .merge(\n        df_brewery_degree,\n        *** FILL THE MISSING LINE ***\n        how=\"inner\",\n        validate=\"m:1\",\n    )\n    .merge(\n        df_user_degree,\n        *** FILL THE MISSING LINE ***\n        how=\"inner\",\n        validate=\"m:1\",\n    )\n    .assign(text_length=lambda df: df.text.str.len())\n    .assign(\n        date=lambda df: (df.timestamp).astype(int).apply(pd.Timestamp.fromtimestamp)\n    )\n    .assign(is_good=lambda df: (df.rating &gt;= 16).astype(int))\n)\ndf_main\n</pre> df_main = (     (df_raw)     .merge(         df_beer_degree,         *** FILL THE MISSING LINE ***         how=\"inner\",         validate=\"m:1\",     )     .merge(         df_brewery_degree,         *** FILL THE MISSING LINE ***         how=\"inner\",         validate=\"m:1\",     )     .merge(         df_user_degree,         *** FILL THE MISSING LINE ***         how=\"inner\",         validate=\"m:1\",     )     .assign(text_length=lambda df: df.text.str.len())     .assign(         date=lambda df: (df.timestamp).astype(int).apply(pd.Timestamp.fromtimestamp)     )     .assign(is_good=lambda df: (df.rating &gt;= 16).astype(int)) ) df_main <p>Save the final result to a parquet file named <code>ratebeer_sample_enriched.parquet</code>.</p> <p>Hint:</p> <ul> <li><code>pd.DataFrame.to_parquet</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre># Uncomment the line below to save the dataset to disk\ndf_main.to_parquet(\"ratebeer_sample_enriched.parquet\")\n</pre> # Uncomment the line below to save the dataset to disk df_main.to_parquet(\"ratebeer_sample_enriched.parquet\") <p>GOOD JOB \ud83d\udc4d</p> <p></p>"},{"location":"10-Tutorials/1_data_engineering_students/#practice-n1-data-engineering-students-version","title":"\ud83d\udc0d Practice n\u00b01: data engineering (students version)\u00b6","text":"<p>The objective of this session is to learn about the basics of data engineering. You will have to explore the Ratebeer dataset using sql and python.</p> <p>This dataset consists of beer reviews from ratebeer. The data span a period of more than 10 years, including all ~3 million reviews up to November 2011. Each review includes ratings in terms of five \"aspects\": appearance, aroma, palate, taste, and overall impression. Reviews include product and user information, followed by each of these five ratings, and a plaintext review. We also have reviews from beeradvocate.</p> <p>source ratebeer dataset description</p> <p>To avoid high compute time, we are going to work with a sample during the session. Also, the data is already cleaned.</p> <p>Here are the main steps of the notebook :</p> <ol> <li>Preparation</li> <li>Data engineering in sql with duckdb</li> <li>Data engineering in python with pandas</li> </ol> <p></p> <p>This is a data engineering tutorial in Python/pandas, it assumes you have already some some knowledge of data engineering in SQL.</p>"},{"location":"10-Tutorials/1_data_engineering_students/#preparation","title":"Preparation\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering_students/#install-import-modules","title":"Install &amp; import modules\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering_students/#preparation","title":"Preparation\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering_students/#get-some-doc","title":"Get some doc\u00b6","text":"<ul> <li>pandas doc: main page</li> <li>pandas doc: API reference</li> </ul>"},{"location":"10-Tutorials/1_data_engineering_students/#read-data","title":"Read data\u00b6","text":"<p>The data is in this git repository: ML-boot-camp/ratebeer.git.</p> <p>The data is located in the <code>ratebeer/data/</code> folder.</p>"},{"location":"10-Tutorials/1_data_engineering_students/#general-information","title":"General information\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering_students/#shape","title":"Shape\u00b6","text":"<p>Have a first overview of the dataframe size, i.e. number of rows &amp; columns.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.shape</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering_students/#overview","title":"Overview\u00b6","text":"<p>Get a few information about the content of the dataframe:</p> <ul> <li>number of null values per column</li> <li>data type of each column</li> <li>memory usage</li> </ul> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.isnull</code></li> <li><code>pd.DataFrame.sum</code></li> <li><code>pd.DataFrame.dtypes</code></li> <li><code>pd.DataFrame.info</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering_students/#sample","title":"Sample\u00b6","text":"<p>Show a sample of the data</p> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.head</code></li> <li><code>pd.DataFrame.sample</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering_students/#describing-statistics","title":"Describing statistics\u00b6","text":"<p>Compute statistics to understand the content of each column.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.describe</code></li> </ul> <p>Bonus: fill NaN values with an empty string <code>\"\"</code> for a better readability using:</p> <ul> <li><code>pd.DataFrame.fillna</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering_students/#select-data","title":"Select data\u00b6","text":"<p>Create the following dataframe :</p> <ul> <li>Keep only those columns:<ul> <li><code>beer</code></li> <li><code>alcohol</code></li> <li><code>type</code></li> <li><code>user</code></li> <li><code>rating</code></li> </ul> </li> <li>Keep only rows for which the <code>type</code> column contains the string <code>\"Stout\"</code></li> </ul> <p>Hint:</p> <ul> <li><code>pd.DataFrame.loc</code></li> <li><code>pd.Series.str.contains</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering_students/#feature-engineering","title":"Feature engineering\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering_students/#high-cardinality-variables","title":"High cardinality variables\u00b6","text":"<ul> <li><code>beer</code></li> <li><code>brewery</code></li> <li><code>user</code></li> </ul> <p>All those high cardinality variables can be thought as links of a network. Indeed, a review is an object comprising a beer, a brewery and a user and can be thought as a network link between them.</p> <p>In other words, the review table is the a table describing the links in a network with 3 types of nodes: users, beers and breweries.</p> <p>The first property to compute about each node is its \"degree\", which is its number of connections with other nodes. High degree means \"highly connected\".</p> <p>To compute the degree you'll need:</p> <ul> <li><code>pd.Series.value_counts</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering_students/#text-length","title":"Text length\u00b6","text":"<p>Compute the length of the texts in the dataset.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.Series.str.len</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering_students/#convert-timestamp","title":"Convert timestamp\u00b6","text":""},{"location":"10-Tutorials/1_data_engineering_students/#binary-target","title":"Binary target\u00b6","text":"<p>The prediction problem is to predict <code>rating</code> based on other information. Since <code>rating</code> is a numeric variable, it is a regression problem. We'd like also to do a classification so we'll create a binary target based on the <code>rating</code>.</p> <p>The <code>\u00ecs_good</code> column is True if the rating is above or equal to 16 (expert judgement), False otherwise.</p> <p>Note: also convert the binary target to integer (O or 1) for better readability.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.Series.astype</code></li> </ul>"},{"location":"10-Tutorials/1_data_engineering_students/#combine-dataframes","title":"Combine dataframes\u00b6","text":"<p>Create a dataframe combining information from:</p> <ul> <li><code>df_raw</code>: the original dataset</li> <li><code>df_beer_degree</code>: merged on <code>beer</code> column</li> <li><code>df_brewery_degree</code>: merged on <code>brewery</code> column</li> <li><code>df_user_degree</code>: merged on <code>user</code> column</li> <li><code>text_length</code>: added as a new column</li> <li><code>date</code>: added as a new column</li> <li><code>is_good</code>: added as a new column</li> </ul> <p>Note: <code>merge</code> is the equivalent of <code>JOIN</code> in SQL, and it changes the order of the rows ! So to add our data columns properly in the dataset, we have 2 options:</p> <ul> <li>add the new columns using precomputed arrays, but before merging (not recommended): e.g: <code>df_raw.text.str.len()</code></li> <li>add the new columns using a function (recommended): e.g: <code>lambda df: df.text.str.len()</code></li> </ul> <p>Hint:</p> <ul> <li><code>pd.DataFrame.merge</code></li> <li><code>pd.DataFrame.assign</code></li> </ul> <p>Note: If some columns in both the left &amp; right dataframes have the same name, you'll obtain duplicated columns in the merge result. <code>pandas</code> adds the suffixes <code>_x</code> and <code>_y</code> to avoid creating duplicate columns. Use the <code>suffixes</code> argument to specify the suffixes to apply to duplicated columns. In this example, there is no common column name in both dataframes.</p> <p>We made lots of transformation to our datasets: we want to verify that all values in the \"primary keys\" columns are indeed unique. Use the <code>validate</code> argument to do so.</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis/","title":"\ud83d\udc0d Practice n\u00b02: exploratory data analysis","text":"In\u00a0[1]: Copied! <pre>! pip install seaborn\n</pre> ! pip install seaborn <pre>Requirement already satisfied: seaborn in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (0.12.2)\r\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.17 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from seaborn) (1.26.0)\r\nRequirement already satisfied: pandas&gt;=0.25 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from seaborn) (2.1.1)\r\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.1 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from seaborn) (3.8.0)\r\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (1.1.1)\r\nRequirement already satisfied: cycler&gt;=0.10 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (0.11.0)\r\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (4.42.1)\r\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (1.4.5)\r\nRequirement already satisfied: packaging&gt;=20.0 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (23.1)\r\nRequirement already satisfied: pillow&gt;=6.2.0 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (10.0.1)\r\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (3.1.1)\r\nRequirement already satisfied: python-dateutil&gt;=2.7 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (2.8.2)\r\nRequirement already satisfied: pytz&gt;=2020.1 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from pandas&gt;=0.25-&gt;seaborn) (2023.3.post1)\r\nRequirement already satisfied: tzdata&gt;=2022.1 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from pandas&gt;=0.25-&gt;seaborn) (2023.3)\r\nRequirement already satisfied: six&gt;=1.5 in /home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (1.16.0)\r\n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd import seaborn as sns import matplotlib.pyplot as plt In\u00a0[3]: Copied! <pre>pd.set_option(\"display.precision\", 2)\nsns.set_style(\"whitegrid\")\nsns.set_context(rc={\"patch.linewidth\": 0.15})\n</pre> pd.set_option(\"display.precision\", 2) sns.set_style(\"whitegrid\") sns.set_context(rc={\"patch.linewidth\": 0.15}) In\u00a0[4]: Copied! <pre>file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\"\n</pre> file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\" In\u00a0[5]: Copied! <pre>df = pd.read_parquet(file_url)\n</pre> df = pd.read_parquet(file_url) In\u00a0[6]: Copied! <pre>df.shape\n</pre> df.shape Out[6]: <pre>(200000, 18)</pre> In\u00a0[7]: Copied! <pre>df.info(memory_usage=\"deep\")  # LINE TO BE REMOVED FOR STUDENTS\n</pre> df.info(memory_usage=\"deep\")  # LINE TO BE REMOVED FOR STUDENTS <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200000 entries, 0 to 199999\nData columns (total 18 columns):\n #   Column             Non-Null Count   Dtype         \n---  ------             --------------   -----         \n 0   beer               200000 non-null  object        \n 1   brewery            200000 non-null  object        \n 2   alcohol            200000 non-null  float64       \n 3   type               200000 non-null  object        \n 4   rating_appearance  200000 non-null  int64         \n 5   rating_aroma       200000 non-null  int64         \n 6   rating_palate      200000 non-null  int64         \n 7   rating_taste       200000 non-null  int64         \n 8   rating             200000 non-null  int64         \n 9   timestamp          200000 non-null  object        \n 10  user               200000 non-null  object        \n 11  text               200000 non-null  object        \n 12  beer_degree        200000 non-null  int64         \n 13  brewery_degree     200000 non-null  int64         \n 14  user_degree        200000 non-null  int64         \n 15  text_length        200000 non-null  int64         \n 16  date               200000 non-null  datetime64[us]\n 17  is_good            200000 non-null  int64         \ndtypes: datetime64[us](1), float64(1), int64(10), object(6)\nmemory usage: 153.7 MB\n</pre> In\u00a0[8]: Copied! <pre>df\n</pre> df Out[8]: beer brewery alcohol type rating_appearance rating_aroma rating_palate rating_taste rating timestamp user text beer_degree brewery_degree user_degree text_length date is_good 0 Breckenridge Oatmeal Stout 383 4.95 Stout 4 7 4 7 14 1217462400 blutt59 bottle, oat nose with black color, bitter choc... 51 413 300 90 2008-07-31 02:00:00 0 1 Breckenridge 471 Small Batch Imperial Porter 383 7.50 Imperial/Strong Porter 3 8 3 8 14 1312588800 blutt59 bottle, received in trade, dark brown with gar... 4 413 300 191 2011-08-06 02:00:00 0 2 Breckenridge Avalanche Amber 383 5.41 Amber Ale 3 5 3 5 10 1205020800 blutt59 12 oz. bottle, amber color with soapy head, sl... 43 413 300 109 2008-03-09 01:00:00 0 3 Breckenridge Lucky U IPA 383 6.20 India Pale Ale (IPA) 3 6 3 7 12 1255737600 blutt59 bottle, golden orange color with light tan foa... 20 413 300 151 2009-10-17 02:00:00 0 4 Fullers Vintage Ale 2009 55 8.50 English Strong Ale 3 7 3 8 14 1282003200 blutt59 bottle, thanks to SS, almond amber colored pou... 18 978 300 159 2010-08-17 02:00:00 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 199995 Die M Dunkel 6587 -1.00 Dunkel 4 5 3 3 8 1204070400 kuleko Bottle, supermarket. Very dark colour. Sweet c... 2 2 1 163 2008-02-27 01:00:00 0 199996 Montt Hafen Porter 8240 6.00 Porter 5 10 5 10 19 1175040000 Andressantis Sabrosa,con amargor justo y ligeramente dulce.... 1 1 1 97 2007-03-28 02:00:00 1 199997 Oxymore 8022 5.00 Saison 4 8 3 6 12 1181952000 DesmondLangham Very earthy nose.\\tClear and golden, head kept... 1 1 1 192 2007-06-16 02:00:00 0 199998 Stonehouse Station Bitter 8561 3.90 Bitter 3 5 4 6 13 1238716800 thedees Cask at the Three Pigeons Nescliffe. Maly swee... 1 2 1 88 2009-04-03 02:00:00 0 199999 Pilker Negra 6687 5.00 Stout 4 6 2 5 5 1142553600 apeters Roasty aroma.Black color.Dry and bitter flavor... 1 1 1 144 2006-03-17 01:00:00 0 <p>200000 rows \u00d7 18 columns</p> In\u00a0[9]: Copied! <pre>df.describe(include=\"all\").fillna(\"\").T\n</pre> df.describe(include=\"all\").fillna(\"\").T Out[9]: count unique top freq mean min 25% 50% 75% max std beer 200000 42125 Guinness Draught 252 brewery 200000 5413 32 3217 alcohol 200000.0 6.28 -1.0 5.0 5.8 8.0 57.7 2.78 type 200000 89 India Pale Ale (IPA) 12698 rating_appearance 200000.0 3.43 1.0 3.0 3.0 4.0 5.0 0.81 rating_aroma 200000.0 6.35 1.0 6.0 7.0 7.0 10.0 1.64 rating_palate 200000.0 3.26 1.0 3.0 3.0 4.0 5.0 0.83 rating_taste 200000.0 6.45 1.0 6.0 7.0 8.0 10.0 1.62 rating 200000.0 13.19 1.0 12.0 14.0 15.0 20.0 3.35 timestamp 200000 4105 1188604800 392 user 200000 11251 fonefan 1146 text 200000 199261 334 beer_degree 200000.0 34.88 1.0 4.0 16.0 50.0 252.0 44.31 brewery_degree 200000.0 605.39 1.0 68.0 304.0 828.0 3217.0 748.4 user_degree 200000.0 156.38 1.0 34.0 97.0 204.0 1146.0 185.3 text_length 200000.0 307.46 0.0 176.0 254.0 375.0 7989.0 212.23 date 200000 2008-03-08 05:48:03.672000 2000-04-23 02:00:00 2006-05-23 02:00:00 2008-07-08 02:00:00 2010-05-08 02:00:00 2012-01-13 01:00:00 is_good 200000.0 0.24 0.0 0.0 0.0 0.0 1.0 0.43 In\u00a0[10]: Copied! <pre>(\n    (df.is_good)\n    .value_counts()\n    .plot.bar()  # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> (     (df.is_good)     .value_counts()     .plot.bar()  # LINE TO BE REMOVED FOR STUDENTS ) Out[10]: <pre>&lt;Axes: xlabel='is_good'&gt;</pre> In\u00a0[11]: Copied! <pre>sns.countplot(\n    df,\n    x=\"is_good\",  # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> sns.countplot(     df,     x=\"is_good\",  # LINE TO BE REMOVED FOR STUDENTS ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n</pre> Out[11]: <pre>&lt;Axes: xlabel='is_good', ylabel='count'&gt;</pre> In\u00a0[12]: Copied! <pre>((df.alcohol).plot.hist(bins=100))  # LINE TO BE REMOVED FOR STUDENTS\n</pre> ((df.alcohol).plot.hist(bins=100))  # LINE TO BE REMOVED FOR STUDENTS Out[12]: <pre>&lt;Axes: ylabel='Frequency'&gt;</pre> In\u00a0[13]: Copied! <pre>sns.displot(\n    df,\n    x=\"alcohol\",\n)\n</pre> sns.displot(     df,     x=\"alcohol\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[13]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac4fb92790&gt;</pre> In\u00a0[14]: Copied! <pre>sns.displot(df, x=\"alcohol\", hue=\"is_good\")\n</pre> sns.displot(df, x=\"alcohol\", hue=\"is_good\") <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[14]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac4f0af690&gt;</pre> In\u00a0[15]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.alcohol &lt; 20], x=\"alcohol\", hue=\"is_good\"\n)\n</pre> sns.displot(     df.loc[lambda df: df.alcohol &lt; 20], x=\"alcohol\", hue=\"is_good\" ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[15]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac4e8c70d0&gt;</pre> In\u00a0[16]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.alcohol &lt; 20],\n    x=\"alcohol\",\n    hue=\"is_good\",\n    kde=True,\n)\n</pre> sns.displot(     df.loc[lambda df: df.alcohol &lt; 20],     x=\"alcohol\",     hue=\"is_good\",     kde=True, ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[16]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac5493dc50&gt;</pre> In\u00a0[17]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.alcohol &lt; 20],\n    x=\"alcohol\",\n    hue=\"is_good\",\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df.loc[lambda df: df.alcohol &lt; 20],     x=\"alcohol\",     hue=\"is_good\",     multiple=\"fill\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[17]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac4e386290&gt;</pre> In\u00a0[18]: Copied! <pre>sns.displot(\n    df,\n    x=\"date\",  # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> sns.displot(     df,     x=\"date\",  # LINE TO BE REMOVED FOR STUDENTS ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[18]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac4dc46c50&gt;</pre> In\u00a0[19]: Copied! <pre>sns.displot(\n    df,\n    x=\"date\",  # LINE TO BE REMOVED FOR STUDENTS\n    hue=\"is_good\",  # LINE TO BE REMOVED FOR STUDENTS\n    kde=True,\n)\n</pre> sns.displot(     df,     x=\"date\",  # LINE TO BE REMOVED FOR STUDENTS     hue=\"is_good\",  # LINE TO BE REMOVED FOR STUDENTS     kde=True, ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[19]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac4db6cb90&gt;</pre> In\u00a0[20]: Copied! <pre>sns.displot(\n    df,\n    x=\"date\",  # LINE TO BE REMOVED FOR STUDENTS\n    hue=\"is_good\",  # LINE TO BE REMOVED FOR STUDENTS\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df,     x=\"date\",  # LINE TO BE REMOVED FOR STUDENTS     hue=\"is_good\",  # LINE TO BE REMOVED FOR STUDENTS     multiple=\"fill\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[20]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac4e68fe10&gt;</pre> In\u00a0[21]: Copied! <pre>review_columns = [\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",  # LINE TO BE REMOVED FOR STUDENTS\n    \"rating_taste\",  # LINE TO BE REMOVED FOR STUDENTS\n]\ndf_rating_long = df.melt(id_vars=\"is_good\", value_vars=review_columns)\ndf_rating_long\n</pre> review_columns = [     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",  # LINE TO BE REMOVED FOR STUDENTS     \"rating_taste\",  # LINE TO BE REMOVED FOR STUDENTS ] df_rating_long = df.melt(id_vars=\"is_good\", value_vars=review_columns) df_rating_long Out[21]: is_good variable value 0 0 rating_appearance 4 1 0 rating_appearance 3 2 0 rating_appearance 3 3 0 rating_appearance 3 4 0 rating_appearance 3 ... ... ... ... 799995 0 rating_taste 3 799996 1 rating_taste 10 799997 0 rating_taste 6 799998 0 rating_taste 6 799999 0 rating_taste 5 <p>800000 rows \u00d7 3 columns</p> In\u00a0[22]: Copied! <pre>sns.displot(\n    df_rating_long,\n    x=\"value\",\n    row=\"variable\",\n    discrete=True,\n    height=3,\n    aspect=2,\n)\n</pre> sns.displot(     df_rating_long,     x=\"value\",     row=\"variable\",     discrete=True,     height=3,     aspect=2, ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[22]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac4d83e810&gt;</pre> In\u00a0[23]: Copied! <pre>sns.displot(\n    df_rating_long,\n    x=\"value\",\n    hue=\"is_good\",\n    row=\"variable\",\n    discrete=True,\n    height=3,\n    aspect=2,\n)\n</pre> sns.displot(     df_rating_long,     x=\"value\",     hue=\"is_good\",     row=\"variable\",     discrete=True,     height=3,     aspect=2, ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[23]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac4d996b90&gt;</pre> In\u00a0[24]: Copied! <pre>sns.displot(\n    df_rating_long,\n    x=\"value\",\n    hue=\"is_good\",\n    row=\"variable\",\n    discrete=True,\n    height=3,\n    aspect=2,\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df_rating_long,     x=\"value\",     hue=\"is_good\",     row=\"variable\",     discrete=True,     height=3,     aspect=2,     multiple=\"fill\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[24]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac11005010&gt;</pre> In\u00a0[25]: Copied! <pre>(\n    (df.type)\n    .value_counts()\n    .plot.bar()\n)\n</pre> (     (df.type)     .value_counts()     .plot.bar() ) Out[25]: <pre>&lt;Axes: xlabel='type'&gt;</pre> In\u00a0[26]: Copied! <pre>sns.displot(\n    df,\n    x=\"type\",\n    discrete=True,\n    hue=\"is_good\",\n)\n</pre> sns.displot(     df,     x=\"type\",     discrete=True,     hue=\"is_good\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[26]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac110496d0&gt;</pre> In\u00a0[27]: Copied! <pre>df_styles = (\n    (df)\n    .groupby(\"type\")\n    .is_good.agg([\"count\", \"mean\"])\n    .add_prefix(\"review_\")\n    .reset_index()\n    .sort_values(by=\"review_mean\", ascending=False)\n    .reset_index(drop=True)\n    .assign(\n        bar_left_position=lambda df: df.review_count.cumsum().shift(1, fill_value=0)\n    )\n)\ndf_styles\n</pre> df_styles = (     (df)     .groupby(\"type\")     .is_good.agg([\"count\", \"mean\"])     .add_prefix(\"review_\")     .reset_index()     .sort_values(by=\"review_mean\", ascending=False)     .reset_index(drop=True)     .assign(         bar_left_position=lambda df: df.review_count.cumsum().shift(1, fill_value=0)     ) ) df_styles Out[27]: type review_count review_mean bar_left_position 0 Imperial Stout 8460 0.63 0 1 Ice Cider/Perry 131 0.56 8460 2 Abt/Quadrupel 1327 0.55 8591 3 Sour Ale/Wild Ale 3264 0.55 9918 4 Imperial/Double IPA 7215 0.53 13182 ... ... ... ... ... 84 Sak - Infused 16 0.00 199959 85 Sak - Honjozo 5 0.00 199975 86 Sak - Genshu 6 0.00 199980 87 Sak - Taru 9 0.00 199986 88 Sak - Tokubetsu 5 0.00 199995 <p>89 rows \u00d7 4 columns</p> In\u00a0[28]: Copied! <pre>plt.bar(\n    x=df_styles.bar_left_position,\n    height=df_styles.review_mean,\n    width=df_styles.review_count,\n    align=\"edge\",\n    alpha=0.5,\n    edgecolor=\"k\",\n    linewidth=0.5,\n)\n</pre> plt.bar(     x=df_styles.bar_left_position,     height=df_styles.review_mean,     width=df_styles.review_count,     align=\"edge\",     alpha=0.5,     edgecolor=\"k\",     linewidth=0.5, ) Out[28]: <pre>&lt;BarContainer object of 89 artists&gt;</pre> <p>All those high cardinality variables can be thought as links of a network. Indeed, a review is an object comprising a beer, a brewery and a user and can be thought as a network link between them.</p> <p>In other words, the review table is the a table describing the links in a network with 3 types of nodes: users, beers and breweries.</p> <p>The first property to compute about each node is its \"degree\", which is its number of connections with other nodes. High degree means \"highly connected\".</p> <p>Analyse the degree of the nodes is a way to answer the following questions:</p> <ul> <li>is an experienced user more severe ?</li> <li>is a new user more forgiving ?</li> <li>is a popular beer (or a big brewery) disadvantaged by a \"boreness factor\" ?</li> <li>is a new beer (a small brewery) benefitting from a \"novelty factor\" ?</li> </ul> <p>To compute the degree you'll need:</p> <ul> <li><code>pd.Series.value_counts</code></li> </ul> In\u00a0[29]: Copied! <pre>df_beer_degree = df.loc[:, [\"beer\", \"beer_degree\"]].drop_duplicates()\ndf_beer_degree\n</pre> df_beer_degree = df.loc[:, [\"beer\", \"beer_degree\"]].drop_duplicates() df_beer_degree Out[29]: beer beer_degree 0 Breckenridge Oatmeal Stout 51 1 Breckenridge 471 Small Batch Imperial Porter 4 2 Breckenridge Avalanche Amber 43 3 Breckenridge Lucky U IPA 20 4 Fullers Vintage Ale 2009 18 ... ... ... 199994 Acadie-Broue La Patente 1 199996 Montt Hafen Porter 1 199997 Oxymore 1 199998 Stonehouse Station Bitter 1 199999 Pilker Negra 1 <p>42125 rows \u00d7 2 columns</p> In\u00a0[30]: Copied! <pre>(\n    (df_beer_degree.beer_degree)\n    .value_counts()\n    .reset_index()\n    .plot.scatter(\n        x=\"beer_degree\", y=\"count\", marker=\".\"  # LINE TO BE REMOVED FOR STUDENTS\n    )\n)\n</pre> (     (df_beer_degree.beer_degree)     .value_counts()     .reset_index()     .plot.scatter(         x=\"beer_degree\", y=\"count\", marker=\".\"  # LINE TO BE REMOVED FOR STUDENTS     ) ) Out[30]: <pre>&lt;Axes: xlabel='beer_degree', ylabel='count'&gt;</pre> <p>Many networks are scale free networks: </p> <p>Meaning that the node's degree distribution follows a power law distribution, which is better visualized using a log-log scale: </p> In\u00a0[31]: Copied! <pre>(\n    (df_beer_degree.beer_degree)\n    .value_counts()\n    .reset_index()\n    .plot.scatter(  # LINE TO BE REMOVED FOR STUDENTS\n        x=\"beer_degree\",  # LINE TO BE REMOVED FOR STUDENTS\n        y=\"count\",  # LINE TO BE REMOVED FOR STUDENTS\n        loglog=True,  # LINE TO BE REMOVED FOR STUDENTS\n        marker=\".\"  # LINE TO BE REMOVED FOR STUDENTS\n    )  # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> (     (df_beer_degree.beer_degree)     .value_counts()     .reset_index()     .plot.scatter(  # LINE TO BE REMOVED FOR STUDENTS         x=\"beer_degree\",  # LINE TO BE REMOVED FOR STUDENTS         y=\"count\",  # LINE TO BE REMOVED FOR STUDENTS         loglog=True,  # LINE TO BE REMOVED FOR STUDENTS         marker=\".\"  # LINE TO BE REMOVED FOR STUDENTS     )  # LINE TO BE REMOVED FOR STUDENTS ) Out[31]: <pre>&lt;Axes: xlabel='beer_degree', ylabel='count'&gt;</pre> <p>Another way to visualize the power law distribution is to visualize the rank-frequency plot.</p> <p>Instead of plotting the frequency as a function of the value (like in a normal distribution plot), we plot the frequency as a function of its rank. Note: It's always monotonically decreasing.</p> <p>Power law distributions exhibit also a distinctive visual pattern in the rank-frequency plot, known as the Zipf's law: </p> In\u00a0[32]: Copied! <pre>def plot_rank_size(series):\n    return (\n        (series)\n        .rename(\"\")\n        .value_counts()\n        .reset_index()\n        .assign(rank=lambda df: range(1, 1 + df.shape[0]))\n        .plot(x=\"rank\", y=\"count\", loglog=True, marker=\".\")\n    )\n</pre> def plot_rank_size(series):     return (         (series)         .rename(\"\")         .value_counts()         .reset_index()         .assign(rank=lambda df: range(1, 1 + df.shape[0]))         .plot(x=\"rank\", y=\"count\", loglog=True, marker=\".\")     ) In\u00a0[33]: Copied! <pre>plot_rank_size(df_beer_degree.beer_degree)\n</pre> plot_rank_size(df_beer_degree.beer_degree) Out[33]: <pre>&lt;Axes: xlabel='rank'&gt;</pre> In\u00a0[34]: Copied! <pre>sns.displot(df, x=\"beer_degree\", hue=\"is_good\")\n</pre> sns.displot(df, x=\"beer_degree\", hue=\"is_good\") <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[34]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac10416c50&gt;</pre> In\u00a0[35]: Copied! <pre>sns.displot(df, x=\"beer_degree\", hue=\"is_good\", log_scale=True)\n</pre> sns.displot(df, x=\"beer_degree\", hue=\"is_good\", log_scale=True) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[35]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0ff42610&gt;</pre> In\u00a0[36]: Copied! <pre>sns.displot(\n    df,\n    x=\"beer_degree\",\n    hue=\"is_good\",\n    log_scale=True,\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df,     x=\"beer_degree\",     hue=\"is_good\",     log_scale=True,     multiple=\"fill\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[36]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0fc496d0&gt;</pre> In\u00a0[37]: Copied! <pre>df_brewery_degree = df.loc[:, [\"brewery\", \"brewery_degree\"]].drop_duplicates()\ndf_brewery_degree\n</pre> df_brewery_degree = df.loc[:, [\"brewery\", \"brewery_degree\"]].drop_duplicates() df_brewery_degree Out[37]: brewery brewery_degree 0 383 413 4 55 978 5 32 3217 9 284 610 10 149 400 ... ... ... 199992 13436 2 199994 12747 1 199996 8240 1 199997 8022 1 199999 6687 1 <p>5413 rows \u00d7 2 columns</p> In\u00a0[38]: Copied! <pre>(\n    (df_brewery_degree.brewery_degree)\n    .value_counts()\n    .reset_index()\n    .plot.scatter(  # LINE TO BE REMOVED FOR STUDENTS\n        x=\"brewery_degree\",  # LINE TO BE REMOVED FOR STUDENTS\n        y=\"count\",  # LINE TO BE REMOVED FOR STUDENTS\n        marker=\".\",  # LINE TO BE REMOVED FOR STUDENTS\n    )  # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> (     (df_brewery_degree.brewery_degree)     .value_counts()     .reset_index()     .plot.scatter(  # LINE TO BE REMOVED FOR STUDENTS         x=\"brewery_degree\",  # LINE TO BE REMOVED FOR STUDENTS         y=\"count\",  # LINE TO BE REMOVED FOR STUDENTS         marker=\".\",  # LINE TO BE REMOVED FOR STUDENTS     )  # LINE TO BE REMOVED FOR STUDENTS ) Out[38]: <pre>&lt;Axes: xlabel='brewery_degree', ylabel='count'&gt;</pre> In\u00a0[39]: Copied! <pre>(\n    (df_brewery_degree.brewery_degree)\n    .value_counts()\n    .reset_index()\n    .plot.scatter(  # LINE TO BE REMOVED FOR STUDENTS\n        x=\"brewery_degree\",  # LINE TO BE REMOVED FOR STUDENTS\n        y=\"count\",  # LINE TO BE REMOVED FOR STUDENTS\n        loglog=True,  # LINE TO BE REMOVED FOR STUDENTS\n        marker=\".\",  # LINE TO BE REMOVED FOR STUDENTS\n    )  # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> (     (df_brewery_degree.brewery_degree)     .value_counts()     .reset_index()     .plot.scatter(  # LINE TO BE REMOVED FOR STUDENTS         x=\"brewery_degree\",  # LINE TO BE REMOVED FOR STUDENTS         y=\"count\",  # LINE TO BE REMOVED FOR STUDENTS         loglog=True,  # LINE TO BE REMOVED FOR STUDENTS         marker=\".\",  # LINE TO BE REMOVED FOR STUDENTS     )  # LINE TO BE REMOVED FOR STUDENTS ) Out[39]: <pre>&lt;Axes: xlabel='brewery_degree', ylabel='count'&gt;</pre> In\u00a0[40]: Copied! <pre>plot_rank_size(df_brewery_degree.brewery_degree)\n</pre> plot_rank_size(df_brewery_degree.brewery_degree) Out[40]: <pre>&lt;Axes: xlabel='rank'&gt;</pre> In\u00a0[41]: Copied! <pre>sns.displot(df, x=\"brewery_degree\", hue=\"is_good\")\n</pre> sns.displot(df, x=\"brewery_degree\", hue=\"is_good\") <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[41]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0f8b3c10&gt;</pre> In\u00a0[42]: Copied! <pre>sns.displot(\n    df, x=\"brewery_degree\", hue=\"is_good\", log_scale=True\n)\n</pre> sns.displot(     df, x=\"brewery_degree\", hue=\"is_good\", log_scale=True ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[42]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0f328f10&gt;</pre> In\u00a0[43]: Copied! <pre>sns.displot(\n    df,\n    x=\"brewery_degree\",\n    hue=\"is_good\",\n    log_scale=True,\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df,     x=\"brewery_degree\",     hue=\"is_good\",     log_scale=True,     multiple=\"fill\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[43]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0f8d1490&gt;</pre> In\u00a0[44]: Copied! <pre>df_user_degree = df.loc[:, [\"user\", \"user_degree\"]].drop_duplicates()\ndf_user_degree\n</pre> df_user_degree = df.loc[:, [\"user\", \"user_degree\"]].drop_duplicates() df_user_degree Out[44]: user user_degree 0 blutt59 300 300 VAYankee 23 323 garretjax 7 330 randolphjj 10 340 Leafs93 56 ... ... ... 199995 kuleko 1 199996 Andressantis 1 199997 DesmondLangham 1 199998 thedees 1 199999 apeters 1 <p>11251 rows \u00d7 2 columns</p> In\u00a0[45]: Copied! <pre>(\n    (df_user_degree.user_degree)\n    .value_counts()\n    .reset_index()\n    .plot.scatter(  # LINE TO BE REMOVED FOR STUDENTS\n        x=\"user_degree\",  # LINE TO BE REMOVED FOR STUDENTS\n        y=\"count\",  # LINE TO BE REMOVED FOR STUDENTS\n        marker=\".\",  # LINE TO BE REMOVED FOR STUDENTS\n    )  # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> (     (df_user_degree.user_degree)     .value_counts()     .reset_index()     .plot.scatter(  # LINE TO BE REMOVED FOR STUDENTS         x=\"user_degree\",  # LINE TO BE REMOVED FOR STUDENTS         y=\"count\",  # LINE TO BE REMOVED FOR STUDENTS         marker=\".\",  # LINE TO BE REMOVED FOR STUDENTS     )  # LINE TO BE REMOVED FOR STUDENTS ) Out[45]: <pre>&lt;Axes: xlabel='user_degree', ylabel='count'&gt;</pre> In\u00a0[46]: Copied! <pre>(\n    (df_user_degree.user_degree)\n    .value_counts()\n    .reset_index()\n    .plot.scatter(  # LINE TO BE REMOVED FOR STUDENTS\n        x=\"user_degree\",  # LINE TO BE REMOVED FOR STUDENTS\n        y=\"count\",  # LINE TO BE REMOVED FOR STUDENTS\n        loglog=True,  # LINE TO BE REMOVED FOR STUDENTS\n        marker=\".\",  # LINE TO BE REMOVED FOR STUDENTS\n    )  # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> (     (df_user_degree.user_degree)     .value_counts()     .reset_index()     .plot.scatter(  # LINE TO BE REMOVED FOR STUDENTS         x=\"user_degree\",  # LINE TO BE REMOVED FOR STUDENTS         y=\"count\",  # LINE TO BE REMOVED FOR STUDENTS         loglog=True,  # LINE TO BE REMOVED FOR STUDENTS         marker=\".\",  # LINE TO BE REMOVED FOR STUDENTS     )  # LINE TO BE REMOVED FOR STUDENTS ) Out[46]: <pre>&lt;Axes: xlabel='user_degree', ylabel='count'&gt;</pre> In\u00a0[47]: Copied! <pre>plot_rank_size(df_user_degree.user_degree)\n</pre> plot_rank_size(df_user_degree.user_degree) Out[47]: <pre>&lt;Axes: xlabel='rank'&gt;</pre> In\u00a0[48]: Copied! <pre>sns.displot(\n    df, x=\"user_degree\", hue=\"is_good\"\n)\n</pre> sns.displot(     df, x=\"user_degree\", hue=\"is_good\" ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[48]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0efb1050&gt;</pre> In\u00a0[49]: Copied! <pre>sns.displot(\n    df,\n    x=\"user_degree\",\n    hue=\"is_good\",\n    log_scale=True,\n)\n</pre> sns.displot(     df,     x=\"user_degree\",     hue=\"is_good\",     log_scale=True, ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[49]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0ea93750&gt;</pre> In\u00a0[50]: Copied! <pre>sns.displot(\n    df,\n    x=\"user_degree\",\n    hue=\"is_good\",\n    log_scale=True,\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df,     x=\"user_degree\",     hue=\"is_good\",     log_scale=True,     multiple=\"fill\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[50]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0e51fbd0&gt;</pre> In\u00a0[51]: Copied! <pre>(df.text_length).plot.hist(bins=200)  # LINE TO BE REMOVED FOR STUDENTS\n</pre> (df.text_length).plot.hist(bins=200)  # LINE TO BE REMOVED FOR STUDENTS Out[51]: <pre>&lt;Axes: ylabel='Frequency'&gt;</pre> In\u00a0[52]: Copied! <pre>((df.text_length).plot.hist(bins=200, logy=True))\n</pre> ((df.text_length).plot.hist(bins=200, logy=True)) Out[52]: <pre>&lt;Axes: ylabel='Frequency'&gt;</pre> In\u00a0[53]: Copied! <pre>sns.displot(df, x=\"text_length\")\n</pre> sns.displot(df, x=\"text_length\") <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[53]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0e153b90&gt;</pre> In\u00a0[54]: Copied! <pre>sns.displot(\n    df,\n    x=\"text_length\",\n    hue=\"is_good\",\n)\n</pre> sns.displot(     df,     x=\"text_length\",     hue=\"is_good\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[54]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0d4efe10&gt;</pre> In\u00a0[55]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.text_length &lt; 1500],\n    x=\"text_length\",\n    hue=\"is_good\",\n)\n</pre> sns.displot(     df.loc[lambda df: df.text_length &lt; 1500],     x=\"text_length\",     hue=\"is_good\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[55]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0c3cbcd0&gt;</pre> In\u00a0[56]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.text_length &lt; 1500],\n    x=\"text_length\",\n    hue=\"is_good\",\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df.loc[lambda df: df.text_length &lt; 1500],     x=\"text_length\",     hue=\"is_good\",     multiple=\"fill\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[56]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac0c311cd0&gt;</pre> In\u00a0[57]: Copied! <pre>(\n    (df)\n    .head(100000)\n    .assign(\n        tokenized_text=lambda df: (df.text)\n        .str.lower()\n        .str.replace(r\"[^a-z]\", \" \")\n        .str.replace(r\" +\", \" \")\n        .str.split(\" \")\n    )\n    .loc[:, [\"rating\", \"tokenized_text\"]]\n    .explode(\"tokenized_text\")\n    .loc[lambda df: df.tokenized_text != \"\"]\n    .groupby(\"tokenized_text\", as_index=False)\n    .agg([\"mean\", \"count\"])\n    .reset_index()\n    .sort_values(by=(\"rating\", \"count\"), ascending=False)\n    .head(200)\n    .style.background_gradient(cmap=\"RdYlGn\")\n)\n</pre> (     (df)     .head(100000)     .assign(         tokenized_text=lambda df: (df.text)         .str.lower()         .str.replace(r\"[^a-z]\", \" \")         .str.replace(r\" +\", \" \")         .str.split(\" \")     )     .loc[:, [\"rating\", \"tokenized_text\"]]     .explode(\"tokenized_text\")     .loc[lambda df: df.tokenized_text != \"\"]     .groupby(\"tokenized_text\", as_index=False)     .agg([\"mean\", \"count\"])     .reset_index()     .sort_values(by=(\"rating\", \"count\"), ascending=False)     .head(200)     .style.background_gradient(cmap=\"RdYlGn\") ) Out[57]: index tokenized_text rating mean count 23275 23275 a 13.586334 241381 28030 28030 and 13.721123 212352 148797 148797 the 13.713381 169186 161860 161860 with 13.563324 156450 112885 112885 of 13.675712 133776 90940 90940 is 13.447157 109834 150780 150780 to 13.522130 58857 89101 89101 in 13.507647 49298 97612 97612 light 12.859438 47296 82869 82869 head. 13.214893 47270 29776 29776 aroma 13.304910 46335 138634 138634 some 13.531472 44992 44510 44510 but 13.407445 44109 149542 149542 this 13.633697 42211 157022 157022 very 13.926570 39943 88160 88160 i 13.446812 39135 145181 145181 sweet 13.353797 37920 72292 72292 flavor 13.438029 32943 91152 91152 it 13.442805 32057 111067 111067 not 12.693228 30922 160838 160838 white 12.765617 30655 148708 148708 that 13.628333 30191 113892 113892 on 13.704276 29913 31158 31158 at 13.454488 28498 58150 58150 dark 14.380695 27723 109609 109609 nice 14.524580 26078 122501 122501 pours 13.746651 25230 103581 103581 medium 13.422387 25086 30626 30626 as 13.803236 24471 100764 100764 malt 13.348653 24196 158640 158640 was 13.377105 22503 73925 73925 for 13.503043 21360 71204 71204 finish. 13.413420 21327 82746 82746 head 13.647756 20923 37198 37198 bit 13.368343 20899 43168 43168 brown 14.151154 20833 34593 34593 beer 13.235946 20564 78974 78974 good 13.946021 20156 147289 147289 taste 13.303670 18609 40087 40087 bottle 13.538968 18079 75063 75063 from 13.663889 17399 27959 27959 an 13.509422 17141 39125 39125 body 13.382865 16878 97915 97915 like 13.042391 16442 37357 37357 bitter 13.499561 15952 98706 98706 little 13.228940 15812 45933 45933 caramel 13.783775 15655 86751 86751 hops 13.759464 15137 111258 111258 notes 13.309095 14701 27424 27424 amber 13.368961 14511 86086 86086 hop 14.022538 14376 106960 106960 more 13.861212 14324 53151 53151 color 13.430517 14320 63943 63943 dry 13.494540 13827 82338 82338 has 13.569667 13299 136869 136869 slightly 13.206275 13196 51533 51533 clear 12.633244 12736 78761 78761 golden 12.470602 12688 101560 101560 malty 12.990689 11922 29931 29931 aroma. 12.967441 11794 149239 149239 thin 12.733708 11754 137156 137156 small 12.979860 11569 9029 9029 - 13.323211 11559 91489 91489 its 13.395435 11390 71131 71131 finish 13.701222 11373 114591 114591 orange 13.661272 11369 129833 129833 roasted 14.284308 11273 125314 125314 quite 13.656752 11196 110779 110779 nose 13.751688 10813 92905 92905 just 13.294800 10597 75764 75764 fruity 13.565176 10579 40285 40285 bottle. 13.097241 10510 49752 49752 chocolate 14.750944 10323 108504 108504 my 13.549403 9969 107899 107899 much 12.856110 9959 82665 82665 hazy 13.491174 9801 100788 100788 malt, 13.555246 9693 126698 126698 really 13.882165 9683 50818 50818 citrus 14.087832 9632 113998 113998 one 13.868399 9582 105037 105037 mild 13.179398 9465 146515 146515 tan 14.401638 9399 44828 44828 by 13.822816 9318 56539 56539 creamy 14.200086 9311 159634 159634 well 14.450652 9129 25862 25862 alcohol 14.389866 8980 29570 29570 are 13.712486 8970 34777 34777 beer. 13.502987 8871 38010 38010 black 14.801697 8840 45947 45947 caramel, 13.911252 8834 148625 148625 than 13.355771 8750 110115 110115 no 12.417195 8677 86895 86895 hops. 13.367686 8374 123193 123193 pretty 13.506425 8327 97827 97827 lightly 13.027896 8209 106284 106284 moderate 12.945842 8069 151530 151530 too 12.780524 8051 86611 86611 hoppy 13.600974 8012 151961 151961 touch 13.537463 8008 34195 34195 be 13.331522 7912 145204 145204 sweet, 13.192654 7895 113079 113079 off-white 13.344354 7864 116984 116984 pale 11.721063 7862 46556 46556 carbonation. 13.440301 7831 137670 137670 smooth 14.616456 7754 136840 136840 slight 13.355573 7734 36787 36787 big 14.338218 7643 163706 163706 yellow 11.484215 7507 148977 148977 there 13.509148 7379 75280 75280 fruit 13.969281 7357 155508 155508 up 13.993055 7343 85156 85156 hint 13.467099 7249 82445 82445 have 13.392852 7247 72755 72755 flavour 12.493092 7238 37654 37654 bitterness 13.744235 7112 101371 101371 malts 13.707405 6955 99421 99421 lots 14.327616 6938 81563 81563 had 13.487399 6904 59011 59011 deep 14.241690 6889 146796 146796 tap 13.618348 6878 122472 122472 poured 13.585611 6769 142930 142930 strong 14.024537 6643 26720 26720 all 13.653858 6636 86792 86792 hops, 13.856798 6627 52552 52552 coffee 14.676586 6620 76183 76183 full 14.875551 6573 148650 148650 thanks 14.364516 6510 112957 112957 off 13.485213 6492 72613 72613 flavors 14.252025 6420 73089 73089 floral 13.862421 6360 58841 58841 decent 13.161631 6255 80396 80396 great 15.095230 6206 49772 49772 chocolate, 15.006938 6198 46520 46520 carbonation 13.379681 6142 27023 27023 almost 13.432958 6123 140221 140221 spicy 13.992455 6097 39186 39186 body. 13.241162 6025 95061 95061 lacing. 13.933050 5885 115474 115474 out 13.864741 5885 111288 111288 notes. 13.252891 5880 139069 139069 sour 13.625856 5840 116823 116823 palate 13.731880 5781 149136 149136 thick 14.581053 5774 164131 164131 you 13.451472 5739 122396 122396 pour 13.908388 5687 114544 114544 or 13.060817 5607 155607 155607 updated: 14.085699 5566 138108 138108 so 13.434869 5512 29811 29811 aroma, 13.159846 5449 27175 27175 also 13.554172 5381 101387 101387 malts, 13.882528 5363 31741 31741 average 12.630856 5315 129208 129208 rich 15.086973 5197 145613 145613 sweetness 13.762656 5195 72390 72390 flavor. 13.148306 5165 162647 162647 would 13.489411 5147 35536 35536 beige 13.784184 5134 82798 82798 head, 13.087473 5133 85189 85189 hints 13.777712 5079 163330 163330 yeast 13.754000 5000 138370 138370 soft 13.829122 4986 103132 103132 me 13.453575 4965 127058 127058 red 13.999385 4878 101031 101031 malt. 13.419189 4857 107175 107175 mostly 12.826060 4789 141901 141901 still 13.993242 4735 51887 51887 cloudy 13.662056 4699 107460 107460 mouthfeel 14.115672 4677 55211 55211 copper 13.671597 4665 64989 64989 earthy 13.888191 4615 37699 37699 bitterness. 13.345585 4598 39142 39142 body, 13.120541 4513 69532 69532 fairly 13.391995 4472 160299 160299 what 13.128217 4313 78657 78657 gold 12.690288 4304 160627 160627 which 13.716495 4268 88438 88438 if 12.988051 4268 101571 101571 malty, 12.852782 4259 90414 90414 into 13.997877 4240 91235 91235 it. 13.330701 4179 159886 159886 well. 14.203650 4110 147569 147569 tastes 12.829526 4071 39064 39064 bodied 13.725253 4051 147086 147086 tart 14.334900 4043 30284 30284 aromas 13.767154 4037 99124 99124 long 14.158210 4001 150850 150850 toasted 13.797226 3965 95981 95981 lasting 13.619108 3946 53206 53206 color. 13.207562 3941 74725 74725 fresh 14.431298 3930 80176 80176 grassy 12.579619 3837 160367 160367 wheat 12.803390 3835 83719 83719 heavy 14.096791 3833 32776 32776 balanced 14.709516 3804 138768 138768 somewhat 13.084395 3768 21967 21967 : 12.685775 3768 72321 72321 flavor, 13.207923 3761 56872 56872 crisp 13.542400 3750 36423 36423 better 13.078177 3620 45340 45340 can 12.682158 3615 In\u00a0[58]: Copied! <pre>df_word_frequencies = (\n    (df.text)\n    .str.lower()\n    .str.replace(r\"[^a-z\\ ]\", \"\")\n    .str.replace(r\"\\ +\", \" \")\n    .str.split(\" \")  # LINE TO BE REMOVED FOR STUDENTS\n    .explode()\n    .loc[lambda x: x != \"\"]\n    .value_counts(normalize=True)\n    .rename(\"word_frequency\")\n    .rename_axis(index=\"word\")\n    .reset_index()\n    .assign(rank=lambda df: range(1, 1 + df.shape[0]))\n)\ndf_word_frequencies\n</pre> df_word_frequencies = (     (df.text)     .str.lower()     .str.replace(r\"[^a-z\\ ]\", \"\")     .str.replace(r\"\\ +\", \" \")     .str.split(\" \")  # LINE TO BE REMOVED FOR STUDENTS     .explode()     .loc[lambda x: x != \"\"]     .value_counts(normalize=True)     .rename(\"word_frequency\")     .rename_axis(index=\"word\")     .reset_index()     .assign(rank=lambda df: range(1, 1 + df.shape[0])) ) df_word_frequencies Out[58]: word word_frequency rank 0 a 4.38e-02 1 1 and 3.63e-02 2 2 the 3.13e-02 3 3 with 2.67e-02 4 4 of 2.35e-02 5 ... ... ... ... 269396 vapeur-like 9.51e-08 269397 269397 tongue-cleaning. 9.51e-08 269398 269398 sidra 9.51e-08 269399 269399 virtuous 9.51e-08 269400 269400 fruit-oriented 9.51e-08 269401 <p>269401 rows \u00d7 3 columns</p> In\u00a0[59]: Copied! <pre>df_word_frequencies.head(10000).plot(x=\"rank\", y=\"word_frequency\", marker=\".\")\n</pre> df_word_frequencies.head(10000).plot(x=\"rank\", y=\"word_frequency\", marker=\".\") Out[59]: <pre>&lt;Axes: xlabel='rank'&gt;</pre> In\u00a0[60]: Copied! <pre>df_word_frequencies.head(10000).plot(\n    x=\"rank\", y=\"word_frequency\", loglog=True, marker=\".\"\n)\n</pre> df_word_frequencies.head(10000).plot(     x=\"rank\", y=\"word_frequency\", loglog=True, marker=\".\" ) Out[60]: <pre>&lt;Axes: xlabel='rank'&gt;</pre> In\u00a0[61]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.alcohol &lt; 20],\n    x=\"alcohol\",\n    hue=\"rating\",\n    multiple=\"fill\",\n    palette=\"RdYlGn\",\n    bins=20,\n)\n</pre> sns.displot(     df.loc[lambda df: df.alcohol &lt; 20],     x=\"alcohol\",     hue=\"rating\",     multiple=\"fill\",     palette=\"RdYlGn\",     bins=20, ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[61]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fac079a4710&gt;</pre> In\u00a0[62]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.alcohol &lt; 20],\n    x=\"alcohol\",\n    y=\"rating\",\n    bins=20,\n)\n</pre> sns.displot(     df.loc[lambda df: df.alcohol &lt; 20],     x=\"alcohol\",     y=\"rating\",     bins=20, ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[62]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fabeba2c290&gt;</pre> In\u00a0[63]: Copied! <pre>sns.displot(\n    df,\n    x=\"rating_appearance\",\n    discrete=True,\n    hue=\"rating\",\n    multiple=\"stack\",\n    palette=\"RdYlGn\",\n)\n</pre> sns.displot(     df,     x=\"rating_appearance\",     discrete=True,     hue=\"rating\",     multiple=\"stack\",     palette=\"RdYlGn\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[63]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fabddd06990&gt;</pre> In\u00a0[64]: Copied! <pre>sns.displot(\n    df,\n    x=\"rating_appearance\",\n    discrete=True,\n    hue=\"rating\",\n    multiple=\"fill\",\n    palette=\"RdYlGn\",\n)\n</pre> sns.displot(     df,     x=\"rating_appearance\",     discrete=True,     hue=\"rating\",     multiple=\"fill\",     palette=\"RdYlGn\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[64]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7fabddd0bed0&gt;</pre> In\u00a0[65]: Copied! <pre>sns.violinplot(\n    df,\n    x=\"rating_appearance\",\n    y=\"rating\",\n)\n</pre> sns.violinplot(     df,     x=\"rating_appearance\",     y=\"rating\", ) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n</pre> Out[65]: <pre>&lt;Axes: xlabel='rating_appearance', ylabel='rating'&gt;</pre>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#practice-n2-exploratory-data-analysis","title":"\ud83d\udc0d Practice n\u00b02: exploratory data analysis\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#preparation","title":"Preparation\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#install-import-modules","title":"Install &amp; import modules\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#read-remote-dataset","title":"Read remote dataset\u00b6","text":"<p>The data is in this git repository: ML-boot-camp/ratebeer.git.</p> <p>The data is located in the <code>ratebeer/data/</code> folder.</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#general-information","title":"General information\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#shape","title":"Shape\u00b6","text":"<p>Have a first overview of the dataframe size, i.e. number of rows &amp; columns.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.shape</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#overview","title":"Overview\u00b6","text":"<p>Get a few information about the content of the dataframe:</p> <ul> <li>number of null values per column</li> <li>data type of each column</li> <li>memory usage</li> </ul> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.info</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#sample","title":"Sample\u00b6","text":"<p>Show a sample of the data</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#describing-statistics","title":"Describing statistics\u00b6","text":"<p>Compute statistics to understand the content of each column.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.describe</code></li> </ul> <p>Bonus: fill NaN values with an empty string <code>\"\"</code> for a better readability using:</p> <ul> <li><code>pd.DataFrame.fillna</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#target","title":"Target\u00b6","text":"<p>The dataset contains 2 possible targets:</p> <ul> <li><code>rating</code>: an ordinal variable, which can be used to define a regression</li> <li><code>is_good</code>: a binary variable, which can be used to define a classification</li> </ul> <p>In the follow-up of the exploratory data analysis, for the sake of clariy, we'll consider only the binary target (classification). Some plots would be different for numeric target.</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#is_good","title":"<code>is_good</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#quantitative-variables","title":"Quantitative variables\u00b6","text":"<ul> <li><code>alcohol</code></li> <li><code>date</code></li> <li><code>rating_appearance</code></li> <li><code>rating_aroma</code></li> <li><code>rating_palate</code></li> <li><code>rating_taste</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#alcohol","title":"<code>alcohol</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#date","title":"<code>date</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#relationship-with-the-target","title":"Relationship with the target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#all-rating-columns-review_-average_rating","title":"All rating columns: review_* &amp; average_rating\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#relationship-with-the-target","title":"Relationship with the target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#categorical-variables","title":"Categorical variables\u00b6","text":"<ul> <li><code>type</code></li> <li><code>beer</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#type","title":"<code>type</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#relationship-with-the-target","title":"Relationship with the target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#high-cardinality-variables","title":"High cardinality variables\u00b6","text":"<ul> <li><code>beer</code></li> <li><code>brewery</code></li> <li><code>user</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#beer-degree","title":"<code>beer</code> degree\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#relationship-with-target","title":"Relationship with target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#brewery","title":"<code>brewery</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#relationship-with-target","title":"Relationship with target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#user","title":"<code>user</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#relationship-with-target","title":"Relationship with target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#text-variable","title":"Text variable\u00b6","text":"<p>Using the <code>pd.Series.str</code> API</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#text-length","title":"Text length\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#distribution","title":"Distribution\u00b6","text":"<p>Is it a Power law distribution ?</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#relationship-with-the-target","title":"Relationship with the target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#words-associated-to-positive-negative-reviews-optional","title":"Words associated to positive &amp; negative reviews (optional)\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#word-frequencies-optional","title":"Word frequencies (optional)\u00b6","text":"<p>Compute the frequency of the most used words in the texts</p> <p>Methods you'll need:</p> <ul> <li><code>pd.Series.str.len</code></li> <li><code>pd.Series.str.split</code></li> <li><code>pd.Series.explode</code></li> <li><code>pd.Series.value_counts</code></li> <li><code>pd.Series.head</code></li> </ul> <p>Bonus: plot an histogram of the values, with log values, using:</p> <ul> <li><code>pd.Series.plot.hist</code></li> </ul> <p>Is it a Power law distribution ?</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#and-if-its-a-regression","title":"And if it's a regression ?\u00b6","text":"<p>We show those plot examples to explain what would be an exploratory data analysis for a regression problem.</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis/#continuous-variable","title":"Continuous variable\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis/#categorical-variable","title":"Categorical variable\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/","title":"\ud83d\udc0d Practice n\u00b02: exploratory data analysis (students version)","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install seaborn\n</pre> ! pip install seaborn In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd import seaborn as sns import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>pd.set_option(\"display.precision\", 2)\nsns.set_style(\"whitegrid\")\nsns.set_context(rc={\"patch.linewidth\": 0.15})\n</pre> pd.set_option(\"display.precision\", 2) sns.set_style(\"whitegrid\") sns.set_context(rc={\"patch.linewidth\": 0.15}) In\u00a0[\u00a0]: Copied! <pre>file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\"\n</pre> file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\" In\u00a0[\u00a0]: Copied! <pre>df = pd.read_parquet(file_url)\n</pre> df = pd.read_parquet(file_url) In\u00a0[\u00a0]: Copied! <pre>df.shape\n</pre> df.shape In\u00a0[\u00a0]: Copied! <pre>*** FILL THE MISSING LINE ***\n</pre> *** FILL THE MISSING LINE *** In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df In\u00a0[\u00a0]: Copied! <pre>df.describe(include=\"all\").fillna(\"\").T\n</pre> df.describe(include=\"all\").fillna(\"\").T In\u00a0[\u00a0]: Copied! <pre>(\n    (df.is_good)\n    .value_counts()\n    *** FILL THE MISSING LINE ***\n)\n</pre> (     (df.is_good)     .value_counts()     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>sns.countplot(\n    df,\n    *** FILL THE MISSING LINE ***\n)\n</pre> sns.countplot(     df,     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>*** FILL THE MISSING LINE ***\n</pre> *** FILL THE MISSING LINE *** In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    x=\"alcohol\",\n)\n</pre> sns.displot(     df,     x=\"alcohol\", ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(df, x=\"alcohol\", hue=\"is_good\")\n</pre> sns.displot(df, x=\"alcohol\", hue=\"is_good\") In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.alcohol &lt; 20], x=\"alcohol\", hue=\"is_good\"\n)\n</pre> sns.displot(     df.loc[lambda df: df.alcohol &lt; 20], x=\"alcohol\", hue=\"is_good\" ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.alcohol &lt; 20],\n    x=\"alcohol\",\n    hue=\"is_good\",\n    kde=True,\n)\n</pre> sns.displot(     df.loc[lambda df: df.alcohol &lt; 20],     x=\"alcohol\",     hue=\"is_good\",     kde=True, ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.alcohol &lt; 20],\n    x=\"alcohol\",\n    hue=\"is_good\",\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df.loc[lambda df: df.alcohol &lt; 20],     x=\"alcohol\",     hue=\"is_good\",     multiple=\"fill\", ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    *** FILL THE MISSING LINE ***\n)\n</pre> sns.displot(     df,     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n    kde=True,\n)\n</pre> sns.displot(     df,     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE ***     kde=True, ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df,     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE ***     multiple=\"fill\", ) In\u00a0[\u00a0]: Copied! <pre>review_columns = [\n    \"rating_appearance\",\n    \"rating_aroma\",\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n]\ndf_rating_long = df.melt(id_vars=\"is_good\", value_vars=review_columns)\ndf_rating_long\n</pre> review_columns = [     \"rating_appearance\",     \"rating_aroma\",     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** ] df_rating_long = df.melt(id_vars=\"is_good\", value_vars=review_columns) df_rating_long In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df_rating_long,\n    x=\"value\",\n    row=\"variable\",\n    discrete=True,\n    height=3,\n    aspect=2,\n)\n</pre> sns.displot(     df_rating_long,     x=\"value\",     row=\"variable\",     discrete=True,     height=3,     aspect=2, ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df_rating_long,\n    x=\"value\",\n    hue=\"is_good\",\n    row=\"variable\",\n    discrete=True,\n    height=3,\n    aspect=2,\n)\n</pre> sns.displot(     df_rating_long,     x=\"value\",     hue=\"is_good\",     row=\"variable\",     discrete=True,     height=3,     aspect=2, ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df_rating_long,\n    x=\"value\",\n    hue=\"is_good\",\n    row=\"variable\",\n    discrete=True,\n    height=3,\n    aspect=2,\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df_rating_long,     x=\"value\",     hue=\"is_good\",     row=\"variable\",     discrete=True,     height=3,     aspect=2,     multiple=\"fill\", ) In\u00a0[\u00a0]: Copied! <pre>(\n    (df.type)\n    .value_counts()\n    .plot.bar()\n)\n</pre> (     (df.type)     .value_counts()     .plot.bar() ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    x=\"type\",\n    discrete=True,\n    hue=\"is_good\",\n)\n</pre> sns.displot(     df,     x=\"type\",     discrete=True,     hue=\"is_good\", ) In\u00a0[\u00a0]: Copied! <pre>df_styles = (\n    (df)\n    .groupby(\"type\")\n    .is_good.agg([\"count\", \"mean\"])\n    .add_prefix(\"review_\")\n    .reset_index()\n    .sort_values(by=\"review_mean\", ascending=False)\n    .reset_index(drop=True)\n    .assign(\n        bar_left_position=lambda df: df.review_count.cumsum().shift(1, fill_value=0)\n    )\n)\ndf_styles\n</pre> df_styles = (     (df)     .groupby(\"type\")     .is_good.agg([\"count\", \"mean\"])     .add_prefix(\"review_\")     .reset_index()     .sort_values(by=\"review_mean\", ascending=False)     .reset_index(drop=True)     .assign(         bar_left_position=lambda df: df.review_count.cumsum().shift(1, fill_value=0)     ) ) df_styles In\u00a0[\u00a0]: Copied! <pre>plt.bar(\n    x=df_styles.bar_left_position,\n    height=df_styles.review_mean,\n    width=df_styles.review_count,\n    align=\"edge\",\n    alpha=0.5,\n    edgecolor=\"k\",\n    linewidth=0.5,\n)\n</pre> plt.bar(     x=df_styles.bar_left_position,     height=df_styles.review_mean,     width=df_styles.review_count,     align=\"edge\",     alpha=0.5,     edgecolor=\"k\",     linewidth=0.5, ) <p>All those high cardinality variables can be thought as links of a network. Indeed, a review is an object comprising a beer, a brewery and a user and can be thought as a network link between them.</p> <p>In other words, the review table is the a table describing the links in a network with 3 types of nodes: users, beers and breweries.</p> <p>The first property to compute about each node is its \"degree\", which is its number of connections with other nodes. High degree means \"highly connected\".</p> <p>Analyse the degree of the nodes is a way to answer the following questions:</p> <ul> <li>is an experienced user more severe ?</li> <li>is a new user more forgiving ?</li> <li>is a popular beer (or a big brewery) disadvantaged by a \"boreness factor\" ?</li> <li>is a new beer (a small brewery) benefitting from a \"novelty factor\" ?</li> </ul> <p>To compute the degree you'll need:</p> <ul> <li><code>pd.Series.value_counts</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre>df_beer_degree = df.loc[:, [\"beer\", \"beer_degree\"]].drop_duplicates()\ndf_beer_degree\n</pre> df_beer_degree = df.loc[:, [\"beer\", \"beer_degree\"]].drop_duplicates() df_beer_degree In\u00a0[\u00a0]: Copied! <pre>(\n    (df_beer_degree.beer_degree)\n    .value_counts()\n    .reset_index()\n    .plot.scatter(\n        *** FILL THE MISSING LINE ***\n    )\n)\n</pre> (     (df_beer_degree.beer_degree)     .value_counts()     .reset_index()     .plot.scatter(         *** FILL THE MISSING LINE ***     ) ) <p>Many networks are scale free networks: </p> <p>Meaning that the node's degree distribution follows a power law distribution, which is better visualized using a log-log scale: </p> In\u00a0[\u00a0]: Copied! <pre>(\n    (df_beer_degree.beer_degree)\n    .value_counts()\n    .reset_index()\n    *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n)\n</pre> (     (df_beer_degree.beer_degree)     .value_counts()     .reset_index()     *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** ) <p>Another way to visualize the power law distribution is to visualize the rank-frequency plot.</p> <p>Instead of plotting the frequency as a function of the value (like in a normal distribution plot), we plot the frequency as a function of its rank. Note: It's always monotonically decreasing.</p> <p>Power law distributions exhibit also a distinctive visual pattern in the rank-frequency plot, known as the Zipf's law: </p> In\u00a0[\u00a0]: Copied! <pre>def plot_rank_size(series):\n    return (\n        (series)\n        .rename(\"\")\n        .value_counts()\n        .reset_index()\n        .assign(rank=lambda df: range(1, 1 + df.shape[0]))\n        .plot(x=\"rank\", y=\"count\", loglog=True, marker=\".\")\n    )\n</pre> def plot_rank_size(series):     return (         (series)         .rename(\"\")         .value_counts()         .reset_index()         .assign(rank=lambda df: range(1, 1 + df.shape[0]))         .plot(x=\"rank\", y=\"count\", loglog=True, marker=\".\")     ) In\u00a0[\u00a0]: Copied! <pre>plot_rank_size(df_beer_degree.beer_degree)\n</pre> plot_rank_size(df_beer_degree.beer_degree) In\u00a0[\u00a0]: Copied! <pre>sns.displot(df, x=\"beer_degree\", hue=\"is_good\")\n</pre> sns.displot(df, x=\"beer_degree\", hue=\"is_good\") In\u00a0[\u00a0]: Copied! <pre>sns.displot(df, x=\"beer_degree\", hue=\"is_good\", log_scale=True)\n</pre> sns.displot(df, x=\"beer_degree\", hue=\"is_good\", log_scale=True) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    x=\"beer_degree\",\n    hue=\"is_good\",\n    log_scale=True,\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df,     x=\"beer_degree\",     hue=\"is_good\",     log_scale=True,     multiple=\"fill\", ) In\u00a0[\u00a0]: Copied! <pre>df_brewery_degree = df.loc[:, [\"brewery\", \"brewery_degree\"]].drop_duplicates()\ndf_brewery_degree\n</pre> df_brewery_degree = df.loc[:, [\"brewery\", \"brewery_degree\"]].drop_duplicates() df_brewery_degree In\u00a0[\u00a0]: Copied! <pre>(\n    (df_brewery_degree.brewery_degree)\n    .value_counts()\n    .reset_index()\n    *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n)\n</pre> (     (df_brewery_degree.brewery_degree)     .value_counts()     .reset_index()     *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>(\n    (df_brewery_degree.brewery_degree)\n    .value_counts()\n    .reset_index()\n    *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n)\n</pre> (     (df_brewery_degree.brewery_degree)     .value_counts()     .reset_index()     *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>plot_rank_size(df_brewery_degree.brewery_degree)\n</pre> plot_rank_size(df_brewery_degree.brewery_degree) In\u00a0[\u00a0]: Copied! <pre>sns.displot(df, x=\"brewery_degree\", hue=\"is_good\")\n</pre> sns.displot(df, x=\"brewery_degree\", hue=\"is_good\") In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df, x=\"brewery_degree\", hue=\"is_good\", log_scale=True\n)\n</pre> sns.displot(     df, x=\"brewery_degree\", hue=\"is_good\", log_scale=True ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    x=\"brewery_degree\",\n    hue=\"is_good\",\n    log_scale=True,\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df,     x=\"brewery_degree\",     hue=\"is_good\",     log_scale=True,     multiple=\"fill\", ) In\u00a0[\u00a0]: Copied! <pre>df_user_degree = df.loc[:, [\"user\", \"user_degree\"]].drop_duplicates()\ndf_user_degree\n</pre> df_user_degree = df.loc[:, [\"user\", \"user_degree\"]].drop_duplicates() df_user_degree In\u00a0[\u00a0]: Copied! <pre>(\n    (df_user_degree.user_degree)\n    .value_counts()\n    .reset_index()\n    *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n)\n</pre> (     (df_user_degree.user_degree)     .value_counts()     .reset_index()     *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>(\n    (df_user_degree.user_degree)\n    .value_counts()\n    .reset_index()\n    *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n)\n</pre> (     (df_user_degree.user_degree)     .value_counts()     .reset_index()     *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>plot_rank_size(df_user_degree.user_degree)\n</pre> plot_rank_size(df_user_degree.user_degree) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df, x=\"user_degree\", hue=\"is_good\"\n)\n</pre> sns.displot(     df, x=\"user_degree\", hue=\"is_good\" ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    x=\"user_degree\",\n    hue=\"is_good\",\n    log_scale=True,\n)\n</pre> sns.displot(     df,     x=\"user_degree\",     hue=\"is_good\",     log_scale=True, ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    x=\"user_degree\",\n    hue=\"is_good\",\n    log_scale=True,\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df,     x=\"user_degree\",     hue=\"is_good\",     log_scale=True,     multiple=\"fill\", ) In\u00a0[\u00a0]: Copied! <pre>*** FILL THE MISSING LINE ***\n</pre> *** FILL THE MISSING LINE *** In\u00a0[\u00a0]: Copied! <pre>((df.text_length).plot.hist(bins=200, logy=True))\n</pre> ((df.text_length).plot.hist(bins=200, logy=True)) In\u00a0[\u00a0]: Copied! <pre>sns.displot(df, x=\"text_length\")\n</pre> sns.displot(df, x=\"text_length\") In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    x=\"text_length\",\n    hue=\"is_good\",\n)\n</pre> sns.displot(     df,     x=\"text_length\",     hue=\"is_good\", ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.text_length &lt; 1500],\n    x=\"text_length\",\n    hue=\"is_good\",\n)\n</pre> sns.displot(     df.loc[lambda df: df.text_length &lt; 1500],     x=\"text_length\",     hue=\"is_good\", ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.text_length &lt; 1500],\n    x=\"text_length\",\n    hue=\"is_good\",\n    multiple=\"fill\",\n)\n</pre> sns.displot(     df.loc[lambda df: df.text_length &lt; 1500],     x=\"text_length\",     hue=\"is_good\",     multiple=\"fill\", ) In\u00a0[\u00a0]: Copied! <pre>(\n    (df)\n    .head(100000)\n    .assign(\n        tokenized_text=lambda df: (df.text)\n        .str.lower()\n        .str.replace(r\"[^a-z]\", \" \")\n        .str.replace(r\" +\", \" \")\n        .str.split(\" \")\n    )\n    .loc[:, [\"rating\", \"tokenized_text\"]]\n    .explode(\"tokenized_text\")\n    .loc[lambda df: df.tokenized_text != \"\"]\n    .groupby(\"tokenized_text\", as_index=False)\n    .agg([\"mean\", \"count\"])\n    .reset_index()\n    .sort_values(by=(\"rating\", \"count\"), ascending=False)\n    .head(200)\n    .style.background_gradient(cmap=\"RdYlGn\")\n)\n</pre> (     (df)     .head(100000)     .assign(         tokenized_text=lambda df: (df.text)         .str.lower()         .str.replace(r\"[^a-z]\", \" \")         .str.replace(r\" +\", \" \")         .str.split(\" \")     )     .loc[:, [\"rating\", \"tokenized_text\"]]     .explode(\"tokenized_text\")     .loc[lambda df: df.tokenized_text != \"\"]     .groupby(\"tokenized_text\", as_index=False)     .agg([\"mean\", \"count\"])     .reset_index()     .sort_values(by=(\"rating\", \"count\"), ascending=False)     .head(200)     .style.background_gradient(cmap=\"RdYlGn\") ) In\u00a0[\u00a0]: Copied! <pre>df_word_frequencies = (\n    (df.text)\n    .str.lower()\n    .str.replace(r\"[^a-z\\ ]\", \"\")\n    .str.replace(r\"\\ +\", \" \")\n    *** FILL THE MISSING LINE ***\n    .explode()\n    .loc[lambda x: x != \"\"]\n    .value_counts(normalize=True)\n    .rename(\"word_frequency\")\n    .rename_axis(index=\"word\")\n    .reset_index()\n    .assign(rank=lambda df: range(1, 1 + df.shape[0]))\n)\ndf_word_frequencies\n</pre> df_word_frequencies = (     (df.text)     .str.lower()     .str.replace(r\"[^a-z\\ ]\", \"\")     .str.replace(r\"\\ +\", \" \")     *** FILL THE MISSING LINE ***     .explode()     .loc[lambda x: x != \"\"]     .value_counts(normalize=True)     .rename(\"word_frequency\")     .rename_axis(index=\"word\")     .reset_index()     .assign(rank=lambda df: range(1, 1 + df.shape[0])) ) df_word_frequencies In\u00a0[\u00a0]: Copied! <pre>df_word_frequencies.head(10000).plot(x=\"rank\", y=\"word_frequency\", marker=\".\")\n</pre> df_word_frequencies.head(10000).plot(x=\"rank\", y=\"word_frequency\", marker=\".\") In\u00a0[\u00a0]: Copied! <pre>df_word_frequencies.head(10000).plot(\n    x=\"rank\", y=\"word_frequency\", loglog=True, marker=\".\"\n)\n</pre> df_word_frequencies.head(10000).plot(     x=\"rank\", y=\"word_frequency\", loglog=True, marker=\".\" ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.alcohol &lt; 20],\n    x=\"alcohol\",\n    hue=\"rating\",\n    multiple=\"fill\",\n    palette=\"RdYlGn\",\n    bins=20,\n)\n</pre> sns.displot(     df.loc[lambda df: df.alcohol &lt; 20],     x=\"alcohol\",     hue=\"rating\",     multiple=\"fill\",     palette=\"RdYlGn\",     bins=20, ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df.loc[lambda df: df.alcohol &lt; 20],\n    x=\"alcohol\",\n    y=\"rating\",\n    bins=20,\n)\n</pre> sns.displot(     df.loc[lambda df: df.alcohol &lt; 20],     x=\"alcohol\",     y=\"rating\",     bins=20, ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    x=\"rating_appearance\",\n    discrete=True,\n    hue=\"rating\",\n    multiple=\"stack\",\n    palette=\"RdYlGn\",\n)\n</pre> sns.displot(     df,     x=\"rating_appearance\",     discrete=True,     hue=\"rating\",     multiple=\"stack\",     palette=\"RdYlGn\", ) In\u00a0[\u00a0]: Copied! <pre>sns.displot(\n    df,\n    x=\"rating_appearance\",\n    discrete=True,\n    hue=\"rating\",\n    multiple=\"fill\",\n    palette=\"RdYlGn\",\n)\n</pre> sns.displot(     df,     x=\"rating_appearance\",     discrete=True,     hue=\"rating\",     multiple=\"fill\",     palette=\"RdYlGn\", ) In\u00a0[\u00a0]: Copied! <pre>sns.violinplot(\n    df,\n    x=\"rating_appearance\",\n    y=\"rating\",\n)\n</pre> sns.violinplot(     df,     x=\"rating_appearance\",     y=\"rating\", )"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#practice-n2-exploratory-data-analysis-students-version","title":"\ud83d\udc0d Practice n\u00b02: exploratory data analysis (students version)\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#preparation","title":"Preparation\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#install-import-modules","title":"Install &amp; import modules\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#read-remote-dataset","title":"Read remote dataset\u00b6","text":"<p>The data is in this git repository: ML-boot-camp/ratebeer.git.</p> <p>The data is located in the <code>ratebeer/data/</code> folder.</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#general-information","title":"General information\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#shape","title":"Shape\u00b6","text":"<p>Have a first overview of the dataframe size, i.e. number of rows &amp; columns.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.shape</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#overview","title":"Overview\u00b6","text":"<p>Get a few information about the content of the dataframe:</p> <ul> <li>number of null values per column</li> <li>data type of each column</li> <li>memory usage</li> </ul> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.info</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#sample","title":"Sample\u00b6","text":"<p>Show a sample of the data</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#describing-statistics","title":"Describing statistics\u00b6","text":"<p>Compute statistics to understand the content of each column.</p> <p>Methods you'll need:</p> <ul> <li><code>pd.DataFrame.describe</code></li> </ul> <p>Bonus: fill NaN values with an empty string <code>\"\"</code> for a better readability using:</p> <ul> <li><code>pd.DataFrame.fillna</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#target","title":"Target\u00b6","text":"<p>The dataset contains 2 possible targets:</p> <ul> <li><code>rating</code>: an ordinal variable, which can be used to define a regression</li> <li><code>is_good</code>: a binary variable, which can be used to define a classification</li> </ul> <p>In the follow-up of the exploratory data analysis, for the sake of clariy, we'll consider only the binary target (classification). Some plots would be different for numeric target.</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#is_good","title":"<code>is_good</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#quantitative-variables","title":"Quantitative variables\u00b6","text":"<ul> <li><code>alcohol</code></li> <li><code>date</code></li> <li><code>rating_appearance</code></li> <li><code>rating_aroma</code></li> <li><code>rating_palate</code></li> <li><code>rating_taste</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#alcohol","title":"<code>alcohol</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#date","title":"<code>date</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#relationship-with-the-target","title":"Relationship with the target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#all-rating-columns-review_-average_rating","title":"All rating columns: review_* &amp; average_rating\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#relationship-with-the-target","title":"Relationship with the target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#categorical-variables","title":"Categorical variables\u00b6","text":"<ul> <li><code>type</code></li> <li><code>beer</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#type","title":"<code>type</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#relationship-with-the-target","title":"Relationship with the target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#high-cardinality-variables","title":"High cardinality variables\u00b6","text":"<ul> <li><code>beer</code></li> <li><code>brewery</code></li> <li><code>user</code></li> </ul>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#beer-degree","title":"<code>beer</code> degree\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#relationship-with-target","title":"Relationship with target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#brewery","title":"<code>brewery</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#relationship-with-target","title":"Relationship with target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#user","title":"<code>user</code>\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#distribution","title":"Distribution\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#relationship-with-target","title":"Relationship with target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#text-variable","title":"Text variable\u00b6","text":"<p>Using the <code>pd.Series.str</code> API</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#text-length","title":"Text length\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#distribution","title":"Distribution\u00b6","text":"<p>Is it a Power law distribution ?</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#relationship-with-the-target","title":"Relationship with the target\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#words-associated-to-positive-negative-reviews-optional","title":"Words associated to positive &amp; negative reviews (optional)\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#word-frequencies-optional","title":"Word frequencies (optional)\u00b6","text":"<p>Compute the frequency of the most used words in the texts</p> <p>Methods you'll need:</p> <ul> <li><code>pd.Series.str.len</code></li> <li><code>pd.Series.str.split</code></li> <li><code>pd.Series.explode</code></li> <li><code>pd.Series.value_counts</code></li> <li><code>pd.Series.head</code></li> </ul> <p>Bonus: plot an histogram of the values, with log values, using:</p> <ul> <li><code>pd.Series.plot.hist</code></li> </ul> <p>Is it a Power law distribution ?</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#and-if-its-a-regression","title":"And if it's a regression ?\u00b6","text":"<p>We show those plot examples to explain what would be an exploratory data analysis for a regression problem.</p>"},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#continuous-variable","title":"Continuous variable\u00b6","text":""},{"location":"10-Tutorials/2_exploratory_data_analysis_students/#categorical-variable","title":"Categorical variable\u00b6","text":""},{"location":"10-Tutorials/3_regression/","title":"\ud83d\udc0d Practice n\u00b03: regression","text":"<p>Regression in machine learning consists of mathematical methods that allow to predict a continuous outcome (y) based on the value of one or more predictor variables (x)</p> <p></p> <p>To predict the air traffic controller workload, we need to find the best function y=f(x). For that, we will define an error and find the function which minimizes this error.</p> <p></p> <p>In reality, we often deal with multiple explicative variables and the dimensionality of the input data is greater than 1.</p> <p>$$x = (x_1,...,x_n)$$ $$\\hat{y} = f(x_1,...,x_n)$$</p> <p>If we come back to our traffic control example, the air traffic controller workload can be a function of both the number of aircraft and the weather.</p> <p>The linear regression is a regression using a linear function \u0177 = ax + b</p> <p></p> <p>In reality, we often deal with multiple explicative variables and the dimensionality of the input data is greater than 1. The linear function will look like :</p> <p>$$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$$</p> <p>Now let's practice with the ratebeer dataset !</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom wordcloud import STOPWORDS\nimport numpy as np\nfrom sklearn import (\n    linear_model,\n    preprocessing,\n    pipeline,\n    model_selection,\n    metrics,\n    compose,\n)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport matplotlib as mpl\n\nmpl.rcParams['agg.path.chunksize'] = 10000\npd.set_option(\"display.precision\", 2)\nsns.set_style(\"whitegrid\")\nsns.set_context(rc={\"patch.linewidth\": 0.15})\n</pre> import pandas as pd from wordcloud import STOPWORDS import numpy as np from sklearn import (     linear_model,     preprocessing,     pipeline,     model_selection,     metrics,     compose, ) import seaborn as sns import matplotlib.pyplot as plt from tqdm import tqdm import matplotlib as mpl  mpl.rcParams['agg.path.chunksize'] = 10000 pd.set_option(\"display.precision\", 2) sns.set_style(\"whitegrid\") sns.set_context(rc={\"patch.linewidth\": 0.15}) <p>The data is in this git repository: ML-boot-camp/ratebeer.git.</p> <p>The data is located in the the <code>ratebeer/data/</code> folder.</p> In\u00a0[2]: Copied! <pre>file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\"\n</pre> file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\" In\u00a0[3]: Copied! <pre>df_master = pd.read_parquet(file_url)\n</pre> df_master = pd.read_parquet(file_url) In\u00a0[4]: Copied! <pre>df_master.head()\n</pre> df_master.head() Out[4]: beer brewery alcohol type rating_appearance rating_aroma rating_palate rating_taste rating timestamp user text beer_degree brewery_degree user_degree text_length date is_good 0 Breckenridge Oatmeal Stout 383 4.95 Stout 4 7 4 7 14 1217462400 blutt59 bottle, oat nose with black color, bitter choc... 51 413 300 90 2008-07-31 02:00:00 0 1 Breckenridge 471 Small Batch Imperial Porter 383 7.50 Imperial/Strong Porter 3 8 3 8 14 1312588800 blutt59 bottle, received in trade, dark brown with gar... 4 413 300 191 2011-08-06 02:00:00 0 2 Breckenridge Avalanche Amber 383 5.41 Amber Ale 3 5 3 5 10 1205020800 blutt59 12 oz. bottle, amber color with soapy head, sl... 43 413 300 109 2008-03-09 01:00:00 0 3 Breckenridge Lucky U IPA 383 6.20 India Pale Ale (IPA) 3 6 3 7 12 1255737600 blutt59 bottle, golden orange color with light tan foa... 20 413 300 151 2009-10-17 02:00:00 0 4 Fullers Vintage Ale 2009 55 8.50 English Strong Ale 3 7 3 8 14 1282003200 blutt59 bottle, thanks to SS, almond amber colored pou... 18 978 300 159 2010-08-17 02:00:00 0 In\u00a0[5]: Copied! <pre>df_master.shape\n</pre> df_master.shape Out[5]: <pre>(200000, 18)</pre> <p>The goal of supervised learning is to build a model that performs well on new data. If you have new data, it\u2019s a good idea to see how your model performs on it. The problem is that you may not have new data, but you can simulate this experience by splitting your dataset between train and test.</p> <p></p> In\u00a0[6]: Copied! <pre>(\n    df_features_and_target_train,\n    df_features_and_target_test,\n) = model_selection.train_test_split(\n    df_master, \n    test_size=0.1 # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> (     df_features_and_target_train,     df_features_and_target_test, ) = model_selection.train_test_split(     df_master,      test_size=0.1 # LINE TO BE REMOVED FOR STUDENTS ) In\u00a0[7]: Copied! <pre>def split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n    N=None,\n):\n    if N is None :\n        X_train = df_features_and_target_train[features]\n        y_train = df_features_and_target_train[target]\n    else :\n        X_train = df_features_and_target_train[features].sample(N)\n        y_train = df_features_and_target_train[target].loc[X_train.index]\n    X_test = df_features_and_target_test[features]\n    y_test = df_features_and_target_test[target]\n    return X_train, y_train, X_test, y_test\n\n\ntarget = [\n    \"rating\" # LINE TO BE REMOVED FOR STUDENTS\n] \nfeatures = [\n    \"alcohol\", # LINE TO BE REMOVED FOR STUDENTS\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train, # LINE TO BE REMOVED FOR STUDENTS\n    df_features_and_target_test, # LINE TO BE REMOVED FOR STUDENTS\n    features, # LINE TO BE REMOVED FOR STUDENTS\n    target, # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> def split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target,     N=None, ):     if N is None :         X_train = df_features_and_target_train[features]         y_train = df_features_and_target_train[target]     else :         X_train = df_features_and_target_train[features].sample(N)         y_train = df_features_and_target_train[target].loc[X_train.index]     X_test = df_features_and_target_test[features]     y_test = df_features_and_target_test[target]     return X_train, y_train, X_test, y_test   target = [     \"rating\" # LINE TO BE REMOVED FOR STUDENTS ]  features = [     \"alcohol\", # LINE TO BE REMOVED FOR STUDENTS     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train, # LINE TO BE REMOVED FOR STUDENTS     df_features_and_target_test, # LINE TO BE REMOVED FOR STUDENTS     features, # LINE TO BE REMOVED FOR STUDENTS     target, # LINE TO BE REMOVED FOR STUDENTS ) In\u00a0[8]: Copied! <pre>pipe = pipeline.make_pipeline(\n    linear_model.LinearRegression(),\n)\n\npipe.fit(\n    X_train, # LINE TO BE REMOVED FOR STUDENTS\n    y_train # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> pipe = pipeline.make_pipeline(     linear_model.LinearRegression(), )  pipe.fit(     X_train, # LINE TO BE REMOVED FOR STUDENTS     y_train # LINE TO BE REMOVED FOR STUDENTS ) Out[8]: <pre>Pipeline(steps=[('linearregression', LinearRegression())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('linearregression', LinearRegression())])</pre>LinearRegression<pre>LinearRegression()</pre> <p>To assess the performance of our model, many metrics can be used. The easiest one for regression is $R^2$. It is used to measure the goodness of fit or best-fit line.</p> <p>$$R^2 = 1 - \\frac{\\sum_{i}{(y_i - \\hat{y}_i)^2}}{\\sum_{i}{(y_i - \\bar{y}_i)^2}}$$</p> In\u00a0[9]: Copied! <pre>score_train = pipe.score(\n    X_train, \n    y_train\n)\nscore_test = pipe.score(\n    X_test, # LINE TO BE REMOVED FOR STUDENTS\n    y_test # LINE TO BE REMOVED FOR STUDENTS\n)\n\nprint(f\"R2 (train): {score_train}\")\nprint(f\"R2 (test): {score_test}\")\n</pre> score_train = pipe.score(     X_train,      y_train ) score_test = pipe.score(     X_test, # LINE TO BE REMOVED FOR STUDENTS     y_test # LINE TO BE REMOVED FOR STUDENTS )  print(f\"R2 (train): {score_train}\") print(f\"R2 (test): {score_test}\") <pre>R2 (train): 0.8063431786340525\nR2 (test): 0.8024042308518796\n</pre> In\u00a0[10]: Copied! <pre>def compute_df_residual(pipe, X, y):\n    return pd.DataFrame(\n        {\n            \"y_true\": y.rating.reset_index(drop=True),\n            \"y_pred\": pipe.predict(X).reshape(-1),\n        }\n    ).assign(residual=lambda df: df.y_true - df.y_pred)\n\n\ndef plot_residual(df_residual):\n    df_residual.plot.hist(bins=200, subplots=True, layout=(1,3), figsize=(20, 5))\n    plt.show()\n    sns.violinplot(data=df_residual, x=\"y_true\", y=\"y_pred\")\n\n\ndf_residual_train = compute_df_residual(pipe, X_train, y_train)\ndf_residual_test = compute_df_residual(pipe, X_test, y_test)\nplot_residual(df_residual_train)\nplot_residual(df_residual_test)\n</pre> def compute_df_residual(pipe, X, y):     return pd.DataFrame(         {             \"y_true\": y.rating.reset_index(drop=True),             \"y_pred\": pipe.predict(X).reshape(-1),         }     ).assign(residual=lambda df: df.y_true - df.y_pred)   def plot_residual(df_residual):     df_residual.plot.hist(bins=200, subplots=True, layout=(1,3), figsize=(20, 5))     plt.show()     sns.violinplot(data=df_residual, x=\"y_true\", y=\"y_pred\")   df_residual_train = compute_df_residual(pipe, X_train, y_train) df_residual_test = compute_df_residual(pipe, X_test, y_test) plot_residual(df_residual_train) plot_residual(df_residual_test) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n</pre> <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n</pre> In\u00a0[11]: Copied! <pre>def compute_score(pipe, X, y, metric_names=None, label=None, verbose=False):\n    if metric_names is None:\n        metric_names = [\"neg_root_mean_squared_error\"]\n    scores = dict()\n    for metric_name in metric_names:\n        metric = metrics.get_scorer(metric_name)\n        score = metric(pipe, X, y)\n        printed_label = f\" ({label})\" if label else \"\"\n        if verbose:\n            print(f\"{metric_name}{printed_label}: {score:.3g}\")\n        scores.update({f\"{metric_name}{printed_label}\": score})\n    return scores\n\n\nmetric_names = [\n    \"neg_root_mean_squared_error\",\n    \"neg_mean_absolute_error\",\n    \"neg_mean_squared_error\",\n]\nscore_train = compute_score(\n    pipe, X_train, y_train, metric_names, label=\"train\", verbose=True\n)\nscore_test = compute_score(\n    pipe, X_test, y_test, metric_names, label=\"test\", verbose=True # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> def compute_score(pipe, X, y, metric_names=None, label=None, verbose=False):     if metric_names is None:         metric_names = [\"neg_root_mean_squared_error\"]     scores = dict()     for metric_name in metric_names:         metric = metrics.get_scorer(metric_name)         score = metric(pipe, X, y)         printed_label = f\" ({label})\" if label else \"\"         if verbose:             print(f\"{metric_name}{printed_label}: {score:.3g}\")         scores.update({f\"{metric_name}{printed_label}\": score})     return scores   metric_names = [     \"neg_root_mean_squared_error\",     \"neg_mean_absolute_error\",     \"neg_mean_squared_error\", ] score_train = compute_score(     pipe, X_train, y_train, metric_names, label=\"train\", verbose=True ) score_test = compute_score(     pipe, X_test, y_test, metric_names, label=\"test\", verbose=True # LINE TO BE REMOVED FOR STUDENTS ) <pre>neg_root_mean_squared_error (train): -1.47\nneg_mean_absolute_error (train): -1.09\nneg_mean_squared_error (train): -2.17\nneg_root_mean_squared_error (test): -1.49\nneg_mean_absolute_error (test): -1.09\nneg_mean_squared_error (test): -2.22\n</pre> In\u00a0[12]: Copied! <pre>def get_feature_names(pipe, X):\n    return (\n        X.columns\n        if len(pipe) == 1\n        else pipe[:-1].get_feature_names_out(X.columns)\n    )\n\n\ndef plot_features_coefficients(pipe, X):\n    df_coef = pd.DataFrame(\n        {\"coef\": pipe[-1].coef_.reshape(-1)}, index=get_feature_names(pipe, X)\n    ).assign(color=lambda df: (df.coef &gt;= 0).map({True: \"g\", False: \"r\"}))\n    df_coef.coef.plot(\n        kind=\"barh\", color=df_coef.color, figsize=(6, len(df_coef) * 0.25)\n    )\n\n\nplot_features_coefficients(pipe, X_train)\n</pre> def get_feature_names(pipe, X):     return (         X.columns         if len(pipe) == 1         else pipe[:-1].get_feature_names_out(X.columns)     )   def plot_features_coefficients(pipe, X):     df_coef = pd.DataFrame(         {\"coef\": pipe[-1].coef_.reshape(-1)}, index=get_feature_names(pipe, X)     ).assign(color=lambda df: (df.coef &gt;= 0).map({True: \"g\", False: \"r\"}))     df_coef.coef.plot(         kind=\"barh\", color=df_coef.color, figsize=(6, len(df_coef) * 0.25)     )   plot_features_coefficients(pipe, X_train) <p>The coefficients of the linear regression can be misleading ! They do not represent the feature importance if the explicative variables are not comparable.</p> In\u00a0[13]: Copied! <pre>df_master[\"alcohol\"].plot(kind=\"hist\", bins=100)\n</pre> df_master[\"alcohol\"].plot(kind=\"hist\", bins=100) Out[13]: <pre>&lt;Axes: ylabel='Frequency'&gt;</pre> <p>We can see that we have few lines with alcohol = -1. Let's remove those lines</p> In\u00a0[14]: Copied! <pre>df_clean = (\n    df_master\n    .loc[lambda df: df.alcohol &gt;= 0] # LINE TO BE REMOVED FOR STUDENTS\n)\n\ndf_clean[\"alcohol\"].plot(kind=\"hist\", bins=100)\n</pre> df_clean = (     df_master     .loc[lambda df: df.alcohol &gt;= 0] # LINE TO BE REMOVED FOR STUDENTS )  df_clean[\"alcohol\"].plot(kind=\"hist\", bins=100) Out[14]: <pre>&lt;Axes: ylabel='Frequency'&gt;</pre> In\u00a0[15]: Copied! <pre>(\n    df_features_and_target_train,\n    df_features_and_target_test,\n) = model_selection.train_test_split(df_clean, test_size=0.1)\n</pre> (     df_features_and_target_train,     df_features_and_target_test, ) = model_selection.train_test_split(df_clean, test_size=0.1) In\u00a0[16]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"alcohol\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\npipe = pipeline.make_pipeline(\n    linear_model.LinearRegression(), # LINE TO BE REMOVED FOR STUDENTS\n)\n\npipe.fit(X_train, y_train)\n</pre> target = [\"rating\"] features = [     \"alcohol\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  pipe = pipeline.make_pipeline(     linear_model.LinearRegression(), # LINE TO BE REMOVED FOR STUDENTS )  pipe.fit(X_train, y_train) Out[16]: <pre>Pipeline(steps=[('linearregression', LinearRegression())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('linearregression', LinearRegression())])</pre>LinearRegression<pre>LinearRegression()</pre> In\u00a0[17]: Copied! <pre>plot_features_coefficients(pipe, X_train)\n</pre> plot_features_coefficients(pipe, X_train) <p>Let's have a look at the impact on the MAE</p> In\u00a0[18]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) <pre>MAE train : -1.08\nMAE test : -1.09\n</pre> <p>How to use categorical variables ?</p> type rating Stout 14 Belgian Ale 11 IPA 13 <p>We have to encode them to numerical values. Multiple approaches exist for that.</p> <p>Label encoding</p> type type_encoded rating Stout 1 14 Belgian Ale 2 11 IPA 3 13 <p>Label encoding is a simple way to encode categorical variables. However, it creates a hierarchy/order between the categories, which does not always reflect the reality.</p> <p>One hot encoding</p> type type_Stout type_Belgian_Ale type_IPA rating Stout 1 0 0 14 Belgian Ale 0 1 0 11 IPA 0 0 1 13 <p>Here, there is no hierarchy, but in the case of a lot of categories it will add many columns to the dataset.</p> <p>Let's try to one hot encode some categorical variables ! For that, we will create a custom categorical variable from alcohol.</p> In\u00a0[19]: Copied! <pre>df_clean.head(3)\n</pre> df_clean.head(3) Out[19]: beer brewery alcohol type rating_appearance rating_aroma rating_palate rating_taste rating timestamp user text beer_degree brewery_degree user_degree text_length date is_good 0 Breckenridge Oatmeal Stout 383 4.95 Stout 4 7 4 7 14 1217462400 blutt59 bottle, oat nose with black color, bitter choc... 51 413 300 90 2008-07-31 02:00:00 0 1 Breckenridge 471 Small Batch Imperial Porter 383 7.50 Imperial/Strong Porter 3 8 3 8 14 1312588800 blutt59 bottle, received in trade, dark brown with gar... 4 413 300 191 2011-08-06 02:00:00 0 2 Breckenridge Avalanche Amber 383 5.41 Amber Ale 3 5 3 5 10 1205020800 blutt59 12 oz. bottle, amber color with soapy head, sl... 43 413 300 109 2008-03-09 01:00:00 0 In\u00a0[20]: Copied! <pre>def alcohol_level (row):\n    if row['alcohol'] &lt; 5.5 :\n        return 'Light'\n    if row['alcohol'] &gt; 7 :\n        return 'Strong'\n    else :\n        return 'Medium'\n\ndf_clean['alcohol_level'] = df_clean.apply(lambda row: alcohol_level(row), axis=1)\n\ndf_clean['alcohol_level'].value_counts()\n</pre> def alcohol_level (row):     if row['alcohol'] &lt; 5.5 :         return 'Light'     if row['alcohol'] &gt; 7 :         return 'Strong'     else :         return 'Medium'  df_clean['alcohol_level'] = df_clean.apply(lambda row: alcohol_level(row), axis=1)  df_clean['alcohol_level'].value_counts() <pre>/tmp/ipykernel_2016/4176320187.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_clean['alcohol_level'] = df_clean.apply(lambda row: alcohol_level(row), axis=1)\n</pre> Out[20]: <pre>alcohol_level\nLight     74945\nStrong    62413\nMedium    53205\nName: count, dtype: int64</pre> In\u00a0[21]: Copied! <pre>df_clean.head()\n</pre> df_clean.head() Out[21]: beer brewery alcohol type rating_appearance rating_aroma rating_palate rating_taste rating timestamp user text beer_degree brewery_degree user_degree text_length date is_good alcohol_level 0 Breckenridge Oatmeal Stout 383 4.95 Stout 4 7 4 7 14 1217462400 blutt59 bottle, oat nose with black color, bitter choc... 51 413 300 90 2008-07-31 02:00:00 0 Light 1 Breckenridge 471 Small Batch Imperial Porter 383 7.50 Imperial/Strong Porter 3 8 3 8 14 1312588800 blutt59 bottle, received in trade, dark brown with gar... 4 413 300 191 2011-08-06 02:00:00 0 Strong 2 Breckenridge Avalanche Amber 383 5.41 Amber Ale 3 5 3 5 10 1205020800 blutt59 12 oz. bottle, amber color with soapy head, sl... 43 413 300 109 2008-03-09 01:00:00 0 Light 3 Breckenridge Lucky U IPA 383 6.20 India Pale Ale (IPA) 3 6 3 7 12 1255737600 blutt59 bottle, golden orange color with light tan foa... 20 413 300 151 2009-10-17 02:00:00 0 Medium 4 Fullers Vintage Ale 2009 55 8.50 English Strong Ale 3 7 3 8 14 1282003200 blutt59 bottle, thanks to SS, almond amber colored pou... 18 978 300 159 2010-08-17 02:00:00 0 Strong In\u00a0[22]: Copied! <pre>(\n    df_features_and_target_train,\n    df_features_and_target_test,\n) = model_selection.train_test_split(df_clean, test_size=0.1, random_state=1)\n\ntarget = [\"rating\"]\nfeatures = [\n    \"alcohol_level\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\nX_train = (\n    pd.get_dummies(X_train, columns=[\"alcohol_level\"]) # LINE TO BE REMOVED FOR STUDENTS\n)\nX_test = (\n    pd.get_dummies(X_test, columns=[\"alcohol_level\"]) # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> (     df_features_and_target_train,     df_features_and_target_test, ) = model_selection.train_test_split(df_clean, test_size=0.1, random_state=1)  target = [\"rating\"] features = [     \"alcohol_level\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  X_train = (     pd.get_dummies(X_train, columns=[\"alcohol_level\"]) # LINE TO BE REMOVED FOR STUDENTS ) X_test = (     pd.get_dummies(X_test, columns=[\"alcohol_level\"]) # LINE TO BE REMOVED FOR STUDENTS ) In\u00a0[23]: Copied! <pre>pipe = pipeline.make_pipeline(\n    linear_model.LinearRegression(),\n)\n\npipe.fit(X_train, y_train)\n\nplot_features_coefficients(pipe, X_train)\n</pre> pipe = pipeline.make_pipeline(     linear_model.LinearRegression(), )  pipe.fit(X_train, y_train)  plot_features_coefficients(pipe, X_train) In\u00a0[24]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) <pre>MAE train : -1.08\nMAE test : -1.09\n</pre> In\u00a0[25]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"alcohol_level\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\nX_train = pd.get_dummies(X_train, columns=[\"alcohol_level\"])\nX_test = pd.get_dummies(X_test, columns=[\"alcohol_level\"])\n\npipe = pipeline.make_pipeline(\n    compose.ColumnTransformer([\n        ('scaler', preprocessing.StandardScaler(), ['rating_appearance', 'rating_aroma', 'rating_palate', 'rating_taste']),\n        ('passthrough', \"passthrough\", [\"alcohol_level_Strong\", \"alcohol_level_Medium\", \"alcohol_level_Light\"])\n    ], remainder='passthrough'),\n    linear_model.LinearRegression(),\n)\n\npipe.fit(X_train, y_train)\n</pre> target = [\"rating\"] features = [     \"alcohol_level\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  X_train = pd.get_dummies(X_train, columns=[\"alcohol_level\"]) X_test = pd.get_dummies(X_test, columns=[\"alcohol_level\"])  pipe = pipeline.make_pipeline(     compose.ColumnTransformer([         ('scaler', preprocessing.StandardScaler(), ['rating_appearance', 'rating_aroma', 'rating_palate', 'rating_taste']),         ('passthrough', \"passthrough\", [\"alcohol_level_Strong\", \"alcohol_level_Medium\", \"alcohol_level_Light\"])     ], remainder='passthrough'),     linear_model.LinearRegression(), )  pipe.fit(X_train, y_train) Out[25]: <pre>Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('scaler', StandardScaler(),\n                                                  ['rating_appearance',\n                                                   'rating_aroma',\n                                                   'rating_palate',\n                                                   'rating_taste']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['alcohol_level_Strong',\n                                                   'alcohol_level_Medium',\n                                                   'alcohol_level_Light'])])),\n                ('linearregression', LinearRegression())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('scaler', StandardScaler(),\n                                                  ['rating_appearance',\n                                                   'rating_aroma',\n                                                   'rating_palate',\n                                                   'rating_taste']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['alcohol_level_Strong',\n                                                   'alcohol_level_Medium',\n                                                   'alcohol_level_Light'])])),\n                ('linearregression', LinearRegression())])</pre>columntransformer: ColumnTransformer<pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 ['rating_appearance', 'rating_aroma',\n                                  'rating_palate', 'rating_taste']),\n                                ('passthrough', 'passthrough',\n                                 ['alcohol_level_Strong',\n                                  'alcohol_level_Medium',\n                                  'alcohol_level_Light'])])</pre>scaler<pre>['rating_appearance', 'rating_aroma', 'rating_palate', 'rating_taste']</pre>StandardScaler<pre>StandardScaler()</pre>passthrough<pre>['alcohol_level_Strong', 'alcohol_level_Medium', 'alcohol_level_Light']</pre>passthrough<pre>passthrough</pre>remainder<pre>[]</pre>passthrough<pre>passthrough</pre>LinearRegression<pre>LinearRegression()</pre> <p>Check the features created &amp; the transformation applied to them, using histograms.</p> In\u00a0[26]: Copied! <pre>def plot_features_transformed_histograms(pipe, X, **kwargs):\n    if len(pipe) &gt; 1:\n        columns = pipe[:-1].get_feature_names_out(X.columns)\n        (\n            pd.DataFrame(\n                pipe[:-1].transform(X),\n                columns=columns,\n            ).plot.hist(subplots=True, layout=(2, 4), bins=200, figsize=(20, 10))\n        )\n    else:\n        print(\"no plot: features not transformed\")\n\n\nplot_features_transformed_histograms(pipe, X_train)\n</pre> def plot_features_transformed_histograms(pipe, X, **kwargs):     if len(pipe) &gt; 1:         columns = pipe[:-1].get_feature_names_out(X.columns)         (             pd.DataFrame(                 pipe[:-1].transform(X),                 columns=columns,             ).plot.hist(subplots=True, layout=(2, 4), bins=200, figsize=(20, 10))         )     else:         print(\"no plot: features not transformed\")   plot_features_transformed_histograms(pipe, X_train) <p>And now we can check again the linear regression coefficients. They can now be seen as feature importance.</p> In\u00a0[27]: Copied! <pre>plot_features_coefficients(pipe, X_train)\n</pre> plot_features_coefficients(pipe, X_train) In\u00a0[28]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) <pre>MAE train : -1.08\nMAE test : -1.09\n</pre> <p>We have also seen that some features do not have a gaussian distribution. However one of the assumption made by the linear regression is that all the features follow a gaussian distribution. We can use some transformers to make the features more normal.</p> In\u00a0[29]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"alcohol\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\npipe = pipeline.make_pipeline(\n    preprocessing.PowerTransformer(), # LINE TO BE REMOVED FOR STUDENTS\n    linear_model.LinearRegression(),\n)\n\npipe.fit(X_train, y_train)\n</pre> target = [\"rating\"] features = [     \"alcohol\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  pipe = pipeline.make_pipeline(     preprocessing.PowerTransformer(), # LINE TO BE REMOVED FOR STUDENTS     linear_model.LinearRegression(), )  pipe.fit(X_train, y_train) Out[29]: <pre>Pipeline(steps=[('powertransformer', PowerTransformer()),\n                ('linearregression', LinearRegression())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('powertransformer', PowerTransformer()),\n                ('linearregression', LinearRegression())])</pre>PowerTransformer<pre>PowerTransformer()</pre>LinearRegression<pre>LinearRegression()</pre> In\u00a0[30]: Copied! <pre>def plot_features_transformed_histograms(pipe, X, **kwargs):\n    if len(pipe) &gt; 1:\n        columns = pipe[:-1].get_feature_names_out(X.columns)\n        (\n            pd.DataFrame(\n                pipe[:-1].transform(X),\n                columns=columns,\n            ).plot.hist(subplots=True, layout=(2, 4), bins=200, figsize=(23, 10))\n        )\n    else:\n        print(\"no plot: features not transformed\")\n\nplot_features_transformed_histograms(pipe, X_train)\n</pre> def plot_features_transformed_histograms(pipe, X, **kwargs):     if len(pipe) &gt; 1:         columns = pipe[:-1].get_feature_names_out(X.columns)         (             pd.DataFrame(                 pipe[:-1].transform(X),                 columns=columns,             ).plot.hist(subplots=True, layout=(2, 4), bins=200, figsize=(23, 10))         )     else:         print(\"no plot: features not transformed\")  plot_features_transformed_histograms(pipe, X_train) In\u00a0[31]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) <pre>MAE train : -1.19\nMAE test : -1.2\n</pre> In\u00a0[32]: Copied! <pre>plot_features_coefficients(pipe, X_train)\n</pre> plot_features_coefficients(pipe, X_train) <p>Another possible feature engineering step is to add some non linearity. As the linear regression is a linear model, the non linearity can be managed thanks to the use of polynomial features.</p> In\u00a0[33]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"alcohol\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\npipe = pipeline.make_pipeline(\n    preprocessing.PolynomialFeatures(degree=2), # LINE TO BE REMOVED FOR STUDENTS\n    preprocessing.StandardScaler(),\n    linear_model.LinearRegression(),\n)\n\npipe.fit(X_train, y_train)\n</pre> target = [\"rating\"] features = [     \"alcohol\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  pipe = pipeline.make_pipeline(     preprocessing.PolynomialFeatures(degree=2), # LINE TO BE REMOVED FOR STUDENTS     preprocessing.StandardScaler(),     linear_model.LinearRegression(), )  pipe.fit(X_train, y_train) Out[33]: <pre>Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n                ('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n                ('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])</pre>PolynomialFeatures<pre>PolynomialFeatures()</pre>StandardScaler<pre>StandardScaler()</pre>LinearRegression<pre>LinearRegression()</pre> In\u00a0[34]: Copied! <pre>plot_features_coefficients(pipe, X_train)\n</pre> plot_features_coefficients(pipe, X_train) In\u00a0[35]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) <pre>MAE train : -1.06\nMAE test : -1.07\n</pre> In\u00a0[36]: Copied! <pre>def tokenize(serie):\n    return (\n        (serie)\n        .str.lower() # LINE TO BE REMOVED FOR STUDENTS\n        .str.replace(r\"[^a-z]\", \" \")\n        .str.replace(r\" +\", \" \")\n        .str.split(\" \")\n    )\n\n\ndf_words_count = (\n    (df_clean)\n    .assign(tokenized_text=lambda df: tokenize(df.text))\n    .loc[:, [\"rating\", \"tokenized_text\"]]\n    .explode(\"tokenized_text\")\n    .groupby(\"tokenized_text\", as_index=False)\n    .agg([\"mean\", \"count\"])\n    .reset_index()\n    .sort_values(by=(\"rating\", \"count\"), ascending=False)\n    .loc[lambda df: ~df.tokenized_text.isin(list(STOPWORDS))]\n    .loc[lambda df: df.tokenized_text.str.len() &gt; 1]\n    .head(1000) # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> def tokenize(serie):     return (         (serie)         .str.lower() # LINE TO BE REMOVED FOR STUDENTS         .str.replace(r\"[^a-z]\", \" \")         .str.replace(r\" +\", \" \")         .str.split(\" \")     )   df_words_count = (     (df_clean)     .assign(tokenized_text=lambda df: tokenize(df.text))     .loc[:, [\"rating\", \"tokenized_text\"]]     .explode(\"tokenized_text\")     .groupby(\"tokenized_text\", as_index=False)     .agg([\"mean\", \"count\"])     .reset_index()     .sort_values(by=(\"rating\", \"count\"), ascending=False)     .loc[lambda df: ~df.tokenized_text.isin(list(STOPWORDS))]     .loc[lambda df: df.tokenized_text.str.len() &gt; 1]     .head(1000) # LINE TO BE REMOVED FOR STUDENTS ) In\u00a0[37]: Copied! <pre>df_words_count.sort_values(by=[('rating',  'mean')])\n</pre> df_words_count.sort_values(by=[('rating',  'mean')]) Out[37]: index tokenized_text rating mean count 87390 87390 corn 8.36 2165 76903 76903 cheap 8.79 1153 252071 252071 water 9.10 1129 60309 60309 bland 9.74 1372 62886 62886 boring 9.92 1045 ... ... ... ... ... 188864 188864 perfect 16.08 2141 257196 257196 wonderful 16.25 2040 109976 109976 fantastic 16.29 1047 42498 42498 amazing 16.34 1184 49833 49833 awesome 16.35 1122 <p>1000 rows \u00d7 4 columns</p> In\u00a0[38]: Copied! <pre>word_mean_review = (\n    df_words_count.set_index(\"tokenized_text\").rating[\"mean\"].to_dict()\n)\n</pre> word_mean_review = (     df_words_count.set_index(\"tokenized_text\").rating[\"mean\"].to_dict() ) In\u00a0[39]: Copied! <pre>positive_words_set = set(\n    df_words_count.loc[lambda df: df.rating[\"mean\"] &gt;= 14.5]\n    .loc[:, \"tokenized_text\"]\n    .tolist()\n)\nnegative_words_set = set(\n    df_words_count.loc[lambda df: df.rating[\"mean\"] &lt;= 13] # LINE TO BE REMOVED FOR STUDENTS\n    .loc[:, \"tokenized_text\"]\n    .tolist()\n)\n</pre> positive_words_set = set(     df_words_count.loc[lambda df: df.rating[\"mean\"] &gt;= 14.5]     .loc[:, \"tokenized_text\"]     .tolist() ) negative_words_set = set(     df_words_count.loc[lambda df: df.rating[\"mean\"] &lt;= 13] # LINE TO BE REMOVED FOR STUDENTS     .loc[:, \"tokenized_text\"]     .tolist() ) In\u00a0[40]: Copied! <pre>print(len(negative_words_set))\nprint(len(positive_words_set))\n</pre> print(len(negative_words_set)) print(len(positive_words_set)) <pre>167\n155\n</pre> In\u00a0[41]: Copied! <pre>def compute_mean_word_rating(words, word_mean_review):\n    return np.mean(\n        [word_mean_review[w] for w in words if w in word_mean_review] or [0]\n    )\n\n\ndef count_words_in_set(words, word_set):\n    return len(set(words) &amp; word_set)\n\n\ndf_features_and_target = (\n    (df_clean)\n    .assign(tokenized_text=lambda df: tokenize(df.text))\n    .assign(\n        mean_word_rating=lambda df: (df.tokenized_text).apply(\n            compute_mean_word_rating, args=(word_mean_review,)\n        )\n    )\n    .assign(\n        positive_words_count=lambda df: (df.tokenized_text).apply(\n            count_words_in_set, args=(positive_words_set,)\n        )\n    )\n    .assign(\n        negative_words_count=lambda df: (df.tokenized_text).apply(\n            count_words_in_set, args=(negative_words_set,)\n        )\n    )\n)\n</pre> def compute_mean_word_rating(words, word_mean_review):     return np.mean(         [word_mean_review[w] for w in words if w in word_mean_review] or [0]     )   def count_words_in_set(words, word_set):     return len(set(words) &amp; word_set)   df_features_and_target = (     (df_clean)     .assign(tokenized_text=lambda df: tokenize(df.text))     .assign(         mean_word_rating=lambda df: (df.tokenized_text).apply(             compute_mean_word_rating, args=(word_mean_review,)         )     )     .assign(         positive_words_count=lambda df: (df.tokenized_text).apply(             count_words_in_set, args=(positive_words_set,)         )     )     .assign(         negative_words_count=lambda df: (df.tokenized_text).apply(             count_words_in_set, args=(negative_words_set,)         )     ) ) In\u00a0[42]: Copied! <pre>df_features_and_target.head(5)\n</pre> df_features_and_target.head(5) Out[42]: beer brewery alcohol type rating_appearance rating_aroma rating_palate rating_taste rating timestamp ... brewery_degree user_degree text_length date is_good alcohol_level tokenized_text mean_word_rating positive_words_count negative_words_count 0 Breckenridge Oatmeal Stout 383 4.95 Stout 4 7 4 7 14 1217462400 ... 413 300 90 2008-07-31 02:00:00 0 Light [bottle,, oat, nose, with, black, color,, bitt... 14.09 4 0 1 Breckenridge 471 Small Batch Imperial Porter 383 7.50 Imperial/Strong Porter 3 8 3 8 14 1312588800 ... 413 300 191 2011-08-06 02:00:00 0 Strong [bottle,, received, in, trade,, dark, brown, w... 13.92 4 1 2 Breckenridge Avalanche Amber 383 5.41 Amber Ale 3 5 3 5 10 1205020800 ... 413 300 109 2008-03-09 01:00:00 0 Light [12, oz., bottle,, amber, color, with, soapy, ... 13.24 0 3 3 Breckenridge Lucky U IPA 383 6.20 India Pale Ale (IPA) 3 6 3 7 12 1255737600 ... 413 300 151 2009-10-17 02:00:00 0 Medium [bottle,, golden, orange, color, with, light, ... 13.64 2 2 4 Fullers Vintage Ale 2009 55 8.50 English Strong Ale 3 7 3 8 14 1282003200 ... 978 300 159 2010-08-17 02:00:00 0 Strong [bottle,, thanks, to, ss,, almond, amber, colo... 13.46 1 3 <p>5 rows \u00d7 23 columns</p> In\u00a0[43]: Copied! <pre>(\n    df_features_and_target_train,\n    df_features_and_target_test,\n) = model_selection.train_test_split(df_features_and_target, test_size=0.1, random_state=1)\n</pre> (     df_features_and_target_train,     df_features_and_target_test, ) = model_selection.train_test_split(df_features_and_target, test_size=0.1, random_state=1) In\u00a0[44]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\",\n    \"alcohol\",\n    \"user_degree\",\n    \"positive_words_count\", # LINE TO BE REMOVED FOR STUDENTS\n    \"negative_words_count\", # LINE TO BE REMOVED FOR STUDENTS\n    \"mean_word_rating\" # LINE TO BE REMOVED FOR STUDENTS\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\npipe = pipeline.make_pipeline(\n    preprocessing.StandardScaler(),\n    linear_model.LinearRegression(),\n)\n\npipe.fit(X_train, y_train)\n</pre> target = [\"rating\"] features = [     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\",     \"alcohol\",     \"user_degree\",     \"positive_words_count\", # LINE TO BE REMOVED FOR STUDENTS     \"negative_words_count\", # LINE TO BE REMOVED FOR STUDENTS     \"mean_word_rating\" # LINE TO BE REMOVED FOR STUDENTS ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  pipe = pipeline.make_pipeline(     preprocessing.StandardScaler(),     linear_model.LinearRegression(), )  pipe.fit(X_train, y_train) Out[44]: <pre>Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])</pre>StandardScaler<pre>StandardScaler()</pre>LinearRegression<pre>LinearRegression()</pre> In\u00a0[45]: Copied! <pre>plot_features_coefficients(pipe, X_train)\n</pre> plot_features_coefficients(pipe, X_train) In\u00a0[46]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) <pre>MAE train : -1.08\nMAE test : -1.09\n</pre> In\u00a0[47]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\",\n    \"alcohol\",\n    \"user_degree\",\n    \"positive_words_count\",\n    \"negative_words_count\",\n    \"mean_word_rating\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\nmodel = linear_model.LinearRegression()\ntrain_sizes_percent = np.geomspace(0.001, 1, 20)\n(\n    train_sizes,\n    train_scores,\n    validation_scores,\n    _,\n    _,\n) = model_selection.learning_curve(\n    model,\n    X_train,\n    y_train,\n    cv=7,\n    n_jobs=-1,\n    train_sizes=train_sizes_percent,\n    return_times=True,\n    verbose=1,\n)\n</pre> target = [\"rating\"] features = [     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\",     \"alcohol\",     \"user_degree\",     \"positive_words_count\",     \"negative_words_count\",     \"mean_word_rating\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  model = linear_model.LinearRegression() train_sizes_percent = np.geomspace(0.001, 1, 20) (     train_sizes,     train_scores,     validation_scores,     _,     _, ) = model_selection.learning_curve(     model,     X_train,     y_train,     cv=7,     n_jobs=-1,     train_sizes=train_sizes_percent,     return_times=True,     verbose=1, ) <pre>[learning_curve] Training set sizes: [   147    211    304    437    629    905   1302   1873   2694   3876\n   5575   8019  11536  16594  23870  34336  49391  71046 102196 147005]\n</pre> <pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    2.8s finished\n</pre> <p>Plot the training &amp; validation scores.</p> <p>Since they come from a K-fold cross-validation, you have K values for each training set size: use seaborn's <code>lineplot</code> to plot the mean value with a confidence interval.</p> In\u00a0[48]: Copied! <pre>def compute_df_sizes(train_sizes):\n    return pd.DataFrame(\n        {\n            \"train_size_id\": range(len(train_sizes)),\n            \"train_size\": train_sizes,\n        }\n    )\n\n\ndef compute_df_score(scores, name):\n    df = (\n        pd.DataFrame(scores)\n        .stack()\n        .rename_axis(index=(\"train_size_id\", \"run_id\"))\n        .rename(name)\n        .reset_index()\n    )\n    return df\n\n\ndef compute_df_scores_long(train_scores, validation_scores, train_sizes):\n    return (\n        compute_df_score(train_scores, name=\"train_score\")\n        .merge(compute_df_score(validation_scores, name=\"validation_score\"))\n        .merge(compute_df_sizes(train_sizes), on=\"train_size_id\")\n        .melt(\n            id_vars=[\"train_size_id\", \"run_id\", \"train_size\"],\n            value_vars=[\"train_score\", \"validation_score\"],\n            var_name=\"score_name\",\n            value_name=\"score\",\n        )\n    )\n\n\ndf_scores_long = compute_df_scores_long(\n    train_scores, validation_scores, train_sizes\n)\nsns.lineplot(\n    data=df_scores_long.loc[lambda df: df.score &gt; 0.5],\n    x=\"train_size\",\n    y=\"score\",\n    hue=\"score_name\",\n    style=\"score_name\",\n    dashes=False,\n    markers=True,\n).set(xscale=\"log\")\n</pre> def compute_df_sizes(train_sizes):     return pd.DataFrame(         {             \"train_size_id\": range(len(train_sizes)),             \"train_size\": train_sizes,         }     )   def compute_df_score(scores, name):     df = (         pd.DataFrame(scores)         .stack()         .rename_axis(index=(\"train_size_id\", \"run_id\"))         .rename(name)         .reset_index()     )     return df   def compute_df_scores_long(train_scores, validation_scores, train_sizes):     return (         compute_df_score(train_scores, name=\"train_score\")         .merge(compute_df_score(validation_scores, name=\"validation_score\"))         .merge(compute_df_sizes(train_sizes), on=\"train_size_id\")         .melt(             id_vars=[\"train_size_id\", \"run_id\", \"train_size\"],             value_vars=[\"train_score\", \"validation_score\"],             var_name=\"score_name\",             value_name=\"score\",         )     )   df_scores_long = compute_df_scores_long(     train_scores, validation_scores, train_sizes ) sns.lineplot(     data=df_scores_long.loc[lambda df: df.score &gt; 0.5],     x=\"train_size\",     y=\"score\",     hue=\"score_name\",     style=\"score_name\",     dashes=False,     markers=True, ).set(xscale=\"log\") <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[48]: <pre>[None]</pre> <p>Plot the features importance of the linear model trained on the full dataset</p> In\u00a0[49]: Copied! <pre>pipe = pipeline.make_pipeline(model)\npipe.fit(X_train, y_train)\nplot_features_coefficients(pipe, X_train)\n</pre> pipe = pipeline.make_pipeline(model) pipe.fit(X_train, y_train) plot_features_coefficients(pipe, X_train) In\u00a0[50]: Copied! <pre>features = [\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\",\n    \"alcohol\",\n    \"user_degree\",\n    \"positive_words_count\",\n    \"negative_words_count\",\n    \"mean_word_rating\"\n]\n\npipe = pipeline.make_pipeline(\n    preprocessing.PolynomialFeatures(include_bias=False, degree=2),\n    preprocessing.PowerTransformer(),\n    linear_model.LinearRegression(),\n)\n(\n    train_sizes,\n    train_scores,\n    validation_scores,\n    fit_times,\n    _,\n) = model_selection.learning_curve(\n    pipe,\n    X_train,\n    y_train,\n    cv=5,\n    n_jobs=-1,\n    train_sizes=train_sizes_percent,\n    return_times=True,\n    verbose=1,\n)\n\ndf_scores_long = compute_df_scores_long(\n    train_scores, validation_scores, train_sizes\n)\nsns.lineplot(\n    data=df_scores_long.loc[lambda df: df.score &gt; 0.5],\n    x=\"train_size\",\n    y=\"score\",\n    hue=\"score_name\",\n    style=\"score_name\",\n    dashes=False,\n    markers=True,\n).set(xscale=\"log\")\n</pre> features = [     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\",     \"alcohol\",     \"user_degree\",     \"positive_words_count\",     \"negative_words_count\",     \"mean_word_rating\" ]  pipe = pipeline.make_pipeline(     preprocessing.PolynomialFeatures(include_bias=False, degree=2),     preprocessing.PowerTransformer(),     linear_model.LinearRegression(), ) (     train_sizes,     train_scores,     validation_scores,     fit_times,     _, ) = model_selection.learning_curve(     pipe,     X_train,     y_train,     cv=5,     n_jobs=-1,     train_sizes=train_sizes_percent,     return_times=True,     verbose=1, )  df_scores_long = compute_df_scores_long(     train_scores, validation_scores, train_sizes ) sns.lineplot(     data=df_scores_long.loc[lambda df: df.score &gt; 0.5],     x=\"train_size\",     y=\"score\",     hue=\"score_name\",     style=\"score_name\",     dashes=False,     markers=True, ).set(xscale=\"log\") <pre>[learning_curve] Training set sizes: [   137    197    283    408    587    844   1215   1748   2514   3617\n   5203   7485  10767  15488  22278  32047  46098  66309  95383 137204]\n</pre> <pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   44.2s finished\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> Out[50]: <pre>[None]</pre> <p>Plot the features importance of the linear model with polynomial features trained on the full dataset with 6 features</p> In\u00a0[51]: Copied! <pre>pipe.fit(X_train, y_train)\nplot_features_coefficients(pipe, X_train)\n</pre> pipe.fit(X_train, y_train) plot_features_coefficients(pipe, X_train) <p>From here we'll use a dataset containing 2000 rows &amp; 7 features</p> In\u00a0[52]: Copied! <pre>N = 2000\nfeatures = [\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\",\n    \"mean_word_rating\",\n    \"positive_words_count\",\n    \"negative_words_count\",\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n    N,\n)\n</pre> N = 2000 features = [     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\",     \"mean_word_rating\",     \"positive_words_count\",     \"negative_words_count\", ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target,     N, ) In\u00a0[53]: Copied! <pre>alphas = np.logspace(-6, 3, 28)\n\ndef compute_scores_and_coefs(pipe, alphas):\n    scores = dict()\n    coefs = dict()\n    for alpha in tqdm(alphas):\n        pipe[-1].set_params(alpha=alpha)\n        pipe.fit(X_train, y_train)\n        score_train = compute_score(pipe, X_train, y_train, metric_names = [\"neg_mean_absolute_error\"], label=\"train\")\n        score_test = compute_score(pipe, X_test, y_test, metric_names = [\"neg_mean_absolute_error\"], label=\"test\")\n        scores.update({alpha: {**score_train, **score_test}})\n        coef = dict(\n            zip(get_feature_names(pipe, X_train), pipe[-1].coef_.reshape(-1))\n        )\n        coefs.update({alpha: coef})\n    return scores, coefs\n\n\npipe = pipeline.make_pipeline(\n    preprocessing.PolynomialFeatures(include_bias=False, degree=2),\n    preprocessing.PowerTransformer(),\n    linear_model.Ridge(),\n)\nscores, coefs = compute_scores_and_coefs(pipe, alphas)\n</pre> alphas = np.logspace(-6, 3, 28)  def compute_scores_and_coefs(pipe, alphas):     scores = dict()     coefs = dict()     for alpha in tqdm(alphas):         pipe[-1].set_params(alpha=alpha)         pipe.fit(X_train, y_train)         score_train = compute_score(pipe, X_train, y_train, metric_names = [\"neg_mean_absolute_error\"], label=\"train\")         score_test = compute_score(pipe, X_test, y_test, metric_names = [\"neg_mean_absolute_error\"], label=\"test\")         scores.update({alpha: {**score_train, **score_test}})         coef = dict(             zip(get_feature_names(pipe, X_train), pipe[-1].coef_.reshape(-1))         )         coefs.update({alpha: coef})     return scores, coefs   pipe = pipeline.make_pipeline(     preprocessing.PolynomialFeatures(include_bias=False, degree=2),     preprocessing.PowerTransformer(),     linear_model.Ridge(), ) scores, coefs = compute_scores_and_coefs(pipe, alphas) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28/28 [00:03&lt;00:00,  7.61it/s]\n</pre> <p>Plot the evolution of the score as a function of <code>alpha</code></p> In\u00a0[54]: Copied! <pre>(\n    pd.DataFrame.from_dict(scores, orient=\"index\")\n    .plot(logx=True, marker=\".\", ylim=(-1.2, -1.05))\n)\n</pre> (     pd.DataFrame.from_dict(scores, orient=\"index\")     .plot(logx=True, marker=\".\", ylim=(-1.2, -1.05)) ) Out[54]: <pre>&lt;Axes: &gt;</pre> <p>Plot the evolution of the coefficients as a function of <code>alpha</code></p> In\u00a0[55]: Copied! <pre>(\n    pd.DataFrame.from_dict(coefs, orient=\"index\").plot(\n        logx=True, legend=False, marker=\".\"\n    )\n)\n</pre> (     pd.DataFrame.from_dict(coefs, orient=\"index\").plot(         logx=True, legend=False, marker=\".\"     ) ) Out[55]: <pre>&lt;Axes: &gt;</pre> <p>Plot the features importance of a model trained with a specific <code>alpha</code> value (e.g: <code>alpha=1e-1</code>)</p> In\u00a0[56]: Copied! <pre>pipe[-1].set_params(\n    alpha=1e1 # LINE TO BE REMOVED FOR STUDENTS\n)\npipe.fit(X_train, y_train)\nplot_features_coefficients(pipe, X_train)\n</pre> pipe[-1].set_params(     alpha=1e1 # LINE TO BE REMOVED FOR STUDENTS ) pipe.fit(X_train, y_train) plot_features_coefficients(pipe, X_train) In\u00a0[57]: Copied! <pre>pipe = pipeline.make_pipeline(\n    preprocessing.PolynomialFeatures(include_bias=False, degree=2),\n    preprocessing.PowerTransformer(),\n    linear_model.Lasso(),\n)\nscores, coefs = compute_scores_and_coefs(pipe, alphas)\n</pre> pipe = pipeline.make_pipeline(     preprocessing.PolynomialFeatures(include_bias=False, degree=2),     preprocessing.PowerTransformer(),     linear_model.Lasso(), ) scores, coefs = compute_scores_and_coefs(pipe, alphas) <pre>  0%|          | 0/28 [00:00&lt;?, ?it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.090e+03, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n  4%|\u258e         | 1/28 [00:00&lt;00:04,  5.43it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.088e+03, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n  7%|\u258b         | 2/28 [00:00&lt;00:05,  5.11it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.084e+03, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 11%|\u2588         | 3/28 [00:00&lt;00:04,  5.22it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.076e+03, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 14%|\u2588\u258d        | 4/28 [00:00&lt;00:04,  5.30it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.057e+03, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 18%|\u2588\u258a        | 5/28 [00:00&lt;00:04,  5.32it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.016e+03, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 21%|\u2588\u2588\u258f       | 6/28 [00:01&lt;00:04,  5.35it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.924e+03, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 25%|\u2588\u2588\u258c       | 7/28 [00:01&lt;00:04,  5.19it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.730e+03, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 29%|\u2588\u2588\u258a       | 8/28 [00:01&lt;00:03,  5.24it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.388e+03, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 32%|\u2588\u2588\u2588\u258f      | 9/28 [00:01&lt;00:03,  5.34it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.910e+02, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 36%|\u2588\u2588\u2588\u258c      | 10/28 [00:01&lt;00:03,  5.42it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.635e+01, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 39%|\u2588\u2588\u2588\u2589      | 11/28 [00:02&lt;00:03,  5.48it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+01, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 43%|\u2588\u2588\u2588\u2588\u258e     | 12/28 [00:02&lt;00:02,  5.58it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.858e+00, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 46%|\u2588\u2588\u2588\u2588\u258b     | 13/28 [00:02&lt;00:02,  5.54it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+00, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n 50%|\u2588\u2588\u2588\u2588\u2588     | 14/28 [00:02&lt;00:02,  5.73it/s]/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.760e+00, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28/28 [00:04&lt;00:00,  6.21it/s]\n</pre> <p>Plot the evolution of the score as a function of <code>alpha</code></p> In\u00a0[58]: Copied! <pre>(\n    pd.DataFrame.from_dict(scores, orient=\"index\")\n    .plot(logx=True, marker=\".\", ylim=(-1.2, -1.05))\n)\n</pre> (     pd.DataFrame.from_dict(scores, orient=\"index\")     .plot(logx=True, marker=\".\", ylim=(-1.2, -1.05)) ) Out[58]: <pre>&lt;Axes: &gt;</pre> <p>Plot the evolution of the coefficients as a function of <code>alpha</code></p> In\u00a0[59]: Copied! <pre>(\n    pd.DataFrame.from_dict(coefs, orient=\"index\").plot(\n        logx=True, legend=False, marker=\".\"\n    )\n)\n</pre> (     pd.DataFrame.from_dict(coefs, orient=\"index\").plot(         logx=True, legend=False, marker=\".\"     ) ) Out[59]: <pre>&lt;Axes: &gt;</pre> <p>Plot the features importance of a model trained with a specific <code>alpha</code> value (e.g: <code>alpha=1e-2</code>)</p> In\u00a0[60]: Copied! <pre>pipe[-1].set_params(\n    alpha=1e-2 # LINE TO BE REMOVED FOR STUDENTS\n)\npipe.fit(X_train, y_train)\nplot_features_coefficients(pipe, X_train)\nscore_train = pipe.score(X_train, y_train)\nscore_test = pipe.score(X_test, y_test)\nprint(score_train)\nprint(score_test)\n</pre> pipe[-1].set_params(     alpha=1e-2 # LINE TO BE REMOVED FOR STUDENTS ) pipe.fit(X_train, y_train) plot_features_coefficients(pipe, X_train) score_train = pipe.score(X_train, y_train) score_test = pipe.score(X_test, y_test) print(score_train) print(score_test) <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.858e+00, tolerance: 2.331e+00\n  model = cd_fast.enet_coordinate_descent(\n</pre> <pre>0.8110210044666418\n0.7937570454994202\n</pre>"},{"location":"10-Tutorials/3_regression/#practice-n3-regression","title":"\ud83d\udc0d Practice n\u00b03: regression\u00b6","text":"<p>The objective of this session is to understand the regression problems and the analytical approach to solve them. This notebook discusses theoretical notions and contains practical parts.</p> <p>Here are the main steps of the notebook :</p> <ol> <li>What is regression ?</li> <li>Focus on linear regression</li> <li>Preparation</li> <li>Implementation of a linear regression</li> <li>Model improvement</li> </ol>"},{"location":"10-Tutorials/3_regression/#1-what-is-regression","title":"1. What is regression ?\u00b6","text":""},{"location":"10-Tutorials/3_regression/#1d-example","title":"1D Example\u00b6","text":"<p>x = Number of aircrafts in an airspace sector</p> <p>y = Subjective rating of air traffic controller workload</p>"},{"location":"10-Tutorials/3_regression/#generalisation","title":"Generalisation\u00b6","text":""},{"location":"10-Tutorials/3_regression/#2-focus-on-linear-regression","title":"2. Focus on linear regression\u00b6","text":""},{"location":"10-Tutorials/3_regression/#1d-example","title":"1D Example\u00b6","text":""},{"location":"10-Tutorials/3_regression/#generalisation","title":"Generalisation\u00b6","text":""},{"location":"10-Tutorials/3_regression/#3-preparation","title":"3. Preparation\u00b6","text":""},{"location":"10-Tutorials/3_regression/#install-import-modules","title":"Install &amp; import modules\u00b6","text":""},{"location":"10-Tutorials/3_regression/#read-remote-dataset","title":"Read remote dataset\u00b6","text":""},{"location":"10-Tutorials/3_regression/#implementation-of-a-linear-regression","title":"Implementation of a linear regression\u00b6","text":"<p>In this first section, you will train a linear regression model to predict the overall rating of a review based on its other ratings. For that, you will :</p> <ul> <li>Load &amp; describe the data</li> <li>Split the data between train and test set</li> <li>Train and evaluate the linear regression model</li> </ul>"},{"location":"10-Tutorials/3_regression/#load-data-describe","title":"Load data &amp; describe\u00b6","text":""},{"location":"10-Tutorials/3_regression/#splits-traintest-featurestarget","title":"Splits: Train/test &amp; features/target\u00b6","text":""},{"location":"10-Tutorials/3_regression/#traintest-split","title":"Train/test split\u00b6","text":"<p>Split keeping 10% of the data in the test set.</p> <p>Hint: https://scikit-learn.org/stable/index.html</p>"},{"location":"10-Tutorials/3_regression/#featurestarget-split","title":"Features/target split\u00b6","text":"<p>Split keeping:</p> <ul> <li>only the wanted features</li> <li>N data samples in the training set</li> </ul>"},{"location":"10-Tutorials/3_regression/#model-training-evaluation","title":"Model training &amp; evaluation\u00b6","text":""},{"location":"10-Tutorials/3_regression/#model-training","title":"Model training\u00b6","text":"<p>Use a pipeline to wrap the model with its automatized preprocessing steps</p>"},{"location":"10-Tutorials/3_regression/#model-evaluation","title":"Model evaluation\u00b6","text":""},{"location":"10-Tutorials/3_regression/#r2-score","title":"R2 score\u00b6","text":""},{"location":"10-Tutorials/3_regression/#r2-score-in-practice","title":"R2 score in practice\u00b6","text":""},{"location":"10-Tutorials/3_regression/#residuals","title":"Residuals\u00b6","text":"<p>Compute residuals dataframe containing the true &amp; predicted values of <code>y</code> and and the residual, which is the difference between the true &amp; predicted values.</p> <p>Plot the residuals, using histograms &amp; violinplots.</p>"},{"location":"10-Tutorials/3_regression/#other-regression-metrics","title":"Other regression metrics\u00b6","text":"<p>Another popular metric is the Mean Squared Error (MSE)</p> <p>$$MSE = \\frac{1}{N} \\sum_{i}(y_i \u2013 \\hat{y}_i)^2$$</p> <p>This metric is interesting if we want to penalize large errors. However it is not easily interpretable as it is not in the same unit than the target.</p> <p>This is why the Root Mean Squared Error (RMSE) is widely used.</p> <p>$$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i}(y_i \u2013 \\hat{y}_i)^2}$$</p> <p>As well as the Mean Absolute Error which is even more interpretable.</p> <p>$$MAE = \\frac{1}{N} \\sum_{i}\\lvert(y_i \u2013 \\hat{y}_i)\\lvert$$</p>"},{"location":"10-Tutorials/3_regression/#other-metrics-in-practice","title":"Other metrics in practice\u00b6","text":"<p>Compute a set of metrics on train &amp; test datasets:</p> <ul> <li>negative RMSE</li> <li>negative MAE</li> <li>negative MSE</li> </ul> <p>NB: A \"score\" function in sklearn obeys to the \"greater is better\" principle. That's why error functions are converted to their negative versions to obey this principle.</p>"},{"location":"10-Tutorials/3_regression/#feature-importance","title":"Feature importance\u00b6","text":"<p>Plot model's coefficients in a bar chart</p>"},{"location":"10-Tutorials/3_regression/#model-improvement","title":"Model improvement\u00b6","text":""},{"location":"10-Tutorials/3_regression/#feature-engineering","title":"Feature engineering\u00b6","text":""},{"location":"10-Tutorials/3_regression/#outliers-management","title":"Outliers management\u00b6","text":""},{"location":"10-Tutorials/3_regression/#categorical-variable-encoding","title":"Categorical variable encoding\u00b6","text":""},{"location":"10-Tutorials/3_regression/#feature-scaling","title":"Feature scaling\u00b6","text":""},{"location":"10-Tutorials/3_regression/#feature-standardization","title":"Feature standardization\u00b6","text":""},{"location":"10-Tutorials/3_regression/#polynomial-features-optional","title":"Polynomial Features (optional)\u00b6","text":""},{"location":"10-Tutorials/3_regression/#review-feature-extraction","title":"Review feature extraction\u00b6","text":""},{"location":"10-Tutorials/3_regression/#word-counts-mean-review","title":"Word counts &amp; mean review\u00b6","text":"<p>Compute the \"word counts\" dataframe:</p> <ul> <li>containing:<ul> <li>count of words in review</li> <li>mean rating review of the reviews containing this text</li> </ul> </li> <li>remove stop words</li> <li>remove single letter words</li> <li>keep only the 1000 most frequent words</li> </ul>"},{"location":"10-Tutorials/3_regression/#word-mean-review","title":"Word mean review\u00b6","text":"<p>Compute the \"word ratings\" dictionary containing the mean review for each word</p>"},{"location":"10-Tutorials/3_regression/#positive-negative-words-sets","title":"Positive &amp; negative words sets\u00b6","text":"<ul> <li>Compute the \"positive words\" set containing the words with a rating &gt;= 14.5</li> <li>Compute the \"negative words\" set containing the words with a rating &lt;= 13</li> </ul>"},{"location":"10-Tutorials/3_regression/#features-target","title":"Features &amp; target\u00b6","text":"<p>Compute the \"features &amp; target\" dataframe containing:</p> <ul> <li>tokenized text</li> <li>mean word rating</li> <li>positive words count</li> <li>negative words count</li> <li>negative words negative count</li> </ul>"},{"location":"10-Tutorials/3_regression/#learning-curve","title":"Learning curve\u00b6","text":""},{"location":"10-Tutorials/3_regression/#simple-model","title":"Simple model\u00b6","text":"<p>Compute the training &amp; validation R2-scores for various training sets sizes</p>"},{"location":"10-Tutorials/3_regression/#complex-model-optional","title":"Complex model (optional)\u00b6","text":"<p>Compute the training &amp; validation R2-scores for various training sets sizes</p> <p>Plot the training &amp; validation scores.</p>"},{"location":"10-Tutorials/3_regression/#overfitting-underfitting","title":"Overfitting &amp; Underfitting\u00b6","text":"<p>Overfitting happens when a model fits quite well to the training data but does not generalize well on new data it has never seen. It can have multiple causes :</p> <ul> <li>The training dataset is not representative of the real data</li> <li>The model is too complex and learned from noise on the training dataset</li> </ul> <p>Underfitting happens when the model is too simple to capture the data patterns. in this case, the model would perform bad on the train and on the test dataset.</p> <p></p> <p>To avoid overfitting, multiple actions can be performed.</p>"},{"location":"10-Tutorials/3_regression/#model-regularization","title":"Model regularization\u00b6","text":"<p>The regularization allows to reduce the model complexity. For that, we will not only minimize our cost error, but we will minimize an objective function with a regularization term.</p> <p>$$min \\sum_{i}(y_i \u2013 X_i\\beta)^2 + Regularization$$</p>"},{"location":"10-Tutorials/3_regression/#ridge-l2-regularization","title":"Ridge - L2 regularization\u00b6","text":"<p>For the ridge regression, the regularization term will be the L2-norm of the coefficients :</p> <p>$$min \\sum_{i}(y_i \u2013 X_i\\beta)^2 + \\alpha\\vert\\vert\\beta\\vert\\vert_2^2$$</p> <p>where $$\\vert\\vert\\beta\\vert\\vert_2^2 = \\sum_{j}\\beta_j^2$$</p> <p>It is mainly used in the case of correlated features, which can bring unexpected behaviours with regard to the coefficients.</p>"},{"location":"10-Tutorials/3_regression/#simple-loop-on-regularization-parameter","title":"Simple loop on regularization parameter\u00b6","text":"<ul> <li>Fit <code>n</code> models for <code>n</code> values of <code>alpha</code> (regularization parameter)</li> <li>Compute the scores</li> <li>Retrieve the values of coefficients</li> </ul>"},{"location":"10-Tutorials/3_regression/#lasso-l1-regularization","title":"Lasso - L1 regularization\u00b6","text":"<p>For the lasso regression, the regularization term will be the L1-norm of the coefficients :</p> <p>$$min \\sum_{i}(y_i \u2013 X_i\\beta)^2 + \\alpha\\vert\\vert\\beta\\vert\\vert_1^2$$</p> <p>where $$\\vert\\vert\\beta\\vert\\vert_1^2 = \\sum_{j}\\vert\\beta_j\\vert$$</p>"},{"location":"10-Tutorials/3_regression/#simple-loop-on-regularization-parameter","title":"Simple loop on regularization parameter\u00b6","text":"<ul> <li>Fit <code>n</code> models for <code>n</code> values of <code>alpha</code> (regularization parameter)</li> <li>Compute the scores</li> <li>Retrieve the values of coefficients</li> </ul>"},{"location":"10-Tutorials/3_regression_students/","title":"\ud83d\udc0d Practice n\u00b03: regression (students version)","text":"<p>Regression in machine learning consists of mathematical methods that allow to predict a continuous outcome (y) based on the value of one or more predictor variables (x)</p> <p></p> <p>To predict the air traffic controller workload, we need to find the best function y=f(x). For that, we will define an error and find the function which minimizes this error.</p> <p></p> <p>In reality, we often deal with multiple explicative variables and the dimensionality of the input data is greater than 1.</p> <p>$$x = (x_1,...,x_n)$$ $$\\hat{y} = f(x_1,...,x_n)$$</p> <p>If we come back to our traffic control example, the air traffic controller workload can be a function of both the number of aircraft and the weather.</p> <p>The linear regression is a regression using a linear function \u0177 = ax + b</p> <p></p> <p>In reality, we often deal with multiple explicative variables and the dimensionality of the input data is greater than 1. The linear function will look like :</p> <p>$$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$$</p> <p>Now let's practice with the ratebeer dataset !</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom wordcloud import STOPWORDS\nimport numpy as np\nfrom sklearn import (\n    linear_model,\n    preprocessing,\n    pipeline,\n    model_selection,\n    metrics,\n    compose,\n)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport matplotlib as mpl\n\nmpl.rcParams['agg.path.chunksize'] = 10000\npd.set_option(\"display.precision\", 2)\nsns.set_style(\"whitegrid\")\nsns.set_context(rc={\"patch.linewidth\": 0.15})\n</pre> import pandas as pd from wordcloud import STOPWORDS import numpy as np from sklearn import (     linear_model,     preprocessing,     pipeline,     model_selection,     metrics,     compose, ) import seaborn as sns import matplotlib.pyplot as plt from tqdm import tqdm import matplotlib as mpl  mpl.rcParams['agg.path.chunksize'] = 10000 pd.set_option(\"display.precision\", 2) sns.set_style(\"whitegrid\") sns.set_context(rc={\"patch.linewidth\": 0.15}) <p>The data is in this git repository: ML-boot-camp/ratebeer.git.</p> <p>The data is located in the the <code>ratebeer/data/</code> folder.</p> In\u00a0[\u00a0]: Copied! <pre>file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\"\n</pre> file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\" In\u00a0[\u00a0]: Copied! <pre>df_master = pd.read_parquet(file_url)\n</pre> df_master = pd.read_parquet(file_url) In\u00a0[\u00a0]: Copied! <pre>df_master.head()\n</pre> df_master.head() In\u00a0[\u00a0]: Copied! <pre>df_master.shape\n</pre> df_master.shape <p>The goal of supervised learning is to build a model that performs well on new data. If you have new data, it\u2019s a good idea to see how your model performs on it. The problem is that you may not have new data, but you can simulate this experience by splitting your dataset between train and test.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>(\n    df_features_and_target_train,\n    df_features_and_target_test,\n) = model_selection.train_test_split(\n    df_master, \n    *** FILL THE MISSING LINE ***\n)\n</pre> (     df_features_and_target_train,     df_features_and_target_test, ) = model_selection.train_test_split(     df_master,      *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>def split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n    N=None,\n):\n    if N is None :\n        X_train = df_features_and_target_train[features]\n        y_train = df_features_and_target_train[target]\n    else :\n        X_train = df_features_and_target_train[features].sample(N)\n        y_train = df_features_and_target_train[target].loc[X_train.index]\n    X_test = df_features_and_target_test[features]\n    y_test = df_features_and_target_test[target]\n    return X_train, y_train, X_test, y_test\n\n\ntarget = [\n    *** FILL THE MISSING LINE ***\n] \nfeatures = [\n    *** FILL THE MISSING LINE ***\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n)\n</pre> def split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target,     N=None, ):     if N is None :         X_train = df_features_and_target_train[features]         y_train = df_features_and_target_train[target]     else :         X_train = df_features_and_target_train[features].sample(N)         y_train = df_features_and_target_train[target].loc[X_train.index]     X_test = df_features_and_target_test[features]     y_test = df_features_and_target_test[target]     return X_train, y_train, X_test, y_test   target = [     *** FILL THE MISSING LINE *** ]  features = [     *** FILL THE MISSING LINE ***     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>pipe = pipeline.make_pipeline(\n    linear_model.LinearRegression(),\n)\n\npipe.fit(\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n)\n</pre> pipe = pipeline.make_pipeline(     linear_model.LinearRegression(), )  pipe.fit(     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** ) <p>To assess the performance of our model, many metrics can be used. The easiest one for regression is $R^2$. It is used to measure the goodness of fit or best-fit line.</p> <p>$$R^2 = 1 - \\frac{\\sum_{i}{(y_i - \\hat{y}_i)^2}}{\\sum_{i}{(y_i - \\bar{y}_i)^2}}$$</p> In\u00a0[\u00a0]: Copied! <pre>score_train = pipe.score(\n    X_train, \n    y_train\n)\nscore_test = pipe.score(\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n)\n\nprint(f\"R2 (train): {score_train}\")\nprint(f\"R2 (test): {score_test}\")\n</pre> score_train = pipe.score(     X_train,      y_train ) score_test = pipe.score(     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** )  print(f\"R2 (train): {score_train}\") print(f\"R2 (test): {score_test}\") In\u00a0[\u00a0]: Copied! <pre>def compute_df_residual(pipe, X, y):\n    return pd.DataFrame(\n        {\n            \"y_true\": y.rating.reset_index(drop=True),\n            \"y_pred\": pipe.predict(X).reshape(-1),\n        }\n    ).assign(residual=lambda df: df.y_true - df.y_pred)\n\n\ndef plot_residual(df_residual):\n    df_residual.plot.hist(bins=200, subplots=True, layout=(1,3), figsize=(20, 5))\n    plt.show()\n    sns.violinplot(data=df_residual, x=\"y_true\", y=\"y_pred\")\n\n\ndf_residual_train = compute_df_residual(pipe, X_train, y_train)\ndf_residual_test = compute_df_residual(pipe, X_test, y_test)\nplot_residual(df_residual_train)\nplot_residual(df_residual_test)\n</pre> def compute_df_residual(pipe, X, y):     return pd.DataFrame(         {             \"y_true\": y.rating.reset_index(drop=True),             \"y_pred\": pipe.predict(X).reshape(-1),         }     ).assign(residual=lambda df: df.y_true - df.y_pred)   def plot_residual(df_residual):     df_residual.plot.hist(bins=200, subplots=True, layout=(1,3), figsize=(20, 5))     plt.show()     sns.violinplot(data=df_residual, x=\"y_true\", y=\"y_pred\")   df_residual_train = compute_df_residual(pipe, X_train, y_train) df_residual_test = compute_df_residual(pipe, X_test, y_test) plot_residual(df_residual_train) plot_residual(df_residual_test) In\u00a0[\u00a0]: Copied! <pre>def compute_score(pipe, X, y, metric_names=None, label=None, verbose=False):\n    if metric_names is None:\n        metric_names = [\"neg_root_mean_squared_error\"]\n    scores = dict()\n    for metric_name in metric_names:\n        metric = metrics.get_scorer(metric_name)\n        score = metric(pipe, X, y)\n        printed_label = f\" ({label})\" if label else \"\"\n        if verbose:\n            print(f\"{metric_name}{printed_label}: {score:.3g}\")\n        scores.update({f\"{metric_name}{printed_label}\": score})\n    return scores\n\n\nmetric_names = [\n    \"neg_root_mean_squared_error\",\n    \"neg_mean_absolute_error\",\n    \"neg_mean_squared_error\",\n]\nscore_train = compute_score(\n    pipe, X_train, y_train, metric_names, label=\"train\", verbose=True\n)\nscore_test = compute_score(\n    *** FILL THE MISSING LINE ***\n)\n</pre> def compute_score(pipe, X, y, metric_names=None, label=None, verbose=False):     if metric_names is None:         metric_names = [\"neg_root_mean_squared_error\"]     scores = dict()     for metric_name in metric_names:         metric = metrics.get_scorer(metric_name)         score = metric(pipe, X, y)         printed_label = f\" ({label})\" if label else \"\"         if verbose:             print(f\"{metric_name}{printed_label}: {score:.3g}\")         scores.update({f\"{metric_name}{printed_label}\": score})     return scores   metric_names = [     \"neg_root_mean_squared_error\",     \"neg_mean_absolute_error\",     \"neg_mean_squared_error\", ] score_train = compute_score(     pipe, X_train, y_train, metric_names, label=\"train\", verbose=True ) score_test = compute_score(     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>def get_feature_names(pipe, X):\n    return (\n        X.columns\n        if len(pipe) == 1\n        else pipe[:-1].get_feature_names_out(X.columns)\n    )\n\n\ndef plot_features_coefficients(pipe, X):\n    df_coef = pd.DataFrame(\n        {\"coef\": pipe[-1].coef_.reshape(-1)}, index=get_feature_names(pipe, X)\n    ).assign(color=lambda df: (df.coef &gt;= 0).map({True: \"g\", False: \"r\"}))\n    df_coef.coef.plot(\n        kind=\"barh\", color=df_coef.color, figsize=(6, len(df_coef) * 0.25)\n    )\n\n\nplot_features_coefficients(pipe, X_train)\n</pre> def get_feature_names(pipe, X):     return (         X.columns         if len(pipe) == 1         else pipe[:-1].get_feature_names_out(X.columns)     )   def plot_features_coefficients(pipe, X):     df_coef = pd.DataFrame(         {\"coef\": pipe[-1].coef_.reshape(-1)}, index=get_feature_names(pipe, X)     ).assign(color=lambda df: (df.coef &gt;= 0).map({True: \"g\", False: \"r\"}))     df_coef.coef.plot(         kind=\"barh\", color=df_coef.color, figsize=(6, len(df_coef) * 0.25)     )   plot_features_coefficients(pipe, X_train) <p>The coefficients of the linear regression can be misleading ! They do not represent the feature importance if the explicative variables are not comparable.</p> In\u00a0[\u00a0]: Copied! <pre>df_master[\"alcohol\"].plot(kind=\"hist\", bins=100)\n</pre> df_master[\"alcohol\"].plot(kind=\"hist\", bins=100) <p>We can see that we have few lines with alcohol = -1. Let's remove those lines</p> In\u00a0[\u00a0]: Copied! <pre>df_clean = (\n    df_master\n    *** FILL THE MISSING LINE ***\n)\n\ndf_clean[\"alcohol\"].plot(kind=\"hist\", bins=100)\n</pre> df_clean = (     df_master     *** FILL THE MISSING LINE *** )  df_clean[\"alcohol\"].plot(kind=\"hist\", bins=100) In\u00a0[\u00a0]: Copied! <pre>(\n    df_features_and_target_train,\n    df_features_and_target_test,\n) = model_selection.train_test_split(df_clean, test_size=0.1)\n</pre> (     df_features_and_target_train,     df_features_and_target_test, ) = model_selection.train_test_split(df_clean, test_size=0.1) In\u00a0[\u00a0]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"alcohol\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\npipe = pipeline.make_pipeline(\n    *** FILL THE MISSING LINE ***\n)\n\npipe.fit(X_train, y_train)\n</pre> target = [\"rating\"] features = [     \"alcohol\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  pipe = pipeline.make_pipeline(     *** FILL THE MISSING LINE *** )  pipe.fit(X_train, y_train) In\u00a0[\u00a0]: Copied! <pre>plot_features_coefficients(pipe, X_train)\n</pre> plot_features_coefficients(pipe, X_train) <p>Let's have a look at the impact on the MAE</p> In\u00a0[\u00a0]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) <p>How to use categorical variables ?</p> type rating Stout 14 Belgian Ale 11 IPA 13 <p>We have to encode them to numerical values. Multiple approaches exist for that.</p> <p>Label encoding</p> type type_encoded rating Stout 1 14 Belgian Ale 2 11 IPA 3 13 <p>Label encoding is a simple way to encode categorical variables. However, it creates a hierarchy/order between the categories, which does not always reflect the reality.</p> <p>One hot encoding</p> type type_Stout type_Belgian_Ale type_IPA rating Stout 1 0 0 14 Belgian Ale 0 1 0 11 IPA 0 0 1 13 <p>Here, there is no hierarchy, but in the case of a lot of categories it will add many columns to the dataset.</p> <p>Let's try to one hot encode some categorical variables ! For that, we will create a custom categorical variable from alcohol.</p> In\u00a0[\u00a0]: Copied! <pre>df_clean.head(3)\n</pre> df_clean.head(3) In\u00a0[\u00a0]: Copied! <pre>def alcohol_level (row):\n    if row['alcohol'] &lt; 5.5 :\n        return 'Light'\n    if row['alcohol'] &gt; 7 :\n        return 'Strong'\n    else :\n        return 'Medium'\n\ndf_clean['alcohol_level'] = df_clean.apply(lambda row: alcohol_level(row), axis=1)\n\ndf_clean['alcohol_level'].value_counts()\n</pre> def alcohol_level (row):     if row['alcohol'] &lt; 5.5 :         return 'Light'     if row['alcohol'] &gt; 7 :         return 'Strong'     else :         return 'Medium'  df_clean['alcohol_level'] = df_clean.apply(lambda row: alcohol_level(row), axis=1)  df_clean['alcohol_level'].value_counts() In\u00a0[\u00a0]: Copied! <pre>df_clean.head()\n</pre> df_clean.head() In\u00a0[\u00a0]: Copied! <pre>(\n    df_features_and_target_train,\n    df_features_and_target_test,\n) = model_selection.train_test_split(df_clean, test_size=0.1, random_state=1)\n\ntarget = [\"rating\"]\nfeatures = [\n    \"alcohol_level\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\nX_train = (\n    *** FILL THE MISSING LINE ***\n)\nX_test = (\n    *** FILL THE MISSING LINE ***\n)\n</pre> (     df_features_and_target_train,     df_features_and_target_test, ) = model_selection.train_test_split(df_clean, test_size=0.1, random_state=1)  target = [\"rating\"] features = [     \"alcohol_level\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  X_train = (     *** FILL THE MISSING LINE *** ) X_test = (     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>pipe = pipeline.make_pipeline(\n    linear_model.LinearRegression(),\n)\n\npipe.fit(X_train, y_train)\n\nplot_features_coefficients(pipe, X_train)\n</pre> pipe = pipeline.make_pipeline(     linear_model.LinearRegression(), )  pipe.fit(X_train, y_train)  plot_features_coefficients(pipe, X_train) In\u00a0[\u00a0]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) In\u00a0[\u00a0]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"alcohol_level\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\nX_train = pd.get_dummies(X_train, columns=[\"alcohol_level\"])\nX_test = pd.get_dummies(X_test, columns=[\"alcohol_level\"])\n\npipe = pipeline.make_pipeline(\n    compose.ColumnTransformer([\n        ('scaler', preprocessing.StandardScaler(), ['rating_appearance', 'rating_aroma', 'rating_palate', 'rating_taste']),\n        ('passthrough', \"passthrough\", [\"alcohol_level_Strong\", \"alcohol_level_Medium\", \"alcohol_level_Light\"])\n    ], remainder='passthrough'),\n    linear_model.LinearRegression(),\n)\n\npipe.fit(X_train, y_train)\n</pre> target = [\"rating\"] features = [     \"alcohol_level\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  X_train = pd.get_dummies(X_train, columns=[\"alcohol_level\"]) X_test = pd.get_dummies(X_test, columns=[\"alcohol_level\"])  pipe = pipeline.make_pipeline(     compose.ColumnTransformer([         ('scaler', preprocessing.StandardScaler(), ['rating_appearance', 'rating_aroma', 'rating_palate', 'rating_taste']),         ('passthrough', \"passthrough\", [\"alcohol_level_Strong\", \"alcohol_level_Medium\", \"alcohol_level_Light\"])     ], remainder='passthrough'),     linear_model.LinearRegression(), )  pipe.fit(X_train, y_train) <p>Check the features created &amp; the transformation applied to them, using histograms.</p> In\u00a0[\u00a0]: Copied! <pre>def plot_features_transformed_histograms(pipe, X, **kwargs):\n    if len(pipe) &gt; 1:\n        columns = pipe[:-1].get_feature_names_out(X.columns)\n        (\n            pd.DataFrame(\n                pipe[:-1].transform(X),\n                columns=columns,\n            ).plot.hist(subplots=True, layout=(2, 4), bins=200, figsize=(20, 10))\n        )\n    else:\n        print(\"no plot: features not transformed\")\n\n\nplot_features_transformed_histograms(pipe, X_train)\n</pre> def plot_features_transformed_histograms(pipe, X, **kwargs):     if len(pipe) &gt; 1:         columns = pipe[:-1].get_feature_names_out(X.columns)         (             pd.DataFrame(                 pipe[:-1].transform(X),                 columns=columns,             ).plot.hist(subplots=True, layout=(2, 4), bins=200, figsize=(20, 10))         )     else:         print(\"no plot: features not transformed\")   plot_features_transformed_histograms(pipe, X_train) <p>And now we can check again the linear regression coefficients. They can now be seen as feature importance.</p> In\u00a0[\u00a0]: Copied! <pre>plot_features_coefficients(pipe, X_train)\n</pre> plot_features_coefficients(pipe, X_train) In\u00a0[\u00a0]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) <p>We have also seen that some features do not have a gaussian distribution. However one of the assumption made by the linear regression is that all the features follow a gaussian distribution. We can use some transformers to make the features more normal.</p> In\u00a0[\u00a0]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"alcohol\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\npipe = pipeline.make_pipeline(\n    *** FILL THE MISSING LINE ***\n    linear_model.LinearRegression(),\n)\n\npipe.fit(X_train, y_train)\n</pre> target = [\"rating\"] features = [     \"alcohol\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  pipe = pipeline.make_pipeline(     *** FILL THE MISSING LINE ***     linear_model.LinearRegression(), )  pipe.fit(X_train, y_train) In\u00a0[\u00a0]: Copied! <pre>def plot_features_transformed_histograms(pipe, X, **kwargs):\n    if len(pipe) &gt; 1:\n        columns = pipe[:-1].get_feature_names_out(X.columns)\n        (\n            pd.DataFrame(\n                pipe[:-1].transform(X),\n                columns=columns,\n            ).plot.hist(subplots=True, layout=(2, 4), bins=200, figsize=(23, 10))\n        )\n    else:\n        print(\"no plot: features not transformed\")\n\nplot_features_transformed_histograms(pipe, X_train)\n</pre> def plot_features_transformed_histograms(pipe, X, **kwargs):     if len(pipe) &gt; 1:         columns = pipe[:-1].get_feature_names_out(X.columns)         (             pd.DataFrame(                 pipe[:-1].transform(X),                 columns=columns,             ).plot.hist(subplots=True, layout=(2, 4), bins=200, figsize=(23, 10))         )     else:         print(\"no plot: features not transformed\")  plot_features_transformed_histograms(pipe, X_train) In\u00a0[\u00a0]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) In\u00a0[\u00a0]: Copied! <pre>plot_features_coefficients(pipe, X_train)\n</pre> plot_features_coefficients(pipe, X_train) <p>Another possible feature engineering step is to add some non linearity. As the linear regression is a linear model, the non linearity can be managed thanks to the use of polynomial features.</p> In\u00a0[\u00a0]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"alcohol\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\npipe = pipeline.make_pipeline(\n    *** FILL THE MISSING LINE ***\n    preprocessing.StandardScaler(),\n    linear_model.LinearRegression(),\n)\n\npipe.fit(X_train, y_train)\n</pre> target = [\"rating\"] features = [     \"alcohol\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  pipe = pipeline.make_pipeline(     *** FILL THE MISSING LINE ***     preprocessing.StandardScaler(),     linear_model.LinearRegression(), )  pipe.fit(X_train, y_train) In\u00a0[\u00a0]: Copied! <pre>plot_features_coefficients(pipe, X_train)\n</pre> plot_features_coefficients(pipe, X_train) In\u00a0[\u00a0]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) In\u00a0[\u00a0]: Copied! <pre>def tokenize(serie):\n    return (\n        (serie)\n        *** FILL THE MISSING LINE ***\n        .str.replace(r\"[^a-z]\", \" \")\n        .str.replace(r\" +\", \" \")\n        .str.split(\" \")\n    )\n\n\ndf_words_count = (\n    (df_clean)\n    .assign(tokenized_text=lambda df: tokenize(df.text))\n    .loc[:, [\"rating\", \"tokenized_text\"]]\n    .explode(\"tokenized_text\")\n    .groupby(\"tokenized_text\", as_index=False)\n    .agg([\"mean\", \"count\"])\n    .reset_index()\n    .sort_values(by=(\"rating\", \"count\"), ascending=False)\n    .loc[lambda df: ~df.tokenized_text.isin(list(STOPWORDS))]\n    .loc[lambda df: df.tokenized_text.str.len() &gt; 1]\n    *** FILL THE MISSING LINE ***\n)\n</pre> def tokenize(serie):     return (         (serie)         *** FILL THE MISSING LINE ***         .str.replace(r\"[^a-z]\", \" \")         .str.replace(r\" +\", \" \")         .str.split(\" \")     )   df_words_count = (     (df_clean)     .assign(tokenized_text=lambda df: tokenize(df.text))     .loc[:, [\"rating\", \"tokenized_text\"]]     .explode(\"tokenized_text\")     .groupby(\"tokenized_text\", as_index=False)     .agg([\"mean\", \"count\"])     .reset_index()     .sort_values(by=(\"rating\", \"count\"), ascending=False)     .loc[lambda df: ~df.tokenized_text.isin(list(STOPWORDS))]     .loc[lambda df: df.tokenized_text.str.len() &gt; 1]     *** FILL THE MISSING LINE *** ) In\u00a0[\u00a0]: Copied! <pre>df_words_count.sort_values(by=[('rating',  'mean')])\n</pre> df_words_count.sort_values(by=[('rating',  'mean')]) In\u00a0[\u00a0]: Copied! <pre>word_mean_review = (\n    df_words_count.set_index(\"tokenized_text\").rating[\"mean\"].to_dict()\n)\n</pre> word_mean_review = (     df_words_count.set_index(\"tokenized_text\").rating[\"mean\"].to_dict() ) In\u00a0[\u00a0]: Copied! <pre>positive_words_set = set(\n    df_words_count.loc[lambda df: df.rating[\"mean\"] &gt;= 14.5]\n    .loc[:, \"tokenized_text\"]\n    .tolist()\n)\nnegative_words_set = set(\n    *** FILL THE MISSING LINE ***\n    .loc[:, \"tokenized_text\"]\n    .tolist()\n)\n</pre> positive_words_set = set(     df_words_count.loc[lambda df: df.rating[\"mean\"] &gt;= 14.5]     .loc[:, \"tokenized_text\"]     .tolist() ) negative_words_set = set(     *** FILL THE MISSING LINE ***     .loc[:, \"tokenized_text\"]     .tolist() ) In\u00a0[\u00a0]: Copied! <pre>print(len(negative_words_set))\nprint(len(positive_words_set))\n</pre> print(len(negative_words_set)) print(len(positive_words_set)) In\u00a0[\u00a0]: Copied! <pre>def compute_mean_word_rating(words, word_mean_review):\n    return np.mean(\n        [word_mean_review[w] for w in words if w in word_mean_review] or [0]\n    )\n\n\ndef count_words_in_set(words, word_set):\n    return len(set(words) &amp; word_set)\n\n\ndf_features_and_target = (\n    (df_clean)\n    .assign(tokenized_text=lambda df: tokenize(df.text))\n    .assign(\n        mean_word_rating=lambda df: (df.tokenized_text).apply(\n            compute_mean_word_rating, args=(word_mean_review,)\n        )\n    )\n    .assign(\n        positive_words_count=lambda df: (df.tokenized_text).apply(\n            count_words_in_set, args=(positive_words_set,)\n        )\n    )\n    .assign(\n        negative_words_count=lambda df: (df.tokenized_text).apply(\n            count_words_in_set, args=(negative_words_set,)\n        )\n    )\n)\n</pre> def compute_mean_word_rating(words, word_mean_review):     return np.mean(         [word_mean_review[w] for w in words if w in word_mean_review] or [0]     )   def count_words_in_set(words, word_set):     return len(set(words) &amp; word_set)   df_features_and_target = (     (df_clean)     .assign(tokenized_text=lambda df: tokenize(df.text))     .assign(         mean_word_rating=lambda df: (df.tokenized_text).apply(             compute_mean_word_rating, args=(word_mean_review,)         )     )     .assign(         positive_words_count=lambda df: (df.tokenized_text).apply(             count_words_in_set, args=(positive_words_set,)         )     )     .assign(         negative_words_count=lambda df: (df.tokenized_text).apply(             count_words_in_set, args=(negative_words_set,)         )     ) ) In\u00a0[\u00a0]: Copied! <pre>df_features_and_target.head(5)\n</pre> df_features_and_target.head(5) In\u00a0[\u00a0]: Copied! <pre>(\n    df_features_and_target_train,\n    df_features_and_target_test,\n) = model_selection.train_test_split(df_features_and_target, test_size=0.1, random_state=1)\n</pre> (     df_features_and_target_train,     df_features_and_target_test, ) = model_selection.train_test_split(df_features_and_target, test_size=0.1, random_state=1) In\u00a0[\u00a0]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\",\n    \"alcohol\",\n    \"user_degree\",\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\npipe = pipeline.make_pipeline(\n    preprocessing.StandardScaler(),\n    linear_model.LinearRegression(),\n)\n\npipe.fit(X_train, y_train)\n</pre> target = [\"rating\"] features = [     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\",     \"alcohol\",     \"user_degree\",     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  pipe = pipeline.make_pipeline(     preprocessing.StandardScaler(),     linear_model.LinearRegression(), )  pipe.fit(X_train, y_train) In\u00a0[\u00a0]: Copied! <pre>plot_features_coefficients(pipe, X_train)\n</pre> plot_features_coefficients(pipe, X_train) In\u00a0[\u00a0]: Copied! <pre>metric = metrics.get_scorer(\"neg_mean_absolute_error\")\nscore = metric(pipe, X_train, y_train)\nprint(\"MAE train : \" + str(round(score, 2)))\nscore = metric(pipe, X_test, y_test)\nprint(\"MAE test : \" + str(round(score, 2)))\n</pre> metric = metrics.get_scorer(\"neg_mean_absolute_error\") score = metric(pipe, X_train, y_train) print(\"MAE train : \" + str(round(score, 2))) score = metric(pipe, X_test, y_test) print(\"MAE test : \" + str(round(score, 2))) In\u00a0[\u00a0]: Copied! <pre>target = [\"rating\"]\nfeatures = [\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\",\n    \"alcohol\",\n    \"user_degree\",\n    \"positive_words_count\",\n    \"negative_words_count\",\n    \"mean_word_rating\"\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n)\n\nmodel = linear_model.LinearRegression()\ntrain_sizes_percent = np.geomspace(0.001, 1, 20)\n(\n    train_sizes,\n    train_scores,\n    validation_scores,\n    _,\n    _,\n) = model_selection.learning_curve(\n    model,\n    X_train,\n    y_train,\n    cv=7,\n    n_jobs=-1,\n    train_sizes=train_sizes_percent,\n    return_times=True,\n    verbose=1,\n)\n</pre> target = [\"rating\"] features = [     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\",     \"alcohol\",     \"user_degree\",     \"positive_words_count\",     \"negative_words_count\",     \"mean_word_rating\" ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target, )  model = linear_model.LinearRegression() train_sizes_percent = np.geomspace(0.001, 1, 20) (     train_sizes,     train_scores,     validation_scores,     _,     _, ) = model_selection.learning_curve(     model,     X_train,     y_train,     cv=7,     n_jobs=-1,     train_sizes=train_sizes_percent,     return_times=True,     verbose=1, ) <p>Plot the training &amp; validation scores.</p> <p>Since they come from a K-fold cross-validation, you have K values for each training set size: use seaborn's <code>lineplot</code> to plot the mean value with a confidence interval.</p> In\u00a0[\u00a0]: Copied! <pre>def compute_df_sizes(train_sizes):\n    return pd.DataFrame(\n        {\n            \"train_size_id\": range(len(train_sizes)),\n            \"train_size\": train_sizes,\n        }\n    )\n\n\ndef compute_df_score(scores, name):\n    df = (\n        pd.DataFrame(scores)\n        .stack()\n        .rename_axis(index=(\"train_size_id\", \"run_id\"))\n        .rename(name)\n        .reset_index()\n    )\n    return df\n\n\ndef compute_df_scores_long(train_scores, validation_scores, train_sizes):\n    return (\n        compute_df_score(train_scores, name=\"train_score\")\n        .merge(compute_df_score(validation_scores, name=\"validation_score\"))\n        .merge(compute_df_sizes(train_sizes), on=\"train_size_id\")\n        .melt(\n            id_vars=[\"train_size_id\", \"run_id\", \"train_size\"],\n            value_vars=[\"train_score\", \"validation_score\"],\n            var_name=\"score_name\",\n            value_name=\"score\",\n        )\n    )\n\n\ndf_scores_long = compute_df_scores_long(\n    train_scores, validation_scores, train_sizes\n)\nsns.lineplot(\n    data=df_scores_long.loc[lambda df: df.score &gt; 0.5],\n    x=\"train_size\",\n    y=\"score\",\n    hue=\"score_name\",\n    style=\"score_name\",\n    dashes=False,\n    markers=True,\n).set(xscale=\"log\")\n</pre> def compute_df_sizes(train_sizes):     return pd.DataFrame(         {             \"train_size_id\": range(len(train_sizes)),             \"train_size\": train_sizes,         }     )   def compute_df_score(scores, name):     df = (         pd.DataFrame(scores)         .stack()         .rename_axis(index=(\"train_size_id\", \"run_id\"))         .rename(name)         .reset_index()     )     return df   def compute_df_scores_long(train_scores, validation_scores, train_sizes):     return (         compute_df_score(train_scores, name=\"train_score\")         .merge(compute_df_score(validation_scores, name=\"validation_score\"))         .merge(compute_df_sizes(train_sizes), on=\"train_size_id\")         .melt(             id_vars=[\"train_size_id\", \"run_id\", \"train_size\"],             value_vars=[\"train_score\", \"validation_score\"],             var_name=\"score_name\",             value_name=\"score\",         )     )   df_scores_long = compute_df_scores_long(     train_scores, validation_scores, train_sizes ) sns.lineplot(     data=df_scores_long.loc[lambda df: df.score &gt; 0.5],     x=\"train_size\",     y=\"score\",     hue=\"score_name\",     style=\"score_name\",     dashes=False,     markers=True, ).set(xscale=\"log\") <p>Plot the features importance of the linear model trained on the full dataset</p> In\u00a0[\u00a0]: Copied! <pre>pipe = pipeline.make_pipeline(model)\npipe.fit(X_train, y_train)\nplot_features_coefficients(pipe, X_train)\n</pre> pipe = pipeline.make_pipeline(model) pipe.fit(X_train, y_train) plot_features_coefficients(pipe, X_train) In\u00a0[\u00a0]: Copied! <pre>features = [\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\",\n    \"alcohol\",\n    \"user_degree\",\n    \"positive_words_count\",\n    \"negative_words_count\",\n    \"mean_word_rating\"\n]\n\npipe = pipeline.make_pipeline(\n    preprocessing.PolynomialFeatures(include_bias=False, degree=2),\n    preprocessing.PowerTransformer(),\n    linear_model.LinearRegression(),\n)\n(\n    train_sizes,\n    train_scores,\n    validation_scores,\n    fit_times,\n    _,\n) = model_selection.learning_curve(\n    pipe,\n    X_train,\n    y_train,\n    cv=5,\n    n_jobs=-1,\n    train_sizes=train_sizes_percent,\n    return_times=True,\n    verbose=1,\n)\n\ndf_scores_long = compute_df_scores_long(\n    train_scores, validation_scores, train_sizes\n)\nsns.lineplot(\n    data=df_scores_long.loc[lambda df: df.score &gt; 0.5],\n    x=\"train_size\",\n    y=\"score\",\n    hue=\"score_name\",\n    style=\"score_name\",\n    dashes=False,\n    markers=True,\n).set(xscale=\"log\")\n</pre> features = [     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\",     \"alcohol\",     \"user_degree\",     \"positive_words_count\",     \"negative_words_count\",     \"mean_word_rating\" ]  pipe = pipeline.make_pipeline(     preprocessing.PolynomialFeatures(include_bias=False, degree=2),     preprocessing.PowerTransformer(),     linear_model.LinearRegression(), ) (     train_sizes,     train_scores,     validation_scores,     fit_times,     _, ) = model_selection.learning_curve(     pipe,     X_train,     y_train,     cv=5,     n_jobs=-1,     train_sizes=train_sizes_percent,     return_times=True,     verbose=1, )  df_scores_long = compute_df_scores_long(     train_scores, validation_scores, train_sizes ) sns.lineplot(     data=df_scores_long.loc[lambda df: df.score &gt; 0.5],     x=\"train_size\",     y=\"score\",     hue=\"score_name\",     style=\"score_name\",     dashes=False,     markers=True, ).set(xscale=\"log\") <p>Plot the features importance of the linear model with polynomial features trained on the full dataset with 6 features</p> In\u00a0[\u00a0]: Copied! <pre>pipe.fit(X_train, y_train)\nplot_features_coefficients(pipe, X_train)\n</pre> pipe.fit(X_train, y_train) plot_features_coefficients(pipe, X_train) <p>From here we'll use a dataset containing 2000 rows &amp; 7 features</p> In\u00a0[\u00a0]: Copied! <pre>N = 2000\nfeatures = [\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\",\n    \"mean_word_rating\",\n    \"positive_words_count\",\n    \"negative_words_count\",\n]\nX_train, y_train, X_test, y_test = split_features_and_target(\n    df_features_and_target_train,\n    df_features_and_target_test,\n    features,\n    target,\n    N,\n)\n</pre> N = 2000 features = [     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\",     \"mean_word_rating\",     \"positive_words_count\",     \"negative_words_count\", ] X_train, y_train, X_test, y_test = split_features_and_target(     df_features_and_target_train,     df_features_and_target_test,     features,     target,     N, ) In\u00a0[\u00a0]: Copied! <pre>alphas = np.logspace(-6, 3, 28)\n\ndef compute_scores_and_coefs(pipe, alphas):\n    scores = dict()\n    coefs = dict()\n    for alpha in tqdm(alphas):\n        pipe[-1].set_params(alpha=alpha)\n        pipe.fit(X_train, y_train)\n        score_train = compute_score(pipe, X_train, y_train, metric_names = [\"neg_mean_absolute_error\"], label=\"train\")\n        score_test = compute_score(pipe, X_test, y_test, metric_names = [\"neg_mean_absolute_error\"], label=\"test\")\n        scores.update({alpha: {**score_train, **score_test}})\n        coef = dict(\n            zip(get_feature_names(pipe, X_train), pipe[-1].coef_.reshape(-1))\n        )\n        coefs.update({alpha: coef})\n    return scores, coefs\n\n\npipe = pipeline.make_pipeline(\n    preprocessing.PolynomialFeatures(include_bias=False, degree=2),\n    preprocessing.PowerTransformer(),\n    linear_model.Ridge(),\n)\nscores, coefs = compute_scores_and_coefs(pipe, alphas)\n</pre> alphas = np.logspace(-6, 3, 28)  def compute_scores_and_coefs(pipe, alphas):     scores = dict()     coefs = dict()     for alpha in tqdm(alphas):         pipe[-1].set_params(alpha=alpha)         pipe.fit(X_train, y_train)         score_train = compute_score(pipe, X_train, y_train, metric_names = [\"neg_mean_absolute_error\"], label=\"train\")         score_test = compute_score(pipe, X_test, y_test, metric_names = [\"neg_mean_absolute_error\"], label=\"test\")         scores.update({alpha: {**score_train, **score_test}})         coef = dict(             zip(get_feature_names(pipe, X_train), pipe[-1].coef_.reshape(-1))         )         coefs.update({alpha: coef})     return scores, coefs   pipe = pipeline.make_pipeline(     preprocessing.PolynomialFeatures(include_bias=False, degree=2),     preprocessing.PowerTransformer(),     linear_model.Ridge(), ) scores, coefs = compute_scores_and_coefs(pipe, alphas) <p>Plot the evolution of the score as a function of <code>alpha</code></p> In\u00a0[\u00a0]: Copied! <pre>(\n    pd.DataFrame.from_dict(scores, orient=\"index\")\n    .plot(logx=True, marker=\".\", ylim=(-1.2, -1.05))\n)\n</pre> (     pd.DataFrame.from_dict(scores, orient=\"index\")     .plot(logx=True, marker=\".\", ylim=(-1.2, -1.05)) ) <p>Plot the evolution of the coefficients as a function of <code>alpha</code></p> In\u00a0[\u00a0]: Copied! <pre>(\n    pd.DataFrame.from_dict(coefs, orient=\"index\").plot(\n        logx=True, legend=False, marker=\".\"\n    )\n)\n</pre> (     pd.DataFrame.from_dict(coefs, orient=\"index\").plot(         logx=True, legend=False, marker=\".\"     ) ) <p>Plot the features importance of a model trained with a specific <code>alpha</code> value (e.g: <code>alpha=1e-1</code>)</p> In\u00a0[\u00a0]: Copied! <pre>pipe[-1].set_params(\n    *** FILL THE MISSING LINE ***\n)\npipe.fit(X_train, y_train)\nplot_features_coefficients(pipe, X_train)\n</pre> pipe[-1].set_params(     *** FILL THE MISSING LINE *** ) pipe.fit(X_train, y_train) plot_features_coefficients(pipe, X_train) In\u00a0[\u00a0]: Copied! <pre>pipe = pipeline.make_pipeline(\n    preprocessing.PolynomialFeatures(include_bias=False, degree=2),\n    preprocessing.PowerTransformer(),\n    linear_model.Lasso(),\n)\nscores, coefs = compute_scores_and_coefs(pipe, alphas)\n</pre> pipe = pipeline.make_pipeline(     preprocessing.PolynomialFeatures(include_bias=False, degree=2),     preprocessing.PowerTransformer(),     linear_model.Lasso(), ) scores, coefs = compute_scores_and_coefs(pipe, alphas) <p>Plot the evolution of the score as a function of <code>alpha</code></p> In\u00a0[\u00a0]: Copied! <pre>(\n    pd.DataFrame.from_dict(scores, orient=\"index\")\n    .plot(logx=True, marker=\".\", ylim=(-1.2, -1.05))\n)\n</pre> (     pd.DataFrame.from_dict(scores, orient=\"index\")     .plot(logx=True, marker=\".\", ylim=(-1.2, -1.05)) ) <p>Plot the evolution of the coefficients as a function of <code>alpha</code></p> In\u00a0[\u00a0]: Copied! <pre>(\n    pd.DataFrame.from_dict(coefs, orient=\"index\").plot(\n        logx=True, legend=False, marker=\".\"\n    )\n)\n</pre> (     pd.DataFrame.from_dict(coefs, orient=\"index\").plot(         logx=True, legend=False, marker=\".\"     ) ) <p>Plot the features importance of a model trained with a specific <code>alpha</code> value (e.g: <code>alpha=1e-2</code>)</p> In\u00a0[\u00a0]: Copied! <pre>pipe[-1].set_params(\n    *** FILL THE MISSING LINE ***\n)\npipe.fit(X_train, y_train)\nplot_features_coefficients(pipe, X_train)\nscore_train = pipe.score(X_train, y_train)\nscore_test = pipe.score(X_test, y_test)\nprint(score_train)\nprint(score_test)\n</pre> pipe[-1].set_params(     *** FILL THE MISSING LINE *** ) pipe.fit(X_train, y_train) plot_features_coefficients(pipe, X_train) score_train = pipe.score(X_train, y_train) score_test = pipe.score(X_test, y_test) print(score_train) print(score_test)"},{"location":"10-Tutorials/3_regression_students/#practice-n3-regression-students-version","title":"\ud83d\udc0d Practice n\u00b03: regression (students version)\u00b6","text":"<p>The objective of this session is to understand the regression problems and the analytical approach to solve them. This notebook discusses theoretical notions and contains practical parts.</p> <p>Here are the main steps of the notebook :</p> <ol> <li>What is regression ?</li> <li>Focus on linear regression</li> <li>Preparation</li> <li>Implementation of a linear regression</li> <li>Model improvement</li> </ol>"},{"location":"10-Tutorials/3_regression_students/#1-what-is-regression","title":"1. What is regression ?\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#1d-example","title":"1D Example\u00b6","text":"<p>x = Number of aircrafts in an airspace sector</p> <p>y = Subjective rating of air traffic controller workload</p>"},{"location":"10-Tutorials/3_regression_students/#generalisation","title":"Generalisation\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#2-focus-on-linear-regression","title":"2. Focus on linear regression\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#1d-example","title":"1D Example\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#generalisation","title":"Generalisation\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#3-preparation","title":"3. Preparation\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#install-import-modules","title":"Install &amp; import modules\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#read-remote-dataset","title":"Read remote dataset\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#implementation-of-a-linear-regression","title":"Implementation of a linear regression\u00b6","text":"<p>In this first section, you will train a linear regression model to predict the overall rating of a review based on its other ratings. For that, you will :</p> <ul> <li>Load &amp; describe the data</li> <li>Split the data between train and test set</li> <li>Train and evaluate the linear regression model</li> </ul>"},{"location":"10-Tutorials/3_regression_students/#load-data-describe","title":"Load data &amp; describe\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#splits-traintest-featurestarget","title":"Splits: Train/test &amp; features/target\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#traintest-split","title":"Train/test split\u00b6","text":"<p>Split keeping 10% of the data in the test set.</p> <p>Hint: https://scikit-learn.org/stable/index.html</p>"},{"location":"10-Tutorials/3_regression_students/#featurestarget-split","title":"Features/target split\u00b6","text":"<p>Split keeping:</p> <ul> <li>only the wanted features</li> <li>N data samples in the training set</li> </ul>"},{"location":"10-Tutorials/3_regression_students/#model-training-evaluation","title":"Model training &amp; evaluation\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#model-training","title":"Model training\u00b6","text":"<p>Use a pipeline to wrap the model with its automatized preprocessing steps</p>"},{"location":"10-Tutorials/3_regression_students/#model-evaluation","title":"Model evaluation\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#r2-score","title":"R2 score\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#r2-score-in-practice","title":"R2 score in practice\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#residuals","title":"Residuals\u00b6","text":"<p>Compute residuals dataframe containing the true &amp; predicted values of <code>y</code> and and the residual, which is the difference between the true &amp; predicted values.</p> <p>Plot the residuals, using histograms &amp; violinplots.</p>"},{"location":"10-Tutorials/3_regression_students/#other-regression-metrics","title":"Other regression metrics\u00b6","text":"<p>Another popular metric is the Mean Squared Error (MSE)</p> <p>$$MSE = \\frac{1}{N} \\sum_{i}(y_i \u2013 \\hat{y}_i)^2$$</p> <p>This metric is interesting if we want to penalize large errors. However it is not easily interpretable as it is not in the same unit than the target.</p> <p>This is why the Root Mean Squared Error (RMSE) is widely used.</p> <p>$$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i}(y_i \u2013 \\hat{y}_i)^2}$$</p> <p>As well as the Mean Absolute Error which is even more interpretable.</p> <p>$$MAE = \\frac{1}{N} \\sum_{i}\\lvert(y_i \u2013 \\hat{y}_i)\\lvert$$</p>"},{"location":"10-Tutorials/3_regression_students/#other-metrics-in-practice","title":"Other metrics in practice\u00b6","text":"<p>Compute a set of metrics on train &amp; test datasets:</p> <ul> <li>negative RMSE</li> <li>negative MAE</li> <li>negative MSE</li> </ul> <p>NB: A \"score\" function in sklearn obeys to the \"greater is better\" principle. That's why error functions are converted to their negative versions to obey this principle.</p>"},{"location":"10-Tutorials/3_regression_students/#feature-importance","title":"Feature importance\u00b6","text":"<p>Plot model's coefficients in a bar chart</p>"},{"location":"10-Tutorials/3_regression_students/#model-improvement","title":"Model improvement\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#feature-engineering","title":"Feature engineering\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#outliers-management","title":"Outliers management\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#categorical-variable-encoding","title":"Categorical variable encoding\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#feature-scaling","title":"Feature scaling\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#feature-standardization","title":"Feature standardization\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#polynomial-features-optional","title":"Polynomial Features (optional)\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#review-feature-extraction","title":"Review feature extraction\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#word-counts-mean-review","title":"Word counts &amp; mean review\u00b6","text":"<p>Compute the \"word counts\" dataframe:</p> <ul> <li>containing:<ul> <li>count of words in review</li> <li>mean rating review of the reviews containing this text</li> </ul> </li> <li>remove stop words</li> <li>remove single letter words</li> <li>keep only the 1000 most frequent words</li> </ul>"},{"location":"10-Tutorials/3_regression_students/#word-mean-review","title":"Word mean review\u00b6","text":"<p>Compute the \"word ratings\" dictionary containing the mean review for each word</p>"},{"location":"10-Tutorials/3_regression_students/#positive-negative-words-sets","title":"Positive &amp; negative words sets\u00b6","text":"<ul> <li>Compute the \"positive words\" set containing the words with a rating &gt;= 14.5</li> <li>Compute the \"negative words\" set containing the words with a rating &lt;= 13</li> </ul>"},{"location":"10-Tutorials/3_regression_students/#features-target","title":"Features &amp; target\u00b6","text":"<p>Compute the \"features &amp; target\" dataframe containing:</p> <ul> <li>tokenized text</li> <li>mean word rating</li> <li>positive words count</li> <li>negative words count</li> <li>negative words negative count</li> </ul>"},{"location":"10-Tutorials/3_regression_students/#learning-curve","title":"Learning curve\u00b6","text":""},{"location":"10-Tutorials/3_regression_students/#simple-model","title":"Simple model\u00b6","text":"<p>Compute the training &amp; validation R2-scores for various training sets sizes</p>"},{"location":"10-Tutorials/3_regression_students/#complex-model-optional","title":"Complex model (optional)\u00b6","text":"<p>Compute the training &amp; validation R2-scores for various training sets sizes</p> <p>Plot the training &amp; validation scores.</p>"},{"location":"10-Tutorials/3_regression_students/#overfitting-underfitting","title":"Overfitting &amp; Underfitting\u00b6","text":"<p>Overfitting happens when a model fits quite well to the training data but does not generalize well on new data it has never seen. It can have multiple causes :</p> <ul> <li>The training dataset is not representative of the real data</li> <li>The model is too complex and learned from noise on the training dataset</li> </ul> <p>Underfitting happens when the model is too simple to capture the data patterns. in this case, the model would perform bad on the train and on the test dataset.</p> <p></p> <p>To avoid overfitting, multiple actions can be performed.</p>"},{"location":"10-Tutorials/3_regression_students/#model-regularization","title":"Model regularization\u00b6","text":"<p>The regularization allows to reduce the model complexity. For that, we will not only minimize our cost error, but we will minimize an objective function with a regularization term.</p> <p>$$min \\sum_{i}(y_i \u2013 X_i\\beta)^2 + Regularization$$</p>"},{"location":"10-Tutorials/3_regression_students/#ridge-l2-regularization","title":"Ridge - L2 regularization\u00b6","text":"<p>For the ridge regression, the regularization term will be the L2-norm of the coefficients :</p> <p>$$min \\sum_{i}(y_i \u2013 X_i\\beta)^2 + \\alpha\\vert\\vert\\beta\\vert\\vert_2^2$$</p> <p>where $$\\vert\\vert\\beta\\vert\\vert_2^2 = \\sum_{j}\\beta_j^2$$</p> <p>It is mainly used in the case of correlated features, which can bring unexpected behaviours with regard to the coefficients.</p>"},{"location":"10-Tutorials/3_regression_students/#simple-loop-on-regularization-parameter","title":"Simple loop on regularization parameter\u00b6","text":"<ul> <li>Fit <code>n</code> models for <code>n</code> values of <code>alpha</code> (regularization parameter)</li> <li>Compute the scores</li> <li>Retrieve the values of coefficients</li> </ul>"},{"location":"10-Tutorials/3_regression_students/#lasso-l1-regularization","title":"Lasso - L1 regularization\u00b6","text":"<p>For the lasso regression, the regularization term will be the L1-norm of the coefficients :</p> <p>$$min \\sum_{i}(y_i \u2013 X_i\\beta)^2 + \\alpha\\vert\\vert\\beta\\vert\\vert_1^2$$</p> <p>where $$\\vert\\vert\\beta\\vert\\vert_1^2 = \\sum_{j}\\vert\\beta_j\\vert$$</p>"},{"location":"10-Tutorials/3_regression_students/#simple-loop-on-regularization-parameter","title":"Simple loop on regularization parameter\u00b6","text":"<ul> <li>Fit <code>n</code> models for <code>n</code> values of <code>alpha</code> (regularization parameter)</li> <li>Compute the scores</li> <li>Retrieve the values of coefficients</li> </ul>"},{"location":"10-Tutorials/4_classification/","title":"\ud83d\udc0d Practice n\u00b04: classification","text":"<p>There are several types of classification :</p> <ul> <li>Binary classification: the task of classifying the data into two groups (each called class).</li> </ul> <p>Example: an email can be classified as belonging to one of two classes: \"spam\" and \"not spam\".</p> <ul> <li>Multi-class classification: the task of classifying the data into N groups (N &gt; 2).</li> </ul> <p>Example: an image can be classified as belonging to one of N classes: \"cat\", \"dog\", \"cow\" or \"fish\".</p> <ul> <li>Multi-label classification: this is a generalization of multi-class classification problem where an instance can be assigned to multiple classes.</li> </ul> <p>Example: a movie can be classified as belonging to one or more classes: \"action\", \"adventure\", \"thriller\" or all simultaneously.</p> <p>In this session, we will focus on the Binary classification.</p> <p></p> <p>After passing the linear combination through this function, the output will be considered as a probability $P$ and using a threshold (0.5 in general), the review can be classified as bad review, if $P &lt; 0.5$, or good review, if $P &gt;= 0.5$.</p> <p>This threshold can be modified in some contexts</p> <p>Then during the training phase, we will compute the parameters \u03b2 in order to optimize the <code>Maximum Likelihood</code> i.e., for a given bad review, we want the probalility estimated by our model to be minimal and for a given good review, we want the probalility estimated by our model to be maximal.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, classification_report\nimport nltk\nfrom nltk.corpus import stopwords\nimport seaborn as sns\nnltk.download('stopwords')\n\npd.set_option('display.max_colwidth', None)\npd.set_option(\"display.precision\", 2)\nsns.set_style(\"whitegrid\")\n</pre> import pandas as pd import numpy as np from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, classification_report import nltk from nltk.corpus import stopwords import seaborn as sns nltk.download('stopwords')  pd.set_option('display.max_colwidth', None) pd.set_option(\"display.precision\", 2) sns.set_style(\"whitegrid\") <pre>[nltk_data] Downloading package stopwords to /home/runner/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n</pre> In\u00a0[2]: Copied! <pre>file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\"\ndf_full = pd.read_parquet(file_url)\n</pre> file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\" df_full = pd.read_parquet(file_url) In\u00a0[3]: Copied! <pre>df_full.head(5)\n</pre> df_full.head(5) Out[3]: beer brewery alcohol type rating_appearance rating_aroma rating_palate rating_taste rating timestamp user text beer_degree brewery_degree user_degree text_length date is_good 0 Breckenridge Oatmeal Stout 383 4.95 Stout 4 7 4 7 14 1217462400 blutt59 bottle, oat nose with black color, bitter chocolate flavor with a good coffee cocoa finish 51 413 300 90 2008-07-31 02:00:00 0 1 Breckenridge 471 Small Batch Imperial Porter 383 7.50 Imperial/Strong Porter 3 8 3 8 14 1312588800 blutt59 bottle, received in trade, dark brown with garnet highlights, short lived foam, aroma is quite port like and raisin, flavor is sweet raisin or prune, toasty almond, waffle batter, nice finish 4 413 300 191 2011-08-06 02:00:00 0 2 Breckenridge Avalanche Amber 383 5.41 Amber Ale 3 5 3 5 10 1205020800 blutt59 12 oz. bottle, amber color with soapy head, slight caramel flavor but watery not much hops some effervescence 43 413 300 109 2008-03-09 01:00:00 0 3 Breckenridge Lucky U IPA 383 6.20 India Pale Ale (IPA) 3 6 3 7 12 1255737600 blutt59 bottle, golden orange color with light tan foam , citric soour aroma with hoppy citrus and lime bitter flavor, could use a little more malt for balance 20 413 300 151 2009-10-17 02:00:00 0 4 Fullers Vintage Ale 2009 55 8.50 English Strong Ale 3 7 3 8 14 1282003200 blutt59 bottle, thanks to SS, almond amber colored pour with off white soapy foam, fruity metallic aroma, flavors of dried apricots, metallics, fruity medicinal finish 18 978 300 159 2010-08-17 02:00:00 0 In\u00a0[4]: Copied! <pre># Filter the data\nN_rows = 1000\ndf = df_full[[\"text\", \"rating\", \"is_good\"]].sample(N_rows, random_state=42)\n</pre> # Filter the data N_rows = 1000 df = df_full[[\"text\", \"rating\", \"is_good\"]].sample(N_rows, random_state=42) In\u00a0[5]: Copied! <pre># Display some text reviews\n\nprint(\"GUESS THE RATING ?\")\ndf_example = df.sample(n=1)\ndf_example.text\n</pre> # Display some text reviews  print(\"GUESS THE RATING ?\") df_example = df.sample(n=1) df_example.text <pre>GUESS THE RATING ?\n</pre> Out[5]: <pre>117167    Cask at Spoons, Milton Keynes. Dark amber with a thick white head. Couldnt pick up an aroma. Faint malty taste overlaid by hops. Oily, creamy mouthfeel, probably thanks to the head, which was courtesy of Mr Wetherspoon. Slight bitter finish. Something lacking here.\nName: text, dtype: object</pre> In\u00a0[6]: Copied! <pre>print(f\"RATING: {df_example.rating.iloc[0]}\")\n</pre> print(f\"RATING: {df_example.rating.iloc[0]}\") <pre>RATING: 10\n</pre> <p>To begin with a binary classification problem, we will bin the target into 2 classes: bad review and good review</p> <p>First, look at the target distribution and choose a threshlold to identify good reviews from the rest.</p> In\u00a0[7]: Copied! <pre># display the target distribution\ndf.rating.astype(int).plot(kind=\"hist\")\n</pre> # display the target distribution df.rating.astype(int).plot(kind=\"hist\") Out[7]: <pre>&lt;Axes: ylabel='Frequency'&gt;</pre> <p>You can play with the rating_threshold and look at the new target distribution.</p> In\u00a0[8]: Copied! <pre># Create a binary target and display the target distribution\nrating_threshold = 16 # LINE TO BE REMOVED FOR STUDENTS\n(df.rating &gt;= rating_threshold).astype(int).value_counts(normalize=True)\n</pre> # Create a binary target and display the target distribution rating_threshold = 16 # LINE TO BE REMOVED FOR STUDENTS (df.rating &gt;= rating_threshold).astype(int).value_counts(normalize=True) Out[8]: <pre>rating\n0    0.74\n1    0.26\nName: proportion, dtype: float64</pre> <p>Usually the threshold is defined by looking manually at the data: annotating a few reviews as \"good\" or \"bad\" and see which ratings they had. E.g: on google maps a \"good\" review is above 4 stars (out of 5 stars). For simplicity, here we'll use the <code>is_good</code> binary target defined during the data engineering phase.</p> In\u00a0[9]: Copied! <pre># Create a binary target and display the target distribution\nrating_threshold = df.rating.median() # LINE TO BE REMOVED FOR STUDENTS\n(df.rating &gt;= rating_threshold).astype(int).value_counts(normalize=True)\n</pre> # Create a binary target and display the target distribution rating_threshold = df.rating.median() # LINE TO BE REMOVED FOR STUDENTS (df.rating &gt;= rating_threshold).astype(int).value_counts(normalize=True) Out[9]: <pre>rating\n1    0.56\n0    0.44\nName: proportion, dtype: float64</pre> In\u00a0[10]: Copied! <pre>def convert_text_to_lowercase(df, colname):\n    df[colname] = (\n        df[colname]\n        .str.lower() # LINE TO BE REMOVED FOR STUDENTS\n    )\n    return df\n\ndef not_regex(pattern):\n        return r\"((?!{}).)\".format(pattern)\n\ndef remove_punctuation(df, colname):\n    df[colname] = df[colname].str.replace(\"\\n\", \" \")\n    df[colname] = df[colname].str.replace(\"\\r\", \" \")\n    alphanumeric_characters_extended = \"(\\\\b[-/]\\\\b|[a-zA-Z0-9])\"\n    df[colname] = df[colname].str.replace(not_regex(alphanumeric_characters_extended), \" \")\n    return df\n\ndef tokenize_sentence(df, colname):\n    df[colname] = df[colname].str.split()\n    return df\n\ndef remove_stop_words(df, colname):\n    stop_words = stopwords.words(\"english\")\n    df[colname] = df[colname].apply(lambda x: [word for word in x if word not in stop_words])\n    return df\n\ndef reverse_tokenize_sentence(df, colname):\n    df[colname] = df[colname].map(lambda word: \" \".join(word))\n    return df\n\n\ndef text_cleaning(df, colname):\n\"\"\"\n    Takes in a string of text, then performs the following:\n    1. convert text to lowercase\n    2. remove punctuation and new line characters \"\\n\"\n    3. Tokenize sentences\n    4. Remove all stopwords\n    5. convert tokenized text to text\n    \"\"\"\n    df = (\n        df\n        .pipe(convert_text_to_lowercase, colname) # LINE TO BE REMOVED FOR STUDENTS\n        .pipe(remove_punctuation, colname) # LINE TO BE REMOVED FOR STUDENTS\n        .pipe(tokenize_sentence, colname) # LINE TO BE REMOVED FOR STUDENTS\n        .pipe(remove_stop_words, colname) # LINE TO BE REMOVED FOR STUDENTS\n        .pipe(reverse_tokenize_sentence, colname) # LINE TO BE REMOVED FOR STUDENTS\n    )\n    return df\n</pre> def convert_text_to_lowercase(df, colname):     df[colname] = (         df[colname]         .str.lower() # LINE TO BE REMOVED FOR STUDENTS     )     return df  def not_regex(pattern):         return r\"((?!{}).)\".format(pattern)  def remove_punctuation(df, colname):     df[colname] = df[colname].str.replace(\"\\n\", \" \")     df[colname] = df[colname].str.replace(\"\\r\", \" \")     alphanumeric_characters_extended = \"(\\\\b[-/]\\\\b|[a-zA-Z0-9])\"     df[colname] = df[colname].str.replace(not_regex(alphanumeric_characters_extended), \" \")     return df  def tokenize_sentence(df, colname):     df[colname] = df[colname].str.split()     return df  def remove_stop_words(df, colname):     stop_words = stopwords.words(\"english\")     df[colname] = df[colname].apply(lambda x: [word for word in x if word not in stop_words])     return df  def reverse_tokenize_sentence(df, colname):     df[colname] = df[colname].map(lambda word: \" \".join(word))     return df   def text_cleaning(df, colname):     \"\"\"     Takes in a string of text, then performs the following:     1. convert text to lowercase     2. remove punctuation and new line characters \"\\n\"     3. Tokenize sentences     4. Remove all stopwords     5. convert tokenized text to text     \"\"\"     df = (         df         .pipe(convert_text_to_lowercase, colname) # LINE TO BE REMOVED FOR STUDENTS         .pipe(remove_punctuation, colname) # LINE TO BE REMOVED FOR STUDENTS         .pipe(tokenize_sentence, colname) # LINE TO BE REMOVED FOR STUDENTS         .pipe(remove_stop_words, colname) # LINE TO BE REMOVED FOR STUDENTS         .pipe(reverse_tokenize_sentence, colname) # LINE TO BE REMOVED FOR STUDENTS     )     return df In\u00a0[11]: Copied! <pre># Apply data cleaning\ndf_cleaned = text_cleaning(\n    df, # LINE TO BE REMOVED FOR STUDENTS\n    \"text\"\n)\n</pre> # Apply data cleaning df_cleaned = text_cleaning(     df, # LINE TO BE REMOVED FOR STUDENTS     \"text\" ) In\u00a0[12]: Copied! <pre># Control the cleaning\ndf_cleaned.head()\n</pre> # Control the cleaning df_cleaned.head() Out[12]: text rating is_good 119737 0.33l 2009-bottle. pours deep red body thin bubbly side-sticking almost yellow-beige head. aroma lovely caramel sweet, hops, smoke brown sugar. medium-bodied medium+ carbonated. quite boozy first, caramel- brown sugar-sweet slight floral note. something barlywines normally sweet me, sweet, sweet first would thought. also strong bitterness equalized sweetness. combined warming alcohol made pleasant experience. 15 0 72272 another nice seasonal muskoka. pours nice dark brown beige frothy head goes nice covering top. nose lotsa chocolate cocoa. mouth feel touch thin otherwise works well. flavor lotsa chocolate touch cranberries decent well hidden amount booze. 16 1 158154 pours hazy, white hued light golden, aroma sweet, estery fruity notes banana sweet melon prevalent. rich chewy mouth anise-like bitterness nice light burn finish, slightly warming. 14 0 65426 served draft beer run charlottesville, va 3/30/11. pours clear copper color medium sized creamy tan head. good head retention lacing. aroma pretty much straight bourbon toffee notes quite bit booze. taste bourbon, vanilla, toffee, caramel, booze lightly toasty finish. medium bodied. 15 0 30074 tap old hat. listed \"amber rhy\" scoreboard inclined think spelling error brewpub staff. cloudy orange-red body, thin light cream head. sweet grainy aroma notes caramel. flavor caramely, fruity sweetness stout doppelbock had, beer boring comparison. little rye character permeated unrefined sweetness. sharp carbonation, bland beer. something uninitiated, suppose. 12 0 <p>We still have to transform the list of tokens to a fixed size numerical vector</p> <p>For that, we will use 2 very common techniques : CountVectorizer and TF-IDF</p> <p>1) CountVectorizer:</p> <p>CountVectorizer is used to convert a collection of text documents to a vector of token counts.</p> <p>Example:</p> <pre><code>[\"beer\", \"most\", \"tasty\", \"beer\", \"world\"]\n</code></pre> <p>Will be transformed into \u2b07</p> beer most tasty world 2 1 1 1 <p>In practice, you have to define a vocabulary size and each text will be transform into a vector of size [1 x vocabulary size]. Consequently, zeros will be added to the vector for each word present in the corpus vocabulary but missing in the specific review. The vocabulary space is defined using term frequency across the corpus: the most frequent words are kept.</p> <p>2) TF-IDF (optional):</p> <p>TF-IDF or term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to reflect how important a word is to a document in a collection or corpus.</p> <p>TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:</p> <pre><code>TF(t) = (Nbr of times term t appears in a document) / (Total nbr of terms in the\ndocument)\n</code></pre> <p>IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:</p> <pre><code>IDF(t) = log(Total number of documents / Number of documents with term t in it)\n</code></pre> <p>Example:</p> <p>Consider a document containing 100 words wherein the word book appears 3 times. The term frequency (i.e., tf) for cat is:</p> <pre><code>TF(t) = (Nbr of times term t appears in a document) / (Total nbr of terms in the\ndocument)\n      = 3 / 100\n      = 0.03\n</code></pre> <p>Now, assume we have 10 million documents and the word book appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as</p> <pre><code>IDF(t) = log(Total number of documents / Number of documents with term t in it)\n       = log(10,000,000 / 1,000)\n       = 4\n</code></pre> <p>Thus, the Tf-idf weight is the product of these quantities:</p> <pre><code>Tf-IDF = TF(t) * IDF(t)\n       = 0.03 * 4\n       = 0.12\n</code></pre> In\u00a0[13]: Copied! <pre>TARGET = \"is_good\" # LINE TO BE REMOVED FOR STUDENTS\nFEATURE = \"text\" # LINE TO BE REMOVED FOR STUDENTS\n\nx_train, x_test, y_train, y_test = train_test_split(\n    df_cleaned[FEATURE], # LINE TO BE REMOVED FOR STUDENTS\n    df_cleaned[TARGET], # LINE TO BE REMOVED FOR STUDENTS\n    test_size=0.2,\n    random_state=42)\n</pre> TARGET = \"is_good\" # LINE TO BE REMOVED FOR STUDENTS FEATURE = \"text\" # LINE TO BE REMOVED FOR STUDENTS  x_train, x_test, y_train, y_test = train_test_split(     df_cleaned[FEATURE], # LINE TO BE REMOVED FOR STUDENTS     df_cleaned[TARGET], # LINE TO BE REMOVED FOR STUDENTS     test_size=0.2,     random_state=42) In\u00a0[14]: Copied! <pre># Define the vocabulary size to 100\ncount_vectorizer = CountVectorizer(\n    analyzer=\"word\",\n    max_features=100 # LINE TO BE REMOVED FOR STUDENTS\n    )\n</pre> # Define the vocabulary size to 100 count_vectorizer = CountVectorizer(     analyzer=\"word\",     max_features=100 # LINE TO BE REMOVED FOR STUDENTS     ) In\u00a0[15]: Copied! <pre># Apply the CountVectorizer and check the results on some rows\ncount_vectorizer.fit(x_train)\n\nx_train_features = count_vectorizer.transform(x_train).toarray()\nx_test_features = count_vectorizer.transform(x_test).toarray()\n</pre> # Apply the CountVectorizer and check the results on some rows count_vectorizer.fit(x_train)  x_train_features = count_vectorizer.transform(x_train).toarray() x_test_features = count_vectorizer.transform(x_test).toarray() In\u00a0[16]: Copied! <pre>x_train_features[0]\n</pre> x_train_features[0] Out[16]: <pre>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])</pre> In\u00a0[17]: Copied! <pre>x_train.iloc[0]\n</pre> x_train.iloc[0] Out[17]: <pre>'bottle home ... colins tasting ... deep hazy brown ... thin lacing ... light sour cheery light strawb nose ... tart sourness ... cherry ... light sourness ... ok.'</pre> <p>The next step is to define the model that will use in input the vectors from the the <code>CountVectorizer</code>. We will use a logistic regression model, using again the <code>scikit-learn</code> library.</p> <p>In order to produce cleaner code, we can combine these 2 steps (CountVectorizer and Logistic regression) into a single pipeline.</p> <ul> <li>Initialize the <code>CountVectorizer</code></li> <li>Initialize the <code>LogisticRegression</code></li> <li>Define your <code>Pipeline</code> object with these 2 steps</li> <li>Fit the <code>Pipeline</code></li> </ul> <p>Hint:</p> <ul> <li><code>sklearn.linear_model.LogisticRegression</code></li> <li><code>sklearn.pipeline.Pipeline</code></li> </ul> In\u00a0[18]: Copied! <pre># Initialize the CountVectorizer\ncount_vectorizer = CountVectorizer(\n    analyzer=\"word\",\n    max_features=100\n    )\n\n# Initialize the logistic regression\nlogit = LogisticRegression(solver=\"lbfgs\", verbose=2, n_jobs=-1)\n\n# Combine them into a Pipeline object\npipeline_cv = Pipeline([\n    (\"vectorizer\", count_vectorizer), # LINE TO BE REMOVED FOR STUDENTS\n    (\"model\", logit)]) # LINE TO BE REMOVED FOR STUDENTS\n\n# Fit the Pipeline\npipeline_cv.fit(x_train, y_train)\n</pre> # Initialize the CountVectorizer count_vectorizer = CountVectorizer(     analyzer=\"word\",     max_features=100     )  # Initialize the logistic regression logit = LogisticRegression(solver=\"lbfgs\", verbose=2, n_jobs=-1)  # Combine them into a Pipeline object pipeline_cv = Pipeline([     (\"vectorizer\", count_vectorizer), # LINE TO BE REMOVED FOR STUDENTS     (\"model\", logit)]) # LINE TO BE REMOVED FOR STUDENTS  # Fit the Pipeline pipeline_cv.fit(x_train, y_train) <pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n</pre> <pre>RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =          101     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  5.54518D+02    |proj g|=  2.04000D+02\n\nAt iterate    1    f=  5.31344D+02    |proj g|=  8.78928D+01\n\nAt iterate    2    f=  4.55160D+02    |proj g|=  4.01187D+01\n\nAt iterate    3    f=  4.32123D+02    |proj g|=  8.26324D+01\n\nAt iterate    4    f=  4.16715D+02    |proj g|=  3.98241D+01\n\nAt iterate    5    f=  3.97087D+02    |proj g|=  1.47372D+01\n\nAt iterate    6    f=  3.65120D+02    |proj g|=  1.53607D+01\n\nAt iterate    7    f=  3.40922D+02    |proj g|=  1.62430D+01\n\nAt iterate    8    f=  3.29177D+02    |proj g|=  3.47076D+00\n\nAt iterate    9    f=  3.27222D+02    |proj g|=  3.63787D+00\n\nAt iterate   10    f=  3.24542D+02    |proj g|=  4.96374D+00\n\nAt iterate   11    f=  3.22516D+02    |proj g|=  3.37517D+00\n\nAt iterate   12    f=  3.21303D+02    |proj g|=  1.66355D+00\n\nAt iterate   13    f=  3.21030D+02    |proj g|=  1.21264D+00\n\nAt iterate   14    f=  3.20910D+02    |proj g|=  4.79720D-01\n\nAt iterate   15    f=  3.20867D+02    |proj g|=  2.75051D+00\n\nAt iterate   16    f=  3.20817D+02    |proj g|=  7.42277D-01\n\nAt iterate   17    f=  3.20805D+02    |proj g|=  3.80123D-01\n\nAt iterate   18    f=  3.20798D+02    |proj g|=  2.03584D-01\n\nAt iterate   19    f=  3.20788D+02    |proj g|=  1.89959D-01\n\nAt iterate   20    f=  3.20780D+02    |proj g|=  1.59201D-01\n\nAt iterate   21    f=  3.20776D+02    |proj g|=  1.64005D-01\n\nAt iterate   22    f=  3.20775D+02    |proj g|=  6.25600D-02\n\nAt iterate   23    f=  3.20774D+02    |proj g|=  3.73012D-02\n\nAt iterate   24    f=  3.20774D+02    |proj g|=  2.13423D-02\n\nAt iterate   25    f=  3.20774D+02    |proj g|=  3.09411D-02\n\nAt iterate   26    f=  3.20774D+02    |proj g|=  1.86634D-02\n\nAt iterate   27    f=  3.20774D+02    |proj g|=  1.77506D-02\n\nAt iterate   28    f=  3.20774D+02    |proj g|=  1.19104D-02\n\nAt iterate   29    f=  3.20774D+02    |proj g|=  6.78925D-03\n\nAt iterate   30    f=  3.20774D+02    |proj g|=  4.54960D-03\n\nAt iterate   31    f=  3.20774D+02    |proj g|=  4.83343D-03\n\nAt iterate   32    f=  3.20774D+02    |proj g|=  8.18759D-03\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n  101     32     35      1     0     0   8.188D-03   3.208D+02\n  F =   320.77383276003553     \n\nCONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH             \n</pre> <pre> This problem is unconstrained.\n</pre> Out[18]: <pre>Pipeline(steps=[('vectorizer', CountVectorizer(max_features=100)),\n                ('model', LogisticRegression(n_jobs=-1, verbose=2))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('vectorizer', CountVectorizer(max_features=100)),\n                ('model', LogisticRegression(n_jobs=-1, verbose=2))])</pre>CountVectorizer<pre>CountVectorizer(max_features=100)</pre>LogisticRegression<pre>LogisticRegression(n_jobs=-1, verbose=2)</pre> <p>Now you can make predictions on the test set</p> In\u00a0[19]: Copied! <pre># predictions\ny_pred_cv = pipeline_cv.predict(x_test)\n</pre> # predictions y_pred_cv = pipeline_cv.predict(x_test) <p>How to evaluate our model ?</p> <p><code>Accuracy</code> is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations</p> In\u00a0[20]: Copied! <pre># Compute accuracy\nprint(f\"model accuracy : {accuracy_score(y_pred_cv, y_test)} %\")\n</pre> # Compute accuracy print(f\"model accuracy : {accuracy_score(y_pred_cv, y_test)} %\") <pre>model accuracy : 0.72 %\n</pre> <p>What is you opinion about the <code>accuracy</code> score ?</p> <p>One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model.</p> <p>One might think that if we have high accuracy, our model is the best. Yes, accuracy is an excellent measure, but only when you have balanced data (i.e. an equivalent representation of each class in the data).</p> <p>Let's do a test with a reference model to show how accuracy can be a source of error when evaluating a model: Create a model that predict everytime the most frequent class and compare the results.</p> In\u00a0[21]: Copied! <pre># Baseline model: predict the most-frequent class\ny_pred_baseline = [y_test.mode()[0]] * len(y_test)\n\n# Compute accuracy\nprint(f\"model baseline accuracy : {accuracy_score(y_pred_baseline, y_test)} %\")\n</pre> # Baseline model: predict the most-frequent class y_pred_baseline = [y_test.mode()[0]] * len(y_test)  # Compute accuracy print(f\"model baseline accuracy : {accuracy_score(y_pred_baseline, y_test)} %\") <pre>model baseline accuracy : 0.7 %\n</pre> <p>Accuracy are closed, but the last model is completely idiot !</p> <p>In case of an imbalanced target (let's say 99% zeros), the accuracy of this dumb model will be 99% !</p> <p>Therefore, you need to look at other metrics to evaluate the performance of your model.</p> <p></p> In\u00a0[22]: Copied! <pre># Confusion matrices\nprint(f\"Confusion matrix of the first model: \\n {confusion_matrix(y_test, y_pred_cv)}\")\nprint(f\"Confusion matrix of the baseline model: \\n {confusion_matrix(y_test, y_pred_baseline)}\")\n</pre> # Confusion matrices print(f\"Confusion matrix of the first model: \\n {confusion_matrix(y_test, y_pred_cv)}\") print(f\"Confusion matrix of the baseline model: \\n {confusion_matrix(y_test, y_pred_baseline)}\") <pre>Confusion matrix of the first model: \n [[122  18]\n [ 38  22]]\nConfusion matrix of the baseline model: \n [[140   0]\n [ 60   0]]\n</pre> In\u00a0[23]: Copied! <pre># Evaluate the first model\nprint(f\"first model precision : {precision_score(y_pred_cv, y_test):.{3}f}%\")\nprint(f\"first model recall    : {recall_score(y_pred_cv, y_test)}%\")\nprint(f\"first model f1 score  : {f1_score(y_pred_cv, y_test):.{3}f}%\\n\")\n\n# Evaluate the baseline model\nprint(f\"baseline model precision : {precision_score(y_pred_baseline, y_test)}%\")\nprint(f\"baseline model recall    : {recall_score(y_pred_baseline, y_test)}%\")\nprint(f\"baseline model f1 score  : {f1_score(y_pred_baseline, y_test)}%\")\n</pre> # Evaluate the first model print(f\"first model precision : {precision_score(y_pred_cv, y_test):.{3}f}%\") print(f\"first model recall    : {recall_score(y_pred_cv, y_test)}%\") print(f\"first model f1 score  : {f1_score(y_pred_cv, y_test):.{3}f}%\\n\")  # Evaluate the baseline model print(f\"baseline model precision : {precision_score(y_pred_baseline, y_test)}%\") print(f\"baseline model recall    : {recall_score(y_pred_baseline, y_test)}%\") print(f\"baseline model f1 score  : {f1_score(y_pred_baseline, y_test)}%\") <pre>first model precision : 0.367%\nfirst model recall    : 0.55%\nfirst model f1 score  : 0.440%\n\nbaseline model precision : 0.0%\nbaseline model recall    : 0.0%\nbaseline model f1 score  : 0.0%\n</pre> <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> In\u00a0[24]: Copied! <pre># Classification report\nprint(classification_report(\n    y_test, y_pred_cv # LINE TO BE REMOVED FOR STUDENTS\n))\n</pre> # Classification report print(classification_report(     y_test, y_pred_cv # LINE TO BE REMOVED FOR STUDENTS )) <pre>              precision    recall  f1-score   support\n\n           0       0.76      0.87      0.81       140\n           1       0.55      0.37      0.44        60\n\n    accuracy                           0.72       200\n   macro avg       0.66      0.62      0.63       200\nweighted avg       0.70      0.72      0.70       200\n\n</pre> In\u00a0[25]: Copied! <pre># Classification report\nprint(classification_report(y_test, y_pred_baseline))\n</pre> # Classification report print(classification_report(y_test, y_pred_baseline)) <pre>              precision    recall  f1-score   support\n\n           0       0.70      1.00      0.82       140\n           1       0.00      0.00      0.00        60\n\n    accuracy                           0.70       200\n   macro avg       0.35      0.50      0.41       200\nweighted avg       0.49      0.70      0.58       200\n\n</pre> <pre>/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/runner/micromamba/envs/ml-bootcamp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> In\u00a0[26]: Copied! <pre># Initialize the TF-IDF\ntfidf_vectorizer = TfidfVectorizer(\n    analyzer='word',\n    max_features=100 # LINE TO BE REMOVED FOR STUDENTS\n    )\n\n# Apply the TfidfVectorizer and check the results on some rows\ntfidf_vectorizer.fit(x_train)\n\nx_train_features = tfidf_vectorizer.transform(x_train).toarray()\nx_test_features = tfidf_vectorizer.transform(x_test).toarray()\n</pre> # Initialize the TF-IDF tfidf_vectorizer = TfidfVectorizer(     analyzer='word',     max_features=100 # LINE TO BE REMOVED FOR STUDENTS     )  # Apply the TfidfVectorizer and check the results on some rows tfidf_vectorizer.fit(x_train)  x_train_features = tfidf_vectorizer.transform(x_train).toarray() x_test_features = tfidf_vectorizer.transform(x_test).toarray() In\u00a0[27]: Copied! <pre>x_train_features[0]\n</pre> x_train_features[0] Out[27]: <pre>array([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.19400832, 0.        , 0.22794963, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.32713051, 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.30997343,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.28915725, 0.59635253, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.26488472, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.34835863, 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.27665128, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ])</pre> In\u00a0[28]: Copied! <pre>x_train.iloc[0]\n</pre> x_train.iloc[0] Out[28]: <pre>'bottle home ... colins tasting ... deep hazy brown ... thin lacing ... light sour cheery light strawb nose ... tart sourness ... cherry ... light sourness ... ok.'</pre> <p>You can now combine the vectorizer to a logistic regression in a single pipeline</p> In\u00a0[29]: Copied! <pre># Initialize the TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(\n    analyzer='word',\n    max_features=100\n    )\n\n# Initialize the logistic regression\nlogit = LogisticRegression(solver='lbfgs', verbose=2, n_jobs=-1)\n\n# Combine them into a Pipeline object\npipeline_tfidf = Pipeline([\n    ('vectorizer', tfidf_vectorizer), # LINE TO BE REMOVED FOR STUDENTS\n    ('model', logit)]) # LINE TO BE REMOVED FOR STUDENTS\n\n# Fit the Pipeline\npipeline_tfidf.fit(x_train, y_train)\n</pre> # Initialize the TfidfVectorizer tfidf_vectorizer = TfidfVectorizer(     analyzer='word',     max_features=100     )  # Initialize the logistic regression logit = LogisticRegression(solver='lbfgs', verbose=2, n_jobs=-1)  # Combine them into a Pipeline object pipeline_tfidf = Pipeline([     ('vectorizer', tfidf_vectorizer), # LINE TO BE REMOVED FOR STUDENTS     ('model', logit)]) # LINE TO BE REMOVED FOR STUDENTS  # Fit the Pipeline pipeline_tfidf.fit(x_train, y_train) <pre>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n</pre> <pre>RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =          101     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  5.54518D+02    |proj g|=  2.04000D+02\n\nAt iterate    1    f=  4.43986D+02    |proj g|=  9.94050D+00\n\nAt iterate    2    f=  4.41607D+02    |proj g|=  5.63999D+00\n\nAt iterate    3    f=  4.31240D+02    |proj g|=  2.62300D+01\n\nAt iterate    4    f=  4.16711D+02    |proj g|=  4.32906D+01\n\nAt iterate    5    f=  3.94456D+02    |proj g|=  4.20929D+01\n\nAt iterate    6    f=  3.76615D+02    |proj g|=  8.35456D+00\n\nAt iterate    7    f=  3.75845D+02    |proj g|=  1.10314D+00\n\nAt iterate    8    f=  3.75745D+02    |proj g|=  2.13981D+00\n\nAt iterate    9    f=  3.75463D+02    |proj g|=  2.69875D+00\n\nAt iterate   10    f=  3.74909D+02    |proj g|=  1.97574D+00\n\nAt iterate   11    f=  3.74747D+02    |proj g|=  2.81994D-01\n\nAt iterate   12    f=  3.74739D+02    |proj g|=  9.21477D-02\n\nAt iterate   13    f=  3.74736D+02    |proj g|=  1.40607D-01\n\nAt iterate   14    f=  3.74733D+02    |proj g|=  3.94355D-01\n\nAt iterate   15    f=  3.74732D+02    |proj g|=  9.60931D-02\n\nAt iterate   16    f=  3.74731D+02    |proj g|=  1.79723D-02\n\nAt iterate   17    f=  3.74731D+02    |proj g|=  2.14785D-02\n\nAt iterate   18    f=  3.74731D+02    |proj g|=  1.95197D-02\n\nAt iterate   19    f=  3.74731D+02    |proj g|=  6.76658D-02\n\nAt iterate   20    f=  3.74731D+02    |proj g|=  3.09067D-02\n\nAt iterate   21    f=  3.74731D+02    |proj g|=  7.79600D-03\n\nAt iterate   22    f=  3.74731D+02    |proj g|=  5.07095D-03\n\nAt iterate   23    f=  3.74731D+02    |proj g|=  6.33766D-03\n\nAt iterate   24    f=  3.74731D+02    |proj g|=  2.25929D-03\n\nAt iterate   25    f=  3.74731D+02    |proj g|=  1.51012D-03\n\nAt iterate   26    f=  3.74731D+02    |proj g|=  1.39964D-03\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n  101     26     30      1     0     0   1.400D-03   3.747D+02\n  F =   374.73103485123914     \n\nCONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH             \n</pre> <pre> This problem is unconstrained.\n</pre> Out[29]: <pre>Pipeline(steps=[('vectorizer', TfidfVectorizer(max_features=100)),\n                ('model', LogisticRegression(n_jobs=-1, verbose=2))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('vectorizer', TfidfVectorizer(max_features=100)),\n                ('model', LogisticRegression(n_jobs=-1, verbose=2))])</pre>TfidfVectorizer<pre>TfidfVectorizer(max_features=100)</pre>LogisticRegression<pre>LogisticRegression(n_jobs=-1, verbose=2)</pre> In\u00a0[30]: Copied! <pre># Predictions\ny_pred_tfidf = pipeline_tfidf.predict(x_test)\n\n# Evaluate the second model\nprint(f\"Confusion matrix of the first model: \\n {confusion_matrix(y_test, y_pred_tfidf)}\")\nprint(f\"second model precision : {precision_score(y_pred_tfidf, y_test):.{3}f}%\")\nprint(f\"second model recall    : {recall_score(y_pred_tfidf, y_test)}%\")\nprint(f\"second model f1 score  : {f1_score(y_pred_tfidf, y_test):.{3}f}%\\n\")\n</pre> # Predictions y_pred_tfidf = pipeline_tfidf.predict(x_test)  # Evaluate the second model print(f\"Confusion matrix of the first model: \\n {confusion_matrix(y_test, y_pred_tfidf)}\") print(f\"second model precision : {precision_score(y_pred_tfidf, y_test):.{3}f}%\") print(f\"second model recall    : {recall_score(y_pred_tfidf, y_test)}%\") print(f\"second model f1 score  : {f1_score(y_pred_tfidf, y_test):.{3}f}%\\n\") <pre>Confusion matrix of the first model: \n [[138   2]\n [ 49  11]]\nsecond model precision : 0.183%\nsecond model recall    : 0.8461538461538461%\nsecond model f1 score  : 0.301%\n\n</pre> In\u00a0[31]: Copied! <pre># Classification report\nprint(classification_report(y_test, y_pred_tfidf))\n</pre> # Classification report print(classification_report(y_test, y_pred_tfidf)) <pre>              precision    recall  f1-score   support\n\n           0       0.74      0.99      0.84       140\n           1       0.85      0.18      0.30        60\n\n    accuracy                           0.74       200\n   macro avg       0.79      0.58      0.57       200\nweighted avg       0.77      0.74      0.68       200\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"10-Tutorials/4_classification/#practice-n4-classification","title":"\ud83d\udc0d Practice n\u00b04: classification\u00b6","text":"<p>The objective of this session is to learn about classification task. You will have to build a new model only using the text review.</p> <p>In order to apply classification models, we need to change the target definition. We will divide the reviews into good or bad using the rating.</p> <p>So our goal is to have a model, a function, that takes text as input and output a label/class (good review or bad review).</p> <p>$$f(text) = label$$</p> <p>The data are the same as those used for the regression (source ratebeer dataset description).</p> <p>Here are the main steps of the notebook :</p> <ol> <li>What is classification ?</li> <li>Focus on logistic regression</li> <li>Preparation</li> <li>Binary target definition</li> <li>Text cleaning</li> <li>Modelling</li> </ol> <ul> <li>6.1 First model using CountVectorizer</li> <li>6.2 Choose the right metrics</li> <li>6.3 Second model using TF-IDF</li> </ul>"},{"location":"10-Tutorials/4_classification/#1-what-is-classification","title":"1. What is classification ?\u00b6","text":"<p>Classification in machine learning consists of mathematical methods that allow to predict a discrete outcome (y) based on the value of one or more predictor variables (x).</p>"},{"location":"10-Tutorials/4_classification/#2-focus-on-logistic-regression","title":"2. Focus on logistic regression\u00b6","text":"<p>As seen in the last session, we can represent the link between the explicatives variables and the target to be predicted as follows</p> <p>$$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$$</p> <p>The difference here is that the target is not a continous variable (rating) but a discrete one (good review / bad review). If we limit ourselves to this model, the linear combination of each input will give an unbounded number that does not allow us to classify the review into good or bad.</p> <p>To transform the number provided by the linear combination into a classification, we use a function called <code>sigmoid function</code> which has the interesting property of transforming the numbers passed inside into numbers between 0 and 1.</p>"},{"location":"10-Tutorials/4_classification/#3-preparation","title":"3. Preparation\u00b6","text":""},{"location":"10-Tutorials/4_classification/#install-import-modules","title":"Install &amp; import modules\u00b6","text":""},{"location":"10-Tutorials/4_classification/#read-remote-dataset","title":"Read remote dataset\u00b6","text":""},{"location":"10-Tutorials/4_classification/#4-binary-target-definition","title":"4. Binary target definition\u00b6","text":"<p>Filter the data using only 1000 reviews and then explore some reviews</p>"},{"location":"10-Tutorials/4_classification/#5-text-cleaning","title":"5. Text cleaning\u00b6","text":""},{"location":"10-Tutorials/4_classification/#from-text-reviews-to-numerical-vectors","title":"From text reviews to numerical vectors\u00b6","text":"<p>Before training some models, the first step is to transform text into numbers :</p> <p>from</p> <pre><code>f(raw_text) = rating_classe\n</code></pre> <p>to</p> <pre><code>f(numerical_vector_representing_text) = rating_classe\n</code></pre> <p>Indeed, we can't direclty feed an algortihm with text.</p> <p>For example:</p> <pre><code>Wow, that beer is SOOOO good :O !!\n</code></pre> <p>must be transformed to something like:</p> <pre><code>[3, 4, 21, 0, 0, 8, 19]\n</code></pre> <p>where the values of the vector contain the meaning of the text. Knowing that the closer texts are in terms of meaning, the more closed their vector representation is too. Moreover, it is often more convenient to convert texts of different sizes into vectors of fixed size.</p> <p>For example:</p> <pre><code>\"Wow, that beer is SOOOO good :O !!\"\n-&gt; characters : 34\n-&gt; vector (1x7) : [3, 4, 21, 0, 0, 8, 19]\n</code></pre> <pre><code>\"This beer is very tasty\"\n-&gt; characters : 23\n-&gt; vector (1x7) : [3, 4, 20, 0, 0, 7, 19]\n</code></pre> <p>But:</p> <pre><code>\"It's not a beer, just motor oil at best.\"\n-&gt; characters : 40\n-&gt; vector (1x7) : [0, 4, 1, 12, 14, 0, 0]\n</code></pre>"},{"location":"10-Tutorials/4_classification/#from-raw-text-reviews-to-clean-list-of-words","title":"From raw text reviews to clean list of words\u00b6","text":"<p>Before converting text to numerical vector, the first step is to clean the text to keep only the pertinent information.</p> <p>Here are the following cleaning steps that we will apply on the reviews:</p> <ol> <li>Convert letter to lowercase</li> </ol> <pre><code>\"Wow, that beer is SOOOO good :O !!\" -&gt; \"wow, that beer is soooo good :o !!\"\n</code></pre> <ol> <li>Remove the punctuation letter to lowercase</li> </ol> <pre><code>\"wow, that beer is soooo good :o !!\" -&gt; \"wow that beer is soooo good\"\n</code></pre> <ol> <li>Transform the text into tokens</li> </ol> <pre><code>\"wow that beer is soooo good\" -&gt; [\"wow\", \"that\", \"beer\", \"is\", \"soooo\", \"good\"]\n</code></pre> <ol> <li>Remove the stopwords, the most common english words that often bring noise to the models.</li> </ol> <pre><code>[\"wow\", \"that\", \"beer\", \"is\", \"soooo\", \"good\"] -&gt; [\"wow\", \"beer\", \"soooo\", \"good\"]\n</code></pre> <ol> <li>To go further, some techniques can be used to reduce the forms of each word into a common base or root. This can be done with:</li> </ol> <p>(1) Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.</p> <p>am, are, is  $\\Rightarrow$  be</p> <p>(2) Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma.</p> <p>book, books, book's, book'  $\\Rightarrow$  book</p> <p>The steps presented here are just the most basic, many different things can be applied to the cleaning part of the text.</p>"},{"location":"10-Tutorials/4_classification/#split-the-data-into-traintest-sets","title":"Split the data into train/test sets\u00b6","text":"<p>Before applying these transormation on the text, we will just split the data for the modelling part.</p> <p>Keep 20% of the data for the test dataset</p>"},{"location":"10-Tutorials/4_classification/#6-modelling","title":"6. Modelling\u00b6","text":""},{"location":"10-Tutorials/4_classification/#61-first-model-using-countvectorizer","title":"6.1 First model using CountVectorizer\u00b6","text":"<p>Transform the text reviews into numerical vectors by counting the number of words in each reviews. Use the <code>scikit-learn</code> library.</p> <p>In order not to bring information from the train set into the test set, you must train the <code>CountVectorizer</code> on the train set and apply it to the test set.</p> <p>Hint:</p> <ul> <li><code>sklearn.feature_extraction.text.CountVectorizer</code></li> </ul>"},{"location":"10-Tutorials/4_classification/#62-choose-the-right-metrics","title":"6.2 Choose the right metrics\u00b6","text":"<p>We need now define other metrics to evaluate our model.</p> <ul> <li><p>True Positives (TP): these are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.</p> </li> <li><p>True Negatives (TN): these are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.</p> </li> <li><p>False Positives (FP): when actual class is no and predicted class is yes.</p> </li> <li><p>False Negatives (FN): when actual class is yes but predicted class in no.</p> </li> <li><p>Accuracy: the ratio of correctly predicted observation to the total observations.</p> </li> </ul> <p>Accuracy = $\\frac{TP+TN}{TP+FP+FN+TN}$</p> <ul> <li>Precision: Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. High precision relates to the low false positive rate.</li> </ul> <p>Precision = $\\frac{TP}{TP+FP}$</p> <ul> <li>Recall (Sensitivity): Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes.</li> </ul> <p>Recall = $\\frac{TP}{TP+FN}$</p> <ul> <li>F1 score: F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution.</li> </ul> <p>F1 Score = $2* \\frac{Recall * Precision}{Recall + Precision}$</p> <p>Let's now compare again our 2 models !</p> <p>First, plot the confusion matrix, the precision, recall and f1 score for each model. You can also print the classification report of scikit learn that sums up the main classification metrics.</p> <p>Hint:</p> <ul> <li><code>sklearn.metrics.confusion_matrix</code></li> <li><code>sklearn.metrics.recall_score</code></li> <li><code>sklearn.metrics.precision_score</code></li> <li><code>sklearn.metrics.f1_score</code></li> <li><code>sklearn.metrics.classification_report</code></li> </ul>"},{"location":"10-Tutorials/4_classification/#63-second-model-using-tf-idf-vectorizer-optional","title":"6.3 Second model using TF-IDF Vectorizer (optional)\u00b6","text":"<p>In this last section, you will use a better approach in term of vectorization: TF-IDF</p> <p>Scikit-learn provide the <code>TfidfVectorizer</code> that can be used in the same way as <code>CountVectorizer</code>.</p> <p>Hint:</p> <ul> <li><code>sklearn.feature_extraction.text.TfidfVectorizer</code></li> </ul>"},{"location":"10-Tutorials/4_classification/#64-optimize-the-model","title":"6.4 Optimize the model\u00b6","text":"<p>You can now try to optimize the model by changing a lot of parameters:</p> <ul> <li>Take more reviews in input</li> <li>Increase the <code>max_features</code> parameter</li> <li>Remove the most frequent words</li> <li>Try adding n-grams to the vectorizer</li> <li>Improve the text cleaning</li> <li>etc...</li> </ul>"},{"location":"10-Tutorials/4_classification_students/","title":"\ud83d\udc0d Practice n\u00b04: classification (students version)","text":"<p>There are several types of classification :</p> <ul> <li>Binary classification: the task of classifying the data into two groups (each called class).</li> </ul> <p>Example: an email can be classified as belonging to one of two classes: \"spam\" and \"not spam\".</p> <ul> <li>Multi-class classification: the task of classifying the data into N groups (N &gt; 2).</li> </ul> <p>Example: an image can be classified as belonging to one of N classes: \"cat\", \"dog\", \"cow\" or \"fish\".</p> <ul> <li>Multi-label classification: this is a generalization of multi-class classification problem where an instance can be assigned to multiple classes.</li> </ul> <p>Example: a movie can be classified as belonging to one or more classes: \"action\", \"adventure\", \"thriller\" or all simultaneously.</p> <p>In this session, we will focus on the Binary classification.</p> <p></p> <p>After passing the linear combination through this function, the output will be considered as a probability $P$ and using a threshold (0.5 in general), the review can be classified as bad review, if $P &lt; 0.5$, or good review, if $P &gt;= 0.5$.</p> <p>This threshold can be modified in some contexts</p> <p>Then during the training phase, we will compute the parameters \u03b2 in order to optimize the <code>Maximum Likelihood</code> i.e., for a given bad review, we want the probalility estimated by our model to be minimal and for a given good review, we want the probalility estimated by our model to be maximal.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, classification_report\nimport nltk\nfrom nltk.corpus import stopwords\nimport seaborn as sns\nnltk.download('stopwords')\n\npd.set_option('display.max_colwidth', None)\npd.set_option(\"display.precision\", 2)\nsns.set_style(\"whitegrid\")\n</pre> import pandas as pd import numpy as np from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, classification_report import nltk from nltk.corpus import stopwords import seaborn as sns nltk.download('stopwords')  pd.set_option('display.max_colwidth', None) pd.set_option(\"display.precision\", 2) sns.set_style(\"whitegrid\") In\u00a0[\u00a0]: Copied! <pre>file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\"\ndf_full = pd.read_parquet(file_url)\n</pre> file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\" df_full = pd.read_parquet(file_url) In\u00a0[\u00a0]: Copied! <pre>df_full.head(5)\n</pre> df_full.head(5) In\u00a0[\u00a0]: Copied! <pre># Filter the data\nN_rows = 1000\ndf = df_full[[\"text\", \"rating\", \"is_good\"]].sample(N_rows, random_state=42)\n</pre> # Filter the data N_rows = 1000 df = df_full[[\"text\", \"rating\", \"is_good\"]].sample(N_rows, random_state=42) In\u00a0[\u00a0]: Copied! <pre># Display some text reviews\n\nprint(\"GUESS THE RATING ?\")\ndf_example = df.sample(n=1)\ndf_example.text\n</pre> # Display some text reviews  print(\"GUESS THE RATING ?\") df_example = df.sample(n=1) df_example.text In\u00a0[\u00a0]: Copied! <pre>print(f\"RATING: {df_example.rating.iloc[0]}\")\n</pre> print(f\"RATING: {df_example.rating.iloc[0]}\") <p>To begin with a binary classification problem, we will bin the target into 2 classes: bad review and good review</p> <p>First, look at the target distribution and choose a threshlold to identify good reviews from the rest.</p> In\u00a0[\u00a0]: Copied! <pre># display the target distribution\ndf.rating.astype(int).plot(kind=\"hist\")\n</pre> # display the target distribution df.rating.astype(int).plot(kind=\"hist\") <p>You can play with the rating_threshold and look at the new target distribution.</p> In\u00a0[\u00a0]: Copied! <pre># Create a binary target and display the target distribution\n*** FILL THE MISSING LINE ***\n(df.rating &gt;= rating_threshold).astype(int).value_counts(normalize=True)\n</pre> # Create a binary target and display the target distribution *** FILL THE MISSING LINE *** (df.rating &gt;= rating_threshold).astype(int).value_counts(normalize=True) <p>Usually the threshold is defined by looking manually at the data: annotating a few reviews as \"good\" or \"bad\" and see which ratings they had. E.g: on google maps a \"good\" review is above 4 stars (out of 5 stars). For simplicity, here we'll use the <code>is_good</code> binary target defined during the data engineering phase.</p> In\u00a0[\u00a0]: Copied! <pre># Create a binary target and display the target distribution\n*** FILL THE MISSING LINE ***\n(df.rating &gt;= rating_threshold).astype(int).value_counts(normalize=True)\n</pre> # Create a binary target and display the target distribution *** FILL THE MISSING LINE *** (df.rating &gt;= rating_threshold).astype(int).value_counts(normalize=True) In\u00a0[\u00a0]: Copied! <pre>def convert_text_to_lowercase(df, colname):\n    df[colname] = (\n        df[colname]\n        *** FILL THE MISSING LINE ***\n    )\n    return df\n\ndef not_regex(pattern):\n        return r\"((?!{}).)\".format(pattern)\n\ndef remove_punctuation(df, colname):\n    df[colname] = df[colname].str.replace(\"\\n\", \" \")\n    df[colname] = df[colname].str.replace(\"\\r\", \" \")\n    alphanumeric_characters_extended = \"(\\\\b[-/]\\\\b|[a-zA-Z0-9])\"\n    df[colname] = df[colname].str.replace(not_regex(alphanumeric_characters_extended), \" \")\n    return df\n\ndef tokenize_sentence(df, colname):\n    df[colname] = df[colname].str.split()\n    return df\n\ndef remove_stop_words(df, colname):\n    stop_words = stopwords.words(\"english\")\n    df[colname] = df[colname].apply(lambda x: [word for word in x if word not in stop_words])\n    return df\n\ndef reverse_tokenize_sentence(df, colname):\n    df[colname] = df[colname].map(lambda word: \" \".join(word))\n    return df\n\n\ndef text_cleaning(df, colname):\n\"\"\"\n    Takes in a string of text, then performs the following:\n    1. convert text to lowercase\n    2. remove punctuation and new line characters \"\\n\"\n    3. Tokenize sentences\n    4. Remove all stopwords\n    5. convert tokenized text to text\n    \"\"\"\n    df = (\n        df\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n    )\n    return df\n</pre> def convert_text_to_lowercase(df, colname):     df[colname] = (         df[colname]         *** FILL THE MISSING LINE ***     )     return df  def not_regex(pattern):         return r\"((?!{}).)\".format(pattern)  def remove_punctuation(df, colname):     df[colname] = df[colname].str.replace(\"\\n\", \" \")     df[colname] = df[colname].str.replace(\"\\r\", \" \")     alphanumeric_characters_extended = \"(\\\\b[-/]\\\\b|[a-zA-Z0-9])\"     df[colname] = df[colname].str.replace(not_regex(alphanumeric_characters_extended), \" \")     return df  def tokenize_sentence(df, colname):     df[colname] = df[colname].str.split()     return df  def remove_stop_words(df, colname):     stop_words = stopwords.words(\"english\")     df[colname] = df[colname].apply(lambda x: [word for word in x if word not in stop_words])     return df  def reverse_tokenize_sentence(df, colname):     df[colname] = df[colname].map(lambda word: \" \".join(word))     return df   def text_cleaning(df, colname):     \"\"\"     Takes in a string of text, then performs the following:     1. convert text to lowercase     2. remove punctuation and new line characters \"\\n\"     3. Tokenize sentences     4. Remove all stopwords     5. convert tokenized text to text     \"\"\"     df = (         df         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***     )     return df In\u00a0[\u00a0]: Copied! <pre># Apply data cleaning\ndf_cleaned = text_cleaning(\n    *** FILL THE MISSING LINE ***\n    \"text\"\n)\n</pre> # Apply data cleaning df_cleaned = text_cleaning(     *** FILL THE MISSING LINE ***     \"text\" ) In\u00a0[\u00a0]: Copied! <pre># Control the cleaning\ndf_cleaned.head()\n</pre> # Control the cleaning df_cleaned.head() <p>We still have to transform the list of tokens to a fixed size numerical vector</p> <p>For that, we will use 2 very common techniques : CountVectorizer and TF-IDF</p> <p>1) CountVectorizer:</p> <p>CountVectorizer is used to convert a collection of text documents to a vector of token counts.</p> <p>Example:</p> <pre><code>[\"beer\", \"most\", \"tasty\", \"beer\", \"world\"]\n</code></pre> <p>Will be transformed into \u2b07</p> beer most tasty world 2 1 1 1 <p>In practice, you have to define a vocabulary size and each text will be transform into a vector of size [1 x vocabulary size]. Consequently, zeros will be added to the vector for each word present in the corpus vocabulary but missing in the specific review. The vocabulary space is defined using term frequency across the corpus: the most frequent words are kept.</p> <p>2) TF-IDF (optional):</p> <p>TF-IDF or term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to reflect how important a word is to a document in a collection or corpus.</p> <p>TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:</p> <pre><code>TF(t) = (Nbr of times term t appears in a document) / (Total nbr of terms in the\ndocument)\n</code></pre> <p>IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:</p> <pre><code>IDF(t) = log(Total number of documents / Number of documents with term t in it)\n</code></pre> <p>Example:</p> <p>Consider a document containing 100 words wherein the word book appears 3 times. The term frequency (i.e., tf) for cat is:</p> <pre><code>TF(t) = (Nbr of times term t appears in a document) / (Total nbr of terms in the\ndocument)\n      = 3 / 100\n      = 0.03\n</code></pre> <p>Now, assume we have 10 million documents and the word book appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as</p> <pre><code>IDF(t) = log(Total number of documents / Number of documents with term t in it)\n       = log(10,000,000 / 1,000)\n       = 4\n</code></pre> <p>Thus, the Tf-idf weight is the product of these quantities:</p> <pre><code>Tf-IDF = TF(t) * IDF(t)\n       = 0.03 * 4\n       = 0.12\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>*** FILL THE MISSING LINE ***\n*** FILL THE MISSING LINE ***\n\nx_train, x_test, y_train, y_test = train_test_split(\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n    test_size=0.2,\n    random_state=42)\n</pre> *** FILL THE MISSING LINE *** *** FILL THE MISSING LINE ***  x_train, x_test, y_train, y_test = train_test_split(     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE ***     test_size=0.2,     random_state=42) In\u00a0[\u00a0]: Copied! <pre># Define the vocabulary size to 100\ncount_vectorizer = CountVectorizer(\n    analyzer=\"word\",\n    *** FILL THE MISSING LINE ***\n    )\n</pre> # Define the vocabulary size to 100 count_vectorizer = CountVectorizer(     analyzer=\"word\",     *** FILL THE MISSING LINE ***     ) In\u00a0[\u00a0]: Copied! <pre># Apply the CountVectorizer and check the results on some rows\ncount_vectorizer.fit(x_train)\n\nx_train_features = count_vectorizer.transform(x_train).toarray()\nx_test_features = count_vectorizer.transform(x_test).toarray()\n</pre> # Apply the CountVectorizer and check the results on some rows count_vectorizer.fit(x_train)  x_train_features = count_vectorizer.transform(x_train).toarray() x_test_features = count_vectorizer.transform(x_test).toarray() In\u00a0[\u00a0]: Copied! <pre>x_train_features[0]\n</pre> x_train_features[0] In\u00a0[\u00a0]: Copied! <pre>x_train.iloc[0]\n</pre> x_train.iloc[0] <p>The next step is to define the model that will use in input the vectors from the the <code>CountVectorizer</code>. We will use a logistic regression model, using again the <code>scikit-learn</code> library.</p> <p>In order to produce cleaner code, we can combine these 2 steps (CountVectorizer and Logistic regression) into a single pipeline.</p> <ul> <li>Initialize the <code>CountVectorizer</code></li> <li>Initialize the <code>LogisticRegression</code></li> <li>Define your <code>Pipeline</code> object with these 2 steps</li> <li>Fit the <code>Pipeline</code></li> </ul> <p>Hint:</p> <ul> <li><code>sklearn.linear_model.LogisticRegression</code></li> <li><code>sklearn.pipeline.Pipeline</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre># Initialize the CountVectorizer\ncount_vectorizer = CountVectorizer(\n    analyzer=\"word\",\n    max_features=100\n    )\n\n# Initialize the logistic regression\nlogit = LogisticRegression(solver=\"lbfgs\", verbose=2, n_jobs=-1)\n\n# Combine them into a Pipeline object\npipeline_cv = Pipeline([\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n\n# Fit the Pipeline\npipeline_cv.fit(x_train, y_train)\n</pre> # Initialize the CountVectorizer count_vectorizer = CountVectorizer(     analyzer=\"word\",     max_features=100     )  # Initialize the logistic regression logit = LogisticRegression(solver=\"lbfgs\", verbose=2, n_jobs=-1)  # Combine them into a Pipeline object pipeline_cv = Pipeline([     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE ***  # Fit the Pipeline pipeline_cv.fit(x_train, y_train) <p>Now you can make predictions on the test set</p> In\u00a0[\u00a0]: Copied! <pre># predictions\ny_pred_cv = pipeline_cv.predict(x_test)\n</pre> # predictions y_pred_cv = pipeline_cv.predict(x_test) <p>How to evaluate our model ?</p> <p><code>Accuracy</code> is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations</p> In\u00a0[\u00a0]: Copied! <pre># Compute accuracy\nprint(f\"model accuracy : {accuracy_score(y_pred_cv, y_test)} %\")\n</pre> # Compute accuracy print(f\"model accuracy : {accuracy_score(y_pred_cv, y_test)} %\") <p>What is you opinion about the <code>accuracy</code> score ?</p> <p>One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model.</p> <p>One might think that if we have high accuracy, our model is the best. Yes, accuracy is an excellent measure, but only when you have balanced data (i.e. an equivalent representation of each class in the data).</p> <p>Let's do a test with a reference model to show how accuracy can be a source of error when evaluating a model: Create a model that predict everytime the most frequent class and compare the results.</p> In\u00a0[\u00a0]: Copied! <pre># Baseline model: predict the most-frequent class\ny_pred_baseline = [y_test.mode()[0]] * len(y_test)\n\n# Compute accuracy\nprint(f\"model baseline accuracy : {accuracy_score(y_pred_baseline, y_test)} %\")\n</pre> # Baseline model: predict the most-frequent class y_pred_baseline = [y_test.mode()[0]] * len(y_test)  # Compute accuracy print(f\"model baseline accuracy : {accuracy_score(y_pred_baseline, y_test)} %\") <p>Accuracy are closed, but the last model is completely idiot !</p> <p>In case of an imbalanced target (let's say 99% zeros), the accuracy of this dumb model will be 99% !</p> <p>Therefore, you need to look at other metrics to evaluate the performance of your model.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># Confusion matrices\nprint(f\"Confusion matrix of the first model: \\n {confusion_matrix(y_test, y_pred_cv)}\")\nprint(f\"Confusion matrix of the baseline model: \\n {confusion_matrix(y_test, y_pred_baseline)}\")\n</pre> # Confusion matrices print(f\"Confusion matrix of the first model: \\n {confusion_matrix(y_test, y_pred_cv)}\") print(f\"Confusion matrix of the baseline model: \\n {confusion_matrix(y_test, y_pred_baseline)}\") In\u00a0[\u00a0]: Copied! <pre># Evaluate the first model\nprint(f\"first model precision : {precision_score(y_pred_cv, y_test):.{3}f}%\")\nprint(f\"first model recall    : {recall_score(y_pred_cv, y_test)}%\")\nprint(f\"first model f1 score  : {f1_score(y_pred_cv, y_test):.{3}f}%\\n\")\n\n# Evaluate the baseline model\nprint(f\"baseline model precision : {precision_score(y_pred_baseline, y_test)}%\")\nprint(f\"baseline model recall    : {recall_score(y_pred_baseline, y_test)}%\")\nprint(f\"baseline model f1 score  : {f1_score(y_pred_baseline, y_test)}%\")\n</pre> # Evaluate the first model print(f\"first model precision : {precision_score(y_pred_cv, y_test):.{3}f}%\") print(f\"first model recall    : {recall_score(y_pred_cv, y_test)}%\") print(f\"first model f1 score  : {f1_score(y_pred_cv, y_test):.{3}f}%\\n\")  # Evaluate the baseline model print(f\"baseline model precision : {precision_score(y_pred_baseline, y_test)}%\") print(f\"baseline model recall    : {recall_score(y_pred_baseline, y_test)}%\") print(f\"baseline model f1 score  : {f1_score(y_pred_baseline, y_test)}%\") In\u00a0[\u00a0]: Copied! <pre># Classification report\nprint(classification_report(\n    *** FILL THE MISSING LINE ***\n))\n</pre> # Classification report print(classification_report(     *** FILL THE MISSING LINE *** )) In\u00a0[\u00a0]: Copied! <pre># Classification report\nprint(classification_report(y_test, y_pred_baseline))\n</pre> # Classification report print(classification_report(y_test, y_pred_baseline)) In\u00a0[\u00a0]: Copied! <pre># Initialize the TF-IDF\ntfidf_vectorizer = TfidfVectorizer(\n    analyzer='word',\n    *** FILL THE MISSING LINE ***\n    )\n\n# Apply the TfidfVectorizer and check the results on some rows\ntfidf_vectorizer.fit(x_train)\n\nx_train_features = tfidf_vectorizer.transform(x_train).toarray()\nx_test_features = tfidf_vectorizer.transform(x_test).toarray()\n</pre> # Initialize the TF-IDF tfidf_vectorizer = TfidfVectorizer(     analyzer='word',     *** FILL THE MISSING LINE ***     )  # Apply the TfidfVectorizer and check the results on some rows tfidf_vectorizer.fit(x_train)  x_train_features = tfidf_vectorizer.transform(x_train).toarray() x_test_features = tfidf_vectorizer.transform(x_test).toarray() In\u00a0[\u00a0]: Copied! <pre>x_train_features[0]\n</pre> x_train_features[0] In\u00a0[\u00a0]: Copied! <pre>x_train.iloc[0]\n</pre> x_train.iloc[0] <p>You can now combine the vectorizer to a logistic regression in a single pipeline</p> In\u00a0[\u00a0]: Copied! <pre># Initialize the TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(\n    analyzer='word',\n    max_features=100\n    )\n\n# Initialize the logistic regression\nlogit = LogisticRegression(solver='lbfgs', verbose=2, n_jobs=-1)\n\n# Combine them into a Pipeline object\npipeline_tfidf = Pipeline([\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n\n# Fit the Pipeline\npipeline_tfidf.fit(x_train, y_train)\n</pre> # Initialize the TfidfVectorizer tfidf_vectorizer = TfidfVectorizer(     analyzer='word',     max_features=100     )  # Initialize the logistic regression logit = LogisticRegression(solver='lbfgs', verbose=2, n_jobs=-1)  # Combine them into a Pipeline object pipeline_tfidf = Pipeline([     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE ***  # Fit the Pipeline pipeline_tfidf.fit(x_train, y_train) In\u00a0[\u00a0]: Copied! <pre># Predictions\ny_pred_tfidf = pipeline_tfidf.predict(x_test)\n\n# Evaluate the second model\nprint(f\"Confusion matrix of the first model: \\n {confusion_matrix(y_test, y_pred_tfidf)}\")\nprint(f\"second model precision : {precision_score(y_pred_tfidf, y_test):.{3}f}%\")\nprint(f\"second model recall    : {recall_score(y_pred_tfidf, y_test)}%\")\nprint(f\"second model f1 score  : {f1_score(y_pred_tfidf, y_test):.{3}f}%\\n\")\n</pre> # Predictions y_pred_tfidf = pipeline_tfidf.predict(x_test)  # Evaluate the second model print(f\"Confusion matrix of the first model: \\n {confusion_matrix(y_test, y_pred_tfidf)}\") print(f\"second model precision : {precision_score(y_pred_tfidf, y_test):.{3}f}%\") print(f\"second model recall    : {recall_score(y_pred_tfidf, y_test)}%\") print(f\"second model f1 score  : {f1_score(y_pred_tfidf, y_test):.{3}f}%\\n\") In\u00a0[\u00a0]: Copied! <pre># Classification report\nprint(classification_report(y_test, y_pred_tfidf))\n</pre> # Classification report print(classification_report(y_test, y_pred_tfidf)) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"10-Tutorials/4_classification_students/#practice-n4-classification-students-version","title":"\ud83d\udc0d Practice n\u00b04: classification (students version)\u00b6","text":"<p>The objective of this session is to learn about classification task. You will have to build a new model only using the text review.</p> <p>In order to apply classification models, we need to change the target definition. We will divide the reviews into good or bad using the rating.</p> <p>So our goal is to have a model, a function, that takes text as input and output a label/class (good review or bad review).</p> <p>$$f(text) = label$$</p> <p>The data are the same as those used for the regression (source ratebeer dataset description).</p> <p>Here are the main steps of the notebook :</p> <ol> <li>What is classification ?</li> <li>Focus on logistic regression</li> <li>Preparation</li> <li>Binary target definition</li> <li>Text cleaning</li> <li>Modelling</li> </ol> <ul> <li>6.1 First model using CountVectorizer</li> <li>6.2 Choose the right metrics</li> <li>6.3 Second model using TF-IDF</li> </ul>"},{"location":"10-Tutorials/4_classification_students/#1-what-is-classification","title":"1. What is classification ?\u00b6","text":"<p>Classification in machine learning consists of mathematical methods that allow to predict a discrete outcome (y) based on the value of one or more predictor variables (x).</p>"},{"location":"10-Tutorials/4_classification_students/#2-focus-on-logistic-regression","title":"2. Focus on logistic regression\u00b6","text":"<p>As seen in the last session, we can represent the link between the explicatives variables and the target to be predicted as follows</p> <p>$$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$$</p> <p>The difference here is that the target is not a continous variable (rating) but a discrete one (good review / bad review). If we limit ourselves to this model, the linear combination of each input will give an unbounded number that does not allow us to classify the review into good or bad.</p> <p>To transform the number provided by the linear combination into a classification, we use a function called <code>sigmoid function</code> which has the interesting property of transforming the numbers passed inside into numbers between 0 and 1.</p>"},{"location":"10-Tutorials/4_classification_students/#3-preparation","title":"3. Preparation\u00b6","text":""},{"location":"10-Tutorials/4_classification_students/#install-import-modules","title":"Install &amp; import modules\u00b6","text":""},{"location":"10-Tutorials/4_classification_students/#read-remote-dataset","title":"Read remote dataset\u00b6","text":""},{"location":"10-Tutorials/4_classification_students/#4-binary-target-definition","title":"4. Binary target definition\u00b6","text":"<p>Filter the data using only 1000 reviews and then explore some reviews</p>"},{"location":"10-Tutorials/4_classification_students/#5-text-cleaning","title":"5. Text cleaning\u00b6","text":""},{"location":"10-Tutorials/4_classification_students/#from-text-reviews-to-numerical-vectors","title":"From text reviews to numerical vectors\u00b6","text":"<p>Before training some models, the first step is to transform text into numbers :</p> <p>from</p> <pre><code>f(raw_text) = rating_classe\n</code></pre> <p>to</p> <pre><code>f(numerical_vector_representing_text) = rating_classe\n</code></pre> <p>Indeed, we can't direclty feed an algortihm with text.</p> <p>For example:</p> <pre><code>Wow, that beer is SOOOO good :O !!\n</code></pre> <p>must be transformed to something like:</p> <pre><code>[3, 4, 21, 0, 0, 8, 19]\n</code></pre> <p>where the values of the vector contain the meaning of the text. Knowing that the closer texts are in terms of meaning, the more closed their vector representation is too. Moreover, it is often more convenient to convert texts of different sizes into vectors of fixed size.</p> <p>For example:</p> <pre><code>\"Wow, that beer is SOOOO good :O !!\"\n-&gt; characters : 34\n-&gt; vector (1x7) : [3, 4, 21, 0, 0, 8, 19]\n</code></pre> <pre><code>\"This beer is very tasty\"\n-&gt; characters : 23\n-&gt; vector (1x7) : [3, 4, 20, 0, 0, 7, 19]\n</code></pre> <p>But:</p> <pre><code>\"It's not a beer, just motor oil at best.\"\n-&gt; characters : 40\n-&gt; vector (1x7) : [0, 4, 1, 12, 14, 0, 0]\n</code></pre>"},{"location":"10-Tutorials/4_classification_students/#from-raw-text-reviews-to-clean-list-of-words","title":"From raw text reviews to clean list of words\u00b6","text":"<p>Before converting text to numerical vector, the first step is to clean the text to keep only the pertinent information.</p> <p>Here are the following cleaning steps that we will apply on the reviews:</p> <ol> <li>Convert letter to lowercase</li> </ol> <pre><code>\"Wow, that beer is SOOOO good :O !!\" -&gt; \"wow, that beer is soooo good :o !!\"\n</code></pre> <ol> <li>Remove the punctuation letter to lowercase</li> </ol> <pre><code>\"wow, that beer is soooo good :o !!\" -&gt; \"wow that beer is soooo good\"\n</code></pre> <ol> <li>Transform the text into tokens</li> </ol> <pre><code>\"wow that beer is soooo good\" -&gt; [\"wow\", \"that\", \"beer\", \"is\", \"soooo\", \"good\"]\n</code></pre> <ol> <li>Remove the stopwords, the most common english words that often bring noise to the models.</li> </ol> <pre><code>[\"wow\", \"that\", \"beer\", \"is\", \"soooo\", \"good\"] -&gt; [\"wow\", \"beer\", \"soooo\", \"good\"]\n</code></pre> <ol> <li>To go further, some techniques can be used to reduce the forms of each word into a common base or root. This can be done with:</li> </ol> <p>(1) Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.</p> <p>am, are, is  $\\Rightarrow$  be</p> <p>(2) Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma.</p> <p>book, books, book's, book'  $\\Rightarrow$  book</p> <p>The steps presented here are just the most basic, many different things can be applied to the cleaning part of the text.</p>"},{"location":"10-Tutorials/4_classification_students/#split-the-data-into-traintest-sets","title":"Split the data into train/test sets\u00b6","text":"<p>Before applying these transormation on the text, we will just split the data for the modelling part.</p> <p>Keep 20% of the data for the test dataset</p>"},{"location":"10-Tutorials/4_classification_students/#6-modelling","title":"6. Modelling\u00b6","text":""},{"location":"10-Tutorials/4_classification_students/#61-first-model-using-countvectorizer","title":"6.1 First model using CountVectorizer\u00b6","text":"<p>Transform the text reviews into numerical vectors by counting the number of words in each reviews. Use the <code>scikit-learn</code> library.</p> <p>In order not to bring information from the train set into the test set, you must train the <code>CountVectorizer</code> on the train set and apply it to the test set.</p> <p>Hint:</p> <ul> <li><code>sklearn.feature_extraction.text.CountVectorizer</code></li> </ul>"},{"location":"10-Tutorials/4_classification_students/#62-choose-the-right-metrics","title":"6.2 Choose the right metrics\u00b6","text":"<p>We need now define other metrics to evaluate our model.</p> <ul> <li><p>True Positives (TP): these are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.</p> </li> <li><p>True Negatives (TN): these are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.</p> </li> <li><p>False Positives (FP): when actual class is no and predicted class is yes.</p> </li> <li><p>False Negatives (FN): when actual class is yes but predicted class in no.</p> </li> <li><p>Accuracy: the ratio of correctly predicted observation to the total observations.</p> </li> </ul> <p>Accuracy = $\\frac{TP+TN}{TP+FP+FN+TN}$</p> <ul> <li>Precision: Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. High precision relates to the low false positive rate.</li> </ul> <p>Precision = $\\frac{TP}{TP+FP}$</p> <ul> <li>Recall (Sensitivity): Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes.</li> </ul> <p>Recall = $\\frac{TP}{TP+FN}$</p> <ul> <li>F1 score: F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution.</li> </ul> <p>F1 Score = $2* \\frac{Recall * Precision}{Recall + Precision}$</p> <p>Let's now compare again our 2 models !</p> <p>First, plot the confusion matrix, the precision, recall and f1 score for each model. You can also print the classification report of scikit learn that sums up the main classification metrics.</p> <p>Hint:</p> <ul> <li><code>sklearn.metrics.confusion_matrix</code></li> <li><code>sklearn.metrics.recall_score</code></li> <li><code>sklearn.metrics.precision_score</code></li> <li><code>sklearn.metrics.f1_score</code></li> <li><code>sklearn.metrics.classification_report</code></li> </ul>"},{"location":"10-Tutorials/4_classification_students/#63-second-model-using-tf-idf-vectorizer-optional","title":"6.3 Second model using TF-IDF Vectorizer (optional)\u00b6","text":"<p>In this last section, you will use a better approach in term of vectorization: TF-IDF</p> <p>Scikit-learn provide the <code>TfidfVectorizer</code> that can be used in the same way as <code>CountVectorizer</code>.</p> <p>Hint:</p> <ul> <li><code>sklearn.feature_extraction.text.TfidfVectorizer</code></li> </ul>"},{"location":"10-Tutorials/4_classification_students/#64-optimize-the-model","title":"6.4 Optimize the model\u00b6","text":"<p>You can now try to optimize the model by changing a lot of parameters:</p> <ul> <li>Take more reviews in input</li> <li>Increase the <code>max_features</code> parameter</li> <li>Remove the most frequent words</li> <li>Try adding n-grams to the vectorizer</li> <li>Improve the text cleaning</li> <li>etc...</li> </ul>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning/","title":"\ud83d\udc0d Practice n\u00b05: cross validation &amp; hyperparameter tuning","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn import (\n    pipeline,\n    metrics,\n    linear_model,\n    model_selection,\n    tree\n)\npd.set_option(\"display.max_colwidth\", None)\n</pre> import pandas as pd import numpy as np from sklearn import (     pipeline,     metrics,     linear_model,     model_selection,     tree ) pd.set_option(\"display.max_colwidth\", None) In\u00a0[2]: Copied! <pre>file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\"\ndf = pd.read_parquet(file_url)\n</pre> file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\" df = pd.read_parquet(file_url) In\u00a0[3]: Copied! <pre>df.head()\n</pre> df.head() Out[3]: beer brewery alcohol type rating_appearance rating_aroma rating_palate rating_taste rating timestamp user text beer_degree brewery_degree user_degree text_length date is_good 0 Breckenridge Oatmeal Stout 383 4.95 Stout 4 7 4 7 14 1217462400 blutt59 bottle, oat nose with black color, bitter chocolate flavor with a good coffee cocoa finish 51 413 300 90 2008-07-31 02:00:00 0 1 Breckenridge 471 Small Batch Imperial Porter 383 7.50 Imperial/Strong Porter 3 8 3 8 14 1312588800 blutt59 bottle, received in trade, dark brown with garnet highlights, short lived foam, aroma is quite port like and raisin, flavor is sweet raisin or prune, toasty almond, waffle batter, nice finish 4 413 300 191 2011-08-06 02:00:00 0 2 Breckenridge Avalanche Amber 383 5.41 Amber Ale 3 5 3 5 10 1205020800 blutt59 12 oz. bottle, amber color with soapy head, slight caramel flavor but watery not much hops some effervescence 43 413 300 109 2008-03-09 01:00:00 0 3 Breckenridge Lucky U IPA 383 6.20 India Pale Ale (IPA) 3 6 3 7 12 1255737600 blutt59 bottle, golden orange color with light tan foam , citric soour aroma with hoppy citrus and lime bitter flavor, could use a little more malt for balance 20 413 300 151 2009-10-17 02:00:00 0 4 Fullers Vintage Ale 2009 55 8.50 English Strong Ale 3 7 3 8 14 1282003200 blutt59 bottle, thanks to SS, almond amber colored pour with off white soapy foam, fruity metallic aroma, flavors of dried apricots, metallics, fruity medicinal finish 18 978 300 159 2010-08-17 02:00:00 0 In\u00a0[4]: Copied! <pre>target = \"is_good\"  # LINE TO BE REMOVED FOR STUDENTS\nfeatures = [\n    \"alcohol\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    df[features],\n    df[target],\n    test_size=0.2,  # LINE TO BE REMOVED FOR STUDENTS\n    random_state=42\n)\n</pre> target = \"is_good\"  # LINE TO BE REMOVED FOR STUDENTS features = [     \"alcohol\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ]  X_train, X_test, y_train, y_test = model_selection.train_test_split(     df[features],     df[target],     test_size=0.2,  # LINE TO BE REMOVED FOR STUDENTS     random_state=42 ) In\u00a0[5]: Copied! <pre>pipe = pipeline.make_pipeline(\n    linear_model.LogisticRegression(solver=\"liblinear\"),\n)\n\npipe.fit(\n    X_train,  # LINE TO BE REMOVED FOR STUDENTS\n    y_train  # LINE TO BE REMOVED FOR STUDENTS\n)\n\ny_pred = pipe.predict(X_test)\n</pre> pipe = pipeline.make_pipeline(     linear_model.LogisticRegression(solver=\"liblinear\"), )  pipe.fit(     X_train,  # LINE TO BE REMOVED FOR STUDENTS     y_train  # LINE TO BE REMOVED FOR STUDENTS )  y_pred = pipe.predict(X_test) In\u00a0[6]: Copied! <pre>metrics.confusion_matrix(y_pred, y_test)\n</pre> metrics.confusion_matrix(y_pred, y_test) Out[6]: <pre>array([[28111,  3314],\n       [ 2061,  6514]])</pre> In\u00a0[7]: Copied! <pre>print(metrics.classification_report(y_pred, y_test))\n</pre> print(metrics.classification_report(y_pred, y_test)) <pre>              precision    recall  f1-score   support\n\n           0       0.93      0.89      0.91     31425\n           1       0.66      0.76      0.71      8575\n\n    accuracy                           0.87     40000\n   macro avg       0.80      0.83      0.81     40000\nweighted avg       0.87      0.87      0.87     40000\n\n</pre> <p>Let's use the F1 score metric for evaluation Compute the score on the test set.</p> In\u00a0[8]: Copied! <pre>metrics.f1_score(y_test, y_pred)  # LINE TO BE REMOVED FOR STUDENTS\n</pre> metrics.f1_score(y_test, y_pred)  # LINE TO BE REMOVED FOR STUDENTS Out[8]: <pre>0.70792805520839</pre> <p>Let's try to see which parameters of the Logistic Regression reach the best F1-score on the test set.</p> In\u00a0[9]: Copied! <pre>def compute_test_score(pipe, reg, c):\n    pipe[-1].set_params(penalty=reg)\n    pipe[-1].set_params(C=c)\n    pipe.fit(\n        X_train,\n        y_train\n    )\n    y_pred = pipe.predict(X_test)\n    return (\n        metrics.f1_score(y_test, y_pred)\n    )\n</pre>   def compute_test_score(pipe, reg, c):     pipe[-1].set_params(penalty=reg)     pipe[-1].set_params(C=c)     pipe.fit(         X_train,         y_train     )     y_pred = pipe.predict(X_test)     return (         metrics.f1_score(y_test, y_pred)     ) <p>Find the best combination of penalty method and strength to maximize the F1-score on the test set.</p> In\u00a0[10]: Copied! <pre>compute_test_score(\n    pipe,  # LINE TO BE REMOVED FOR STUDENTS\n    \"l1\",  # LINE TO BE REMOVED FOR STUDENTS\n    10  # LINE TO BE REMOVED FOR STUDENTS\n)\n</pre> compute_test_score(     pipe,  # LINE TO BE REMOVED FOR STUDENTS     \"l1\",  # LINE TO BE REMOVED FOR STUDENTS     10  # LINE TO BE REMOVED FOR STUDENTS ) Out[10]: <pre>0.7079847908745247</pre> <p>TODO: add the concept of validation set before introducting cross validation</p> <p></p> <p></p> <p>Let's implement a grid search with a 3-fold cross validation to optimize our Logistic regression hyperparameters</p> In\u00a0[11]: Copied! <pre>pipe.get_params()\n</pre> pipe.get_params() Out[11]: <pre>{'memory': None,\n 'steps': [('logisticregression',\n   LogisticRegression(C=10, penalty='l1', solver='liblinear'))],\n 'verbose': False,\n 'logisticregression': LogisticRegression(C=10, penalty='l1', solver='liblinear'),\n 'logisticregression__C': 10,\n 'logisticregression__class_weight': None,\n 'logisticregression__dual': False,\n 'logisticregression__fit_intercept': True,\n 'logisticregression__intercept_scaling': 1,\n 'logisticregression__l1_ratio': None,\n 'logisticregression__max_iter': 100,\n 'logisticregression__multi_class': 'auto',\n 'logisticregression__n_jobs': None,\n 'logisticregression__penalty': 'l1',\n 'logisticregression__random_state': None,\n 'logisticregression__solver': 'liblinear',\n 'logisticregression__tol': 0.0001,\n 'logisticregression__verbose': 0,\n 'logisticregression__warm_start': False}</pre> In\u00a0[12]: Copied! <pre>regularization_types = [\"l1\", \"l2\"]\nregularization_strengths = [0.0001, 0.01, 1, 100, 10000]\n\ngridsearch = model_selection.GridSearchCV(\n    pipe,\n    param_grid={\n        # parameters to explore, with the syntax \"pipelinestep__parameter\"\n        # (e.g: logisticregression__penalty)\n        # use pipe.get_params() to know which parameters are available in the pipeline\n        \"logisticregression__C\": regularization_strengths,  # LINE TO BE REMOVED FOR STUDENTS\n        \"logisticregression__penalty\": regularization_types,  # LINE TO BE REMOVED FOR STUDENTS\n    },\n    # scoring function\n    scoring=\"f1\",  # LINE TO BE REMOVED FOR STUDENTS\n    # K-fold cross-validation parameter\n    cv=3,  # LINE TO BE REMOVED FOR STUDENTS\n    n_jobs=-1,\n    refit=True,\n    return_train_score=True,\n    verbose=1,\n)\ngridsearch.fit(X_train, y_train)\n</pre> regularization_types = [\"l1\", \"l2\"] regularization_strengths = [0.0001, 0.01, 1, 100, 10000]  gridsearch = model_selection.GridSearchCV(     pipe,     param_grid={         # parameters to explore, with the syntax \"pipelinestep__parameter\"         # (e.g: logisticregression__penalty)         # use pipe.get_params() to know which parameters are available in the pipeline         \"logisticregression__C\": regularization_strengths,  # LINE TO BE REMOVED FOR STUDENTS         \"logisticregression__penalty\": regularization_types,  # LINE TO BE REMOVED FOR STUDENTS     },     # scoring function     scoring=\"f1\",  # LINE TO BE REMOVED FOR STUDENTS     # K-fold cross-validation parameter     cv=3,  # LINE TO BE REMOVED FOR STUDENTS     n_jobs=-1,     refit=True,     return_train_score=True,     verbose=1, ) gridsearch.fit(X_train, y_train) <pre>Fitting 3 folds for each of 10 candidates, totalling 30 fits\n</pre> Out[12]: <pre>GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('logisticregression',\n                                        LogisticRegression(C=10, penalty='l1',\n                                                           solver='liblinear'))]),\n             n_jobs=-1,\n             param_grid={'logisticregression__C': [0.0001, 0.01, 1, 100, 10000],\n                         'logisticregression__penalty': ['l1', 'l2']},\n             return_train_score=True, scoring='f1', verbose=1)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV<pre>GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('logisticregression',\n                                        LogisticRegression(C=10, penalty='l1',\n                                                           solver='liblinear'))]),\n             n_jobs=-1,\n             param_grid={'logisticregression__C': [0.0001, 0.01, 1, 100, 10000],\n                         'logisticregression__penalty': ['l1', 'l2']},\n             return_train_score=True, scoring='f1', verbose=1)</pre>estimator: Pipeline<pre>Pipeline(steps=[('logisticregression',\n                 LogisticRegression(C=10, penalty='l1', solver='liblinear'))])</pre>LogisticRegression<pre>LogisticRegression(C=10, penalty='l1', solver='liblinear')</pre> In\u00a0[13]: Copied! <pre>(\n    pd.DataFrame(gridsearch.cv_results_)\n    .sort_values(by=\"rank_test_score\")\n    .drop(\"params\", axis=1)\n    .style.background_gradient()\n)\n</pre> (     pd.DataFrame(gridsearch.cv_results_)     .sort_values(by=\"rank_test_score\")     .drop(\"params\", axis=1)     .style.background_gradient() ) Out[13]: mean_fit_time std_fit_time mean_score_time std_score_time param_logisticregression__C param_logisticregression__penalty split0_test_score split1_test_score split2_test_score mean_test_score std_test_score rank_test_score split0_train_score split1_train_score split2_train_score mean_train_score std_train_score 7 0.302523 0.014409 0.027974 0.012377 100 l2 0.712707 0.713193 0.708231 0.711377 0.002233 1 0.711017 0.710367 0.713287 0.711557 0.001252 9 0.324879 0.013821 0.030534 0.014661 10000 l2 0.712707 0.713193 0.708231 0.711377 0.002233 1 0.711017 0.710367 0.713287 0.711557 0.001252 5 0.306638 0.003537 0.028256 0.012410 1 l2 0.712939 0.713086 0.708036 0.711354 0.002346 3 0.710612 0.710109 0.713219 0.711313 0.001363 4 2.994843 0.098568 0.028198 0.014128 1 l1 0.712601 0.713193 0.708245 0.711346 0.002206 4 0.710979 0.710367 0.713352 0.711566 0.001287 8 3.025996 0.038840 0.026668 0.012305 10000 l1 0.712601 0.713164 0.708269 0.711345 0.002187 5 0.710979 0.710353 0.713364 0.711565 0.001297 6 3.007619 0.046535 0.028817 0.014680 100 l1 0.712601 0.713164 0.708269 0.711345 0.002187 5 0.710979 0.710338 0.713364 0.711560 0.001302 2 2.724853 0.040249 0.027870 0.013004 0.010000 l1 0.709125 0.711334 0.706343 0.708934 0.002042 7 0.706986 0.708851 0.711213 0.709017 0.001730 3 0.253460 0.007172 0.034018 0.016331 0.010000 l2 0.670664 0.672511 0.668110 0.670428 0.001805 8 0.670319 0.669710 0.671176 0.670402 0.000602 1 0.154929 0.006636 0.035645 0.007476 0.000100 l2 0.001232 0.001540 0.000616 0.001129 0.000384 9 0.001232 0.000847 0.001540 0.001206 0.000283 0 0.275470 0.008802 0.024821 0.007042 0.000100 l1 0.000000 0.000000 0.000000 0.000000 0.000000 10 0.000000 0.000000 0.000000 0.000000 0.000000 <p>Now we can take our best estimator and compute its score on the test set.</p> In\u00a0[14]: Copied! <pre>optimized_pipe = gridsearch.best_estimator_\n\ny_pred = (\n    optimized_pipe\n    .predict(X_test)  # LINE TO BE REMOVED FOR STUDENTS\n)\n\nmetrics.f1_score(y_test, y_pred)\n</pre> optimized_pipe = gridsearch.best_estimator_  y_pred = (     optimized_pipe     .predict(X_test)  # LINE TO BE REMOVED FOR STUDENTS )  metrics.f1_score(y_test, y_pred) Out[14]: <pre>0.7079847908745247</pre> <p>Now it's your turn ! Choose a classifier and use the sklearn documentation to understand its hyperparameters. Then, use the code above to optimize these hyperparameters using a gridsearch and crossvalidation.</p> In\u00a0[15]: Copied! <pre>pipe = pipeline.make_pipeline(\n    tree.DecisionTreeClassifier(),  # LINE TO BE REMOVED FOR STUDENTS\n)\n\nmax_depths = [3, 5, 10, 20]  # LINE TO BE REMOVED FOR STUDENTS\nclass_weights = [None, \"balanced\"]  # LINE TO BE REMOVED FOR STUDENTS\n\nscoring = \"f1\"\ngridsearch = model_selection.GridSearchCV(\n    pipe,\n    param_grid={\n        \"decisiontreeclassifier__max_depth\": max_depths,  # LINE TO BE REMOVED FOR STUDENTS\n        \"decisiontreeclassifier__class_weight\": class_weights,  # LINE TO BE REMOVED FOR STUDENTS\n    },\n    scoring=scoring,\n    cv=3,\n    n_jobs=-1,\n    refit=True,\n    return_train_score=True,\n    verbose=1,\n)\ngridsearch.fit(X_train, y_train)\n</pre> pipe = pipeline.make_pipeline(     tree.DecisionTreeClassifier(),  # LINE TO BE REMOVED FOR STUDENTS )  max_depths = [3, 5, 10, 20]  # LINE TO BE REMOVED FOR STUDENTS class_weights = [None, \"balanced\"]  # LINE TO BE REMOVED FOR STUDENTS  scoring = \"f1\" gridsearch = model_selection.GridSearchCV(     pipe,     param_grid={         \"decisiontreeclassifier__max_depth\": max_depths,  # LINE TO BE REMOVED FOR STUDENTS         \"decisiontreeclassifier__class_weight\": class_weights,  # LINE TO BE REMOVED FOR STUDENTS     },     scoring=scoring,     cv=3,     n_jobs=-1,     refit=True,     return_train_score=True,     verbose=1, ) gridsearch.fit(X_train, y_train) <pre>Fitting 3 folds for each of 8 candidates, totalling 24 fits\n</pre> Out[15]: <pre>GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('decisiontreeclassifier',\n                                        DecisionTreeClassifier())]),\n             n_jobs=-1,\n             param_grid={'decisiontreeclassifier__class_weight': [None,\n                                                                  'balanced'],\n                         'decisiontreeclassifier__max_depth': [3, 5, 10, 20]},\n             return_train_score=True, scoring='f1', verbose=1)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV<pre>GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('decisiontreeclassifier',\n                                        DecisionTreeClassifier())]),\n             n_jobs=-1,\n             param_grid={'decisiontreeclassifier__class_weight': [None,\n                                                                  'balanced'],\n                         'decisiontreeclassifier__max_depth': [3, 5, 10, 20]},\n             return_train_score=True, scoring='f1', verbose=1)</pre>estimator: Pipeline<pre>Pipeline(steps=[('decisiontreeclassifier', DecisionTreeClassifier())])</pre>DecisionTreeClassifier<pre>DecisionTreeClassifier()</pre> In\u00a0[16]: Copied! <pre>(\n    pd.DataFrame(gridsearch.cv_results_)\n    .sort_values(by=\"rank_test_score\")\n    .drop(\"params\", axis=1)\n    .style.background_gradient()\n)\n</pre> (     pd.DataFrame(gridsearch.cv_results_)     .sort_values(by=\"rank_test_score\")     .drop(\"params\", axis=1)     .style.background_gradient() ) Out[16]: mean_fit_time std_fit_time mean_score_time std_score_time param_decisiontreeclassifier__class_weight param_decisiontreeclassifier__max_depth split0_test_score split1_test_score split2_test_score mean_test_score std_test_score rank_test_score split0_train_score split1_train_score split2_train_score mean_train_score std_train_score 1 0.089219 0.005742 0.030550 0.010181 None 5 0.714246 0.720191 0.715334 0.716590 0.002585 1 0.715069 0.716594 0.719009 0.716890 0.001622 6 0.158087 0.003758 0.029076 0.011842 balanced 10 0.707766 0.705384 0.706907 0.706686 0.000985 2 0.717082 0.715781 0.719324 0.717396 0.001463 2 0.125592 0.003296 0.033634 0.014105 None 10 0.701359 0.709018 0.706432 0.705603 0.003181 3 0.716454 0.719820 0.725999 0.720758 0.003953 5 0.112411 0.003879 0.026811 0.012117 balanced 5 0.701048 0.705499 0.699621 0.702056 0.002503 4 0.704027 0.701759 0.701387 0.702391 0.001166 4 0.095171 0.004111 0.028576 0.012639 balanced 3 0.676007 0.713444 0.712545 0.700666 0.017440 5 0.679652 0.713050 0.713499 0.702067 0.015851 7 0.189083 0.005064 0.033745 0.012654 balanced 20 0.690135 0.687700 0.688462 0.688766 0.001017 6 0.749428 0.748467 0.748198 0.748698 0.000528 3 0.172290 0.014766 0.042265 0.008551 None 20 0.681040 0.680347 0.680676 0.680687 0.000283 7 0.753875 0.751615 0.754406 0.753299 0.001210 0 0.066172 0.004491 0.029563 0.013022 None 3 0.662919 0.665324 0.661313 0.663185 0.001648 8 0.663316 0.662115 0.664121 0.663184 0.000824 In\u00a0[17]: Copied! <pre>optimized_pipe = gridsearch.best_estimator_\n\ny_pred = optimized_pipe.predict(X_test)\n\nmetrics.f1_score(y_test, y_pred)\n</pre> optimized_pipe = gridsearch.best_estimator_  y_pred = optimized_pipe.predict(X_test)  metrics.f1_score(y_test, y_pred) Out[17]: <pre>0.717235566793089</pre>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning/#practice-n5-cross-validation-hyperparameter-tuning","title":"\ud83d\udc0d Practice n\u00b05: cross validation &amp; hyperparameter tuning\u00b6","text":"<p>The objective of this session is to optimize an end-to-end machine learning pipeline from feature engineering to model training and evaluation. Some notions have already been tackled during session 1 or 2, the idea of this notebook is to have a focus on cross validation and hyperparameters tuning.</p> <p>We will remain in the frame of the binary classification of the session 2. Our goal is to have a model that outputs a label/class (good review or bad review).</p> <p>Here are the main steps of the notebook :</p> <ol> <li>Preparation</li> <li>Data split</li> <li>Model training and evaluation</li> <li>Manual hyperparameter tuning</li> <li>Cross validation &amp; Automated hyperparameters tuning</li> </ol>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning/#preparation","title":"Preparation\u00b6","text":""},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning/#install-import-modules","title":"Install &amp; import modules\u00b6","text":""},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning/#read-remote-dataset","title":"Read remote dataset\u00b6","text":""},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning/#splits-traintest-featurestarget","title":"Splits: Train/test &amp; features/target\u00b6","text":"<p>Split the data keeping 20% in the test set.</p>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning/#model-training-and-evaluation","title":"Model training and evaluation\u00b6","text":"<p>Train a logistic regression on the train set</p>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning/#manual-hyperparameter-tuning","title":"Manual hyperparameter tuning\u00b6","text":""},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning/#cross-validation","title":"Cross validation\u00b6","text":"<p>When evaluating different settings (\u201chyperparameters\u201d) for estimators, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can \u201cleak\u201d into the model and evaluation metrics no longer report on generalization performance.</p> <p>A solution to this problem is a procedure called cross-validation. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k \u201cfolds\u201d:</p> <ul> <li>A model is trained using k - 1 of the folds as training data</li> <li>The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy)</li> </ul> <p>The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop.</p>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning/#automated-hyperparameters-tuning","title":"Automated hyperparameters tuning\u00b6","text":"<p>As the manual tuning of hyperparameters can take a long time, some solutions exist to help you find the best combination.</p>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning_students/","title":"\ud83d\udc0d Practice n\u00b05: cross validation &amp; hyperparameter tuning (students version)","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn import (\n    pipeline,\n    metrics,\n    linear_model,\n    model_selection,\n    tree\n)\npd.set_option(\"display.max_colwidth\", None)\n</pre> import pandas as pd import numpy as np from sklearn import (     pipeline,     metrics,     linear_model,     model_selection,     tree ) pd.set_option(\"display.max_colwidth\", None) In\u00a0[\u00a0]: Copied! <pre>file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\"\ndf = pd.read_parquet(file_url)\n</pre> file_url = \"https://github.com/ML-boot-camp/ratebeer/raw/master/data/ratebeer_sample_enriched.parquet\" df = pd.read_parquet(file_url) In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() In\u00a0[\u00a0]: Copied! <pre>*** FILL THE MISSING LINE ***\nfeatures = [\n    \"alcohol\",\n    \"rating_appearance\",\n    \"rating_aroma\",\n    \"rating_palate\",\n    \"rating_taste\"\n]\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    df[features],\n    df[target],\n    *** FILL THE MISSING LINE ***\n    random_state=42\n)\n</pre> *** FILL THE MISSING LINE *** features = [     \"alcohol\",     \"rating_appearance\",     \"rating_aroma\",     \"rating_palate\",     \"rating_taste\" ]  X_train, X_test, y_train, y_test = model_selection.train_test_split(     df[features],     df[target],     *** FILL THE MISSING LINE ***     random_state=42 ) In\u00a0[\u00a0]: Copied! <pre>pipe = pipeline.make_pipeline(\n    linear_model.LogisticRegression(solver=\"liblinear\"),\n)\n\npipe.fit(\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n)\n\ny_pred = pipe.predict(X_test)\n</pre> pipe = pipeline.make_pipeline(     linear_model.LogisticRegression(solver=\"liblinear\"), )  pipe.fit(     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** )  y_pred = pipe.predict(X_test) In\u00a0[\u00a0]: Copied! <pre>metrics.confusion_matrix(y_pred, y_test)\n</pre> metrics.confusion_matrix(y_pred, y_test) In\u00a0[\u00a0]: Copied! <pre>print(metrics.classification_report(y_pred, y_test))\n</pre> print(metrics.classification_report(y_pred, y_test)) <p>Let's use the F1 score metric for evaluation Compute the score on the test set.</p> In\u00a0[\u00a0]: Copied! <pre>*** FILL THE MISSING LINE ***\n</pre> *** FILL THE MISSING LINE *** <p>Let's try to see which parameters of the Logistic Regression reach the best F1-score on the test set.</p> In\u00a0[\u00a0]: Copied! <pre>def compute_test_score(pipe, reg, c):\n    pipe[-1].set_params(penalty=reg)\n    pipe[-1].set_params(C=c)\n    pipe.fit(\n        X_train,\n        y_train\n    )\n    y_pred = pipe.predict(X_test)\n    return (\n        metrics.f1_score(y_test, y_pred)\n    )\n</pre>   def compute_test_score(pipe, reg, c):     pipe[-1].set_params(penalty=reg)     pipe[-1].set_params(C=c)     pipe.fit(         X_train,         y_train     )     y_pred = pipe.predict(X_test)     return (         metrics.f1_score(y_test, y_pred)     ) <p>Find the best combination of penalty method and strength to maximize the F1-score on the test set.</p> In\u00a0[\u00a0]: Copied! <pre>compute_test_score(\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n    *** FILL THE MISSING LINE ***\n)\n</pre> compute_test_score(     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE ***     *** FILL THE MISSING LINE *** ) <p>TODO: add the concept of validation set before introducting cross validation</p> <p></p> <p></p> <p>Let's implement a grid search with a 3-fold cross validation to optimize our Logistic regression hyperparameters</p> In\u00a0[\u00a0]: Copied! <pre>pipe.get_params()\n</pre> pipe.get_params() In\u00a0[\u00a0]: Copied! <pre>regularization_types = [\"l1\", \"l2\"]\nregularization_strengths = [0.0001, 0.01, 1, 100, 10000]\n\ngridsearch = model_selection.GridSearchCV(\n    pipe,\n    param_grid={\n        # parameters to explore, with the syntax \"pipelinestep__parameter\"\n        # (e.g: logisticregression__penalty)\n        # use pipe.get_params() to know which parameters are available in the pipeline\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n    },\n    # scoring function\n    *** FILL THE MISSING LINE ***\n    # K-fold cross-validation parameter\n    *** FILL THE MISSING LINE ***\n    n_jobs=-1,\n    refit=True,\n    return_train_score=True,\n    verbose=1,\n)\ngridsearch.fit(X_train, y_train)\n</pre> regularization_types = [\"l1\", \"l2\"] regularization_strengths = [0.0001, 0.01, 1, 100, 10000]  gridsearch = model_selection.GridSearchCV(     pipe,     param_grid={         # parameters to explore, with the syntax \"pipelinestep__parameter\"         # (e.g: logisticregression__penalty)         # use pipe.get_params() to know which parameters are available in the pipeline         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***     },     # scoring function     *** FILL THE MISSING LINE ***     # K-fold cross-validation parameter     *** FILL THE MISSING LINE ***     n_jobs=-1,     refit=True,     return_train_score=True,     verbose=1, ) gridsearch.fit(X_train, y_train) In\u00a0[\u00a0]: Copied! <pre>(\n    pd.DataFrame(gridsearch.cv_results_)\n    .sort_values(by=\"rank_test_score\")\n    .drop(\"params\", axis=1)\n    .style.background_gradient()\n)\n</pre> (     pd.DataFrame(gridsearch.cv_results_)     .sort_values(by=\"rank_test_score\")     .drop(\"params\", axis=1)     .style.background_gradient() ) <p>Now we can take our best estimator and compute its score on the test set.</p> In\u00a0[\u00a0]: Copied! <pre>optimized_pipe = gridsearch.best_estimator_\n\ny_pred = (\n    optimized_pipe\n    *** FILL THE MISSING LINE ***\n)\n\nmetrics.f1_score(y_test, y_pred)\n</pre> optimized_pipe = gridsearch.best_estimator_  y_pred = (     optimized_pipe     *** FILL THE MISSING LINE *** )  metrics.f1_score(y_test, y_pred) <p>Now it's your turn ! Choose a classifier and use the sklearn documentation to understand its hyperparameters. Then, use the code above to optimize these hyperparameters using a gridsearch and crossvalidation.</p> In\u00a0[\u00a0]: Copied! <pre>pipe = pipeline.make_pipeline(\n    *** FILL THE MISSING LINE ***\n)\n\n*** FILL THE MISSING LINE ***\n*** FILL THE MISSING LINE ***\n\nscoring = \"f1\"\ngridsearch = model_selection.GridSearchCV(\n    pipe,\n    param_grid={\n        *** FILL THE MISSING LINE ***\n        *** FILL THE MISSING LINE ***\n    },\n    scoring=scoring,\n    cv=3,\n    n_jobs=-1,\n    refit=True,\n    return_train_score=True,\n    verbose=1,\n)\ngridsearch.fit(X_train, y_train)\n</pre> pipe = pipeline.make_pipeline(     *** FILL THE MISSING LINE *** )  *** FILL THE MISSING LINE *** *** FILL THE MISSING LINE ***  scoring = \"f1\" gridsearch = model_selection.GridSearchCV(     pipe,     param_grid={         *** FILL THE MISSING LINE ***         *** FILL THE MISSING LINE ***     },     scoring=scoring,     cv=3,     n_jobs=-1,     refit=True,     return_train_score=True,     verbose=1, ) gridsearch.fit(X_train, y_train) In\u00a0[\u00a0]: Copied! <pre>(\n    pd.DataFrame(gridsearch.cv_results_)\n    .sort_values(by=\"rank_test_score\")\n    .drop(\"params\", axis=1)\n    .style.background_gradient()\n)\n</pre> (     pd.DataFrame(gridsearch.cv_results_)     .sort_values(by=\"rank_test_score\")     .drop(\"params\", axis=1)     .style.background_gradient() ) In\u00a0[\u00a0]: Copied! <pre>optimized_pipe = gridsearch.best_estimator_\n\ny_pred = optimized_pipe.predict(X_test)\n\nmetrics.f1_score(y_test, y_pred)\n</pre> optimized_pipe = gridsearch.best_estimator_  y_pred = optimized_pipe.predict(X_test)  metrics.f1_score(y_test, y_pred)"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning_students/#practice-n5-cross-validation-hyperparameter-tuning-students-version","title":"\ud83d\udc0d Practice n\u00b05: cross validation &amp; hyperparameter tuning (students version)\u00b6","text":"<p>The objective of this session is to optimize an end-to-end machine learning pipeline from feature engineering to model training and evaluation. Some notions have already been tackled during session 1 or 2, the idea of this notebook is to have a focus on cross validation and hyperparameters tuning.</p> <p>We will remain in the frame of the binary classification of the session 2. Our goal is to have a model that outputs a label/class (good review or bad review).</p> <p>Here are the main steps of the notebook :</p> <ol> <li>Preparation</li> <li>Data split</li> <li>Model training and evaluation</li> <li>Manual hyperparameter tuning</li> <li>Cross validation &amp; Automated hyperparameters tuning</li> </ol>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning_students/#preparation","title":"Preparation\u00b6","text":""},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning_students/#install-import-modules","title":"Install &amp; import modules\u00b6","text":""},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning_students/#read-remote-dataset","title":"Read remote dataset\u00b6","text":""},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning_students/#splits-traintest-featurestarget","title":"Splits: Train/test &amp; features/target\u00b6","text":"<p>Split the data keeping 20% in the test set.</p>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning_students/#model-training-and-evaluation","title":"Model training and evaluation\u00b6","text":"<p>Train a logistic regression on the train set</p>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning_students/#manual-hyperparameter-tuning","title":"Manual hyperparameter tuning\u00b6","text":""},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning_students/#cross-validation","title":"Cross validation\u00b6","text":"<p>When evaluating different settings (\u201chyperparameters\u201d) for estimators, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can \u201cleak\u201d into the model and evaluation metrics no longer report on generalization performance.</p> <p>A solution to this problem is a procedure called cross-validation. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k \u201cfolds\u201d:</p> <ul> <li>A model is trained using k - 1 of the folds as training data</li> <li>The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy)</li> </ul> <p>The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop.</p>"},{"location":"10-Tutorials/5_cross_validation_and_hyperparameter_tuning_students/#automated-hyperparameters-tuning","title":"Automated hyperparameters tuning\u00b6","text":"<p>As the manual tuning of hyperparameters can take a long time, some solutions exist to help you find the best combination.</p>"}]}